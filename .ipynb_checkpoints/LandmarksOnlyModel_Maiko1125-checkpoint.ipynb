{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58563195",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T09:32:25.856198Z",
     "start_time": "2023-11-25T09:32:13.978227Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ===================== IMPORTS/LIBRARIES =====================\n",
    "\n",
    "from luxpy import (cat, colortf, _CIEOBS, _CIE_ILLUMINANTS, _CRI_RFL, _CIE_D65,_CIE_E,\n",
    "                   spd_to_xyz, plot_color_data, math, cie_interp, getwlr, xyz_to_srgb)\n",
    "from luxpy.utils import np, plt, sp, _PKG_PATH, _SEP, _EPS \n",
    "import warnings\n",
    "from imageio import imsave\n",
    "\n",
    "import tensorflow as tf\n",
    "from mtcnn import MTCNN\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import glob\n",
    "import cv2\n",
    "import csv\n",
    "import os\n",
    "import ast\n",
    "import pydot\n",
    "import pydotplus\n",
    "import graphviz\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import luxpy as lx\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import transform\n",
    "import imageio\n",
    "from skimage.transform import rescale,resize\n",
    "\n",
    "from URetinex_Net import *\n",
    "\n",
    "# Import Arch_network\n",
    "import sys\n",
    "sys.path.append(r\"URetinex_Net\")\n",
    "\n",
    "# testing of URetinex.Net\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from network.Math_Module import P, Q\n",
    "from network.decom import Decom\n",
    "import os\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import time\n",
    "from utils import *\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b136912",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T09:32:39.345258Z",
     "start_time": "2023-11-25T09:32:39.333289Z"
    },
    "code_folding": [
     0,
     14
    ]
   },
   "outputs": [],
   "source": [
    "def historyToCsv():\n",
    "    # convert the history.history dict to a pandas DataFrame:     \n",
    "    hist_df = pd.DataFrame(history.history) \n",
    "    \n",
    "    current_datetime = datetime.now()\n",
    "\n",
    "    # Convert the datetime object to a string\n",
    "    filename_friendly_datetime_string = current_datetime.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    # save to csv: \n",
    "    hist_csv_file = 'history' + filename_friendly_datetime_string + '.csv'\n",
    "    with open(hist_csv_file, mode='w') as f:\n",
    "        hist_df.to_csv(f)\n",
    "\n",
    "def csvToHistory(csv_filename):\n",
    "    # Read the CSV file into a pandas DataFrame\n",
    "    hist_df = pd.read_csv(csv_filename, index_col=0)\n",
    "\n",
    "    # Convert the DataFrame to a dictionary\n",
    "    history_dict = hist_df.to_dict(orient='list')\n",
    "\n",
    "    return history_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889d85ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T09:33:01.067479Z",
     "start_time": "2023-11-25T09:32:59.533496Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ===================== MULTITASK MODEL SETUP =====================\n",
    "base_model = tf.keras.applications.VGG16(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "\n",
    "flattened_features = tf.keras.layers.Flatten(name='flattened_features')(base_model.output)\n",
    "\n",
    "embedding_layer = tf.keras.layers.Dense(512, activation='linear', name='embedding')(flattened_features)\n",
    "\n",
    "additional_dense_layer1 = tf.keras.layers.Dense(64, name='additional_dense1')(embedding_layer)\n",
    "additional_dense_layer2 = tf.keras.layers.Dense(64, name='additional_dense2')(additional_dense_layer1)\n",
    "# additional_dense_layer3 = tf.keras.layers.Dense(64, activation='relu', name='additional_dense3')(flattened_features)\n",
    "\n",
    "landmarks = tf.keras.layers.Dense(10, activation='linear', name='landmark_output')(additional_dense_layer2)\n",
    "# illum = tf.keras.layers.Dense(1, activation='linear', name='previous_illuminance_output')(additional_dense_layer2)\n",
    "\n",
    "# Reshape layer to the desired shape\n",
    "# reshaped_features = tf.keras.layers.Reshape((8, 8, 8))(embedding_layer)\n",
    "\n",
    "# Upsampling layers\n",
    "# upsample1 = tf.keras.layers.UpSampling2D(size=(7, 7))(reshaped_features)\n",
    "# upsample2 = tf.keras.layers.UpSampling2D(size=(2, 2))(upsample1)\n",
    "# upsample3 = tf.keras.layers.UpSampling2D(size=(2, 2))(upsample2)\n",
    "\n",
    "# retIllum = tf.keras.layers.Conv2D(3, kernel_size=(3, 3), activation='relu', padding='same', name='image_retinex_output')(upsample3)\n",
    "\n",
    "task_outputs = None\n",
    "\n",
    "task_outputs = [landmarks]\n",
    "\n",
    "multi_task_model = tf.keras.Model(inputs=base_model.input, outputs=task_outputs)\n",
    "\n",
    "# Compile the model with specific loss functions and metrics for each task\n",
    "multi_task_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={\n",
    "        'landmark_output': 'mean_squared_error'\n",
    "#         'previous_illuminance_output': 'mean_squared_error',\n",
    "#         'image_retinex_output': 'mean_squared_error'\n",
    "    },\n",
    "    metrics={\n",
    "        'landmark_output': ['mse', \"mae\"]\n",
    "#         'previous_illuminance_output': ['mse', \"mae\"],\n",
    "#         'image_retinex_output': ['mse', \"mae\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "# Summary of the multi-task model\n",
    "multi_task_model.summary()\n",
    "# plot_model(multi_task_model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d011f9f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T09:33:18.011791Z",
     "start_time": "2023-11-25T09:33:17.873558Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ===================== DROWSINESS MODEL =====================\n",
    "\n",
    "# retain weights and remove top layer\n",
    "output_layer = multi_task_model.get_layer('embedding').output\n",
    "\n",
    "drowsiness_model = Model(inputs=multi_task_model.input, outputs=output_layer)\n",
    "\n",
    "# drowsiness_model.summary()\n",
    "# tf.keras.utils.plot_model(drowsiness_model)\n",
    "from keras.utils.vis_utils import plot_model\n",
    "# plot_model(drowsiness_model, to_file='drowsinessModel_plot.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "existing_output = drowsiness_model.output\n",
    "\n",
    "reshaped_output = tf.keras.layers.Reshape((16, 16, 2))(existing_output)\n",
    "\n",
    "spatial_attention = tf.keras.layers.Conv2D(2, (1, 1), activation='sigmoid', padding='same')(reshaped_output)\n",
    "spatial_attention = tf.keras.layers.Softmax()(spatial_attention)\n",
    "output_tensor = tf.keras.layers.Multiply()([reshaped_output, spatial_attention])\n",
    "\n",
    "# Add Global Average Pooling layer\n",
    "output_tensor = tf.keras.layers.GlobalAveragePooling2D()(output_tensor)\n",
    "\n",
    "# Add output layer with two classes and softmax activation\n",
    "predictions = tf.keras.layers.Dense(2, activation='softmax', name='drowsiness_output')(output_tensor)\n",
    "\n",
    "# Create the new model with the modified top layers\n",
    "drowsiness_model = Model(inputs=drowsiness_model.input, outputs=predictions)\n",
    "\n",
    "drowsiness_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={\n",
    "        'drowsiness_output': 'binary_crossentropy'\n",
    "    },\n",
    "    metrics={\n",
    "        'drowsiness_output': [\"accuracy\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "drowsiness_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b609b7e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T09:33:40.338386Z",
     "start_time": "2023-11-25T09:33:40.313935Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ===================== LANDMARKS ONLY DATA GEN =====================\n",
    "\n",
    "class CustomDataGen(tf.keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, df, X_col, y_col,\n",
    "                 batch_size,\n",
    "                 input_size=(224, 224, 3),\n",
    "                 shuffle=True):\n",
    "        \n",
    "        self.df = df.copy()\n",
    "        self.X_col = X_col\n",
    "        self.y_col = y_col\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.n = len(self.df)\n",
    "        self.n_coords = 2  # Assuming landmark coordinates are 2-dimensional\n",
    "#         self.n_illuminance = 1  # Assuming a single illuminance value\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    def __get_input(self, path, target_size):\n",
    "    \n",
    "        image = tf.keras.preprocessing.image.load_img(path)\n",
    "        image_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "\n",
    "        image_arr = tf.image.resize(image_arr, (target_size[0], target_size[1])).numpy()\n",
    "\n",
    "        return image_arr / 255.\n",
    "    \n",
    "    def __get_output(self, label, output_type):\n",
    "        # Assuming output_type is 'coordinates', 'illuminance', or 'adjusted_image_path'\n",
    "        if output_type == 'coordinates':\n",
    "            # Assuming label is a string containing a dictionary-like structure\n",
    "            # Safely evaluate the string as a literal dictionary using ast.literal_eval\n",
    "            coordinates_dict = ast.literal_eval(label)\n",
    "            \n",
    "            # Extract x and y coordinates for each landmark\n",
    "            landmarks = ['left_eye', 'right_eye', 'nose', 'mouth_left', 'mouth_right']\n",
    "            coordinates_list = [coordinates_dict[landmark] for landmark in landmarks]\n",
    "            \n",
    "            # Flatten the list and convert to numpy array\n",
    "            coordinates_array = np.array([coord for landmark_coords in coordinates_list for coord in landmark_coords])\n",
    "            \n",
    "#             print(\"Shape of landmarks_array:\", coordinates_array.shape)\n",
    "            \n",
    "            # If there are exactly 10 values, return the array, otherwise raise an error\n",
    "            if len(coordinates_array) == 10:\n",
    "                return coordinates_array\n",
    "            else:\n",
    "                raise ValueError(\"Expected 10 coordinates, but found {}\".format(len(coordinates_array)))\n",
    "#         elif output_type == 'illuminance':\n",
    "#             # Convert the illuminance value to a float\n",
    "#             return float(label)\n",
    "#         elif output_type == 'adjusted_image_path':\n",
    "#             # Assuming label is the path to the adjusted image\n",
    "#             return self.__get_input(label, self.input_size)\n",
    "    \n",
    "    def __get_data(self, batches):\n",
    "        # Generates data containing batch_size samples\n",
    "\n",
    "        path_batch = batches[self.X_col['path']]\n",
    "        \n",
    "        coords_batch = batches[self.y_col['coordinates']]\n",
    "#         illuminance_batch = batches[self.y_col['illuminance']]\n",
    "#         adjusted_image_path_batch = batches[self.y_col['adjusted_image_path']]\n",
    "\n",
    "        X_batch = np.asarray([self.__get_input(x, self.input_size) for x in path_batch])\n",
    "\n",
    "        y0_batch = np.asarray([self.__get_output(y, 'coordinates') for y in coords_batch])\n",
    "#         y1_batch = np.asarray([self.__get_output(y, 'illuminance') for y in illuminance_batch])\n",
    "#         y2_batch = np.asarray([self.__get_output(y, 'adjusted_image_path') for y in adjusted_image_path_batch])\n",
    "\n",
    "        return X_batch, [y0_batch]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batches = self.df[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        X, y = self.__get_data(batches)\n",
    "\n",
    "        # Print a few examples of the data\n",
    "#         print(\"Sample X:\", tf.shape(X[0]))  # Print the first example in the batch\n",
    "#         print(\"Sample y[0] (landmarks):\", tf.shape(y[0][0]))  # Print the first example in the landmarks output\n",
    "#         print(\"Sample y[1] (illum):\", tf.shape(y[1][0]))  # Print the first example in the illum output\n",
    "#         print(\"Sample y[2] (adjusted_image_path):\", tf.shape(y[2][0]))  # Print the first example in the adjusted_image_path output\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f160eeb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T09:33:51.537582Z",
     "start_time": "2023-11-25T09:33:49.893419Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ===================== DATA GEN SETUP (LANDMARKS ONLY TASK) =====================\n",
    "\n",
    "df = pd.read_csv(\"\") # path to train_data csv\n",
    "df[\"Filename\"] = \"./data/Evaluation/\" + df[\"Filename\"] + \".jpg\"\n",
    "\n",
    "# Define column indices or names for X and y\n",
    "X_col = {'path': 'Filename'}\n",
    "y_col = {'coordinates': 'LandmarksRaw'}\n",
    "\n",
    "# Create an instance of CustomDataGen\n",
    "train_gen = CustomDataGen(df, X_col, y_col, batch_size=32, input_size=(224, 224, 3))\n",
    "\n",
    "eval_df = pd.read_csv(\"\") # path to eval_data csv\n",
    "eval_df[\"Filename\"] = \"./data/Evaluation/\" + eval_df[\"Filename\"] + \".jpg\"\n",
    "\n",
    "# Define column indices or names for X and y\n",
    "eval_X_col = {'path': 'Filename'}\n",
    "eval_y_col = {'coordinates': 'LandmarksRaw'}\n",
    "\n",
    "val_gen = CustomDataGen(eval_df, eval_X_col, eval_y_col, batch_size=32, input_size=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6061fde3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T09:37:10.104205Z",
     "start_time": "2023-11-25T09:37:10.069299Z"
    }
   },
   "outputs": [],
   "source": [
    "history = multi_task_model.fit(train_gen, epochs=50, validation_data=val_gen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
