{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f69663c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T06:43:39.691297Z",
     "start_time": "2023-11-28T06:43:26.260884Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michael\\miniconda3\\envs\\torch\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\michael\\miniconda3\\envs\\torch\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ===================== IMPORTS/LIBRARIES =====================\n",
    "\n",
    "from luxpy import (cat, colortf, _CIEOBS, _CIE_ILLUMINANTS, _CRI_RFL, _CIE_D65,_CIE_E,\n",
    "                   spd_to_xyz, plot_color_data, math, cie_interp, getwlr, xyz_to_srgb)\n",
    "from luxpy.utils import np, plt, sp, _PKG_PATH, _SEP, _EPS \n",
    "import warnings\n",
    "from imageio import imsave\n",
    "\n",
    "import tensorflow as tf\n",
    "from mtcnn import MTCNN\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import glob\n",
    "import cv2\n",
    "import csv\n",
    "import os\n",
    "import ast\n",
    "import pydot\n",
    "import pydotplus\n",
    "import graphviz\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import time\n",
    "import luxpy as lx\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import transform\n",
    "import imageio\n",
    "from skimage.transform import rescale,resize\n",
    "\n",
    "from URetinex_Net import *\n",
    "\n",
    "# Import Arch_network\n",
    "import sys\n",
    "sys.path.append(r\"URetinex_Net\")\n",
    "\n",
    "# testing of URetinex.Net\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from network.Math_Module import P, Q\n",
    "from network.decom import Decom\n",
    "import os\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import time\n",
    "from utils import *\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646936c1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-28T03:49:47.731Z"
    }
   },
   "outputs": [],
   "source": [
    "pip uninstall tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf89806",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T08:02:18.412651Z",
     "start_time": "2023-11-25T08:02:18.400169Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Data Storage Lists\n",
    "trainLandmarks = []\n",
    "trainLandmarksRet = []\n",
    "trainFileNames = []\n",
    "trainIllumsRaw = []\n",
    "trainIllumsRet = []\n",
    "\n",
    "evalLandmarks = []\n",
    "evalLandmarksRet = []\n",
    "evalFileNames = []\n",
    "evalIllumsRaw = []\n",
    "evalIllumsRet = []\n",
    "\n",
    "testFileNames = []\n",
    "testDrowsiness = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096c8392",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T08:02:18.444601Z",
     "start_time": "2023-11-25T08:02:18.415646Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# frame_capture(file)\n",
    "def frame_capture(file): \n",
    "    \n",
    "    cap = cv2.VideoCapture(file)\n",
    "    \n",
    "    try:\n",
    "        if not os.path.exists('data'):\n",
    "            os.makedirs('data')\n",
    "    except OSError:\n",
    "        print('Error: Creating directory of data')\n",
    "            \n",
    "    current_frame = 2668 # we change this value to where we resume\n",
    "    start_new_video = False\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            current_frame = 0\n",
    "            break \n",
    "            \n",
    "        if start_new_video:\n",
    "            current_frame = 0\n",
    "            start_new_video = False\n",
    "            \n",
    "        datasetDir = os.path.dirname(os.path.dirname(os.path.dirname(file)))\n",
    "        indexDataset = datasetDir.rfind('\\\\')\n",
    "        datasetDir = datasetDir[indexDataset:]\n",
    "\n",
    "        parentDir = os.path.dirname(os.path.dirname(file)) # refers to <driver_num> e.g. \"001, 002\"\n",
    "        currDir = parentDir\n",
    "\n",
    "        childDir = os.path.dirname(file) # refers to where the current video file is <scenario> e.g. noglasses, glasses, etc.\n",
    "        indexParent = parentDir.rfind('\\\\')\n",
    "        parentDir = parentDir[indexParent:] # refers to <scenario> only\n",
    "\n",
    "        indexChild = childDir.rfind('\\\\')\n",
    "        childDir = childDir[indexChild:]\n",
    "\n",
    "#             filename of txt document containing labels for current video\n",
    "        if datasetDir[1:] == \"Training Dataset\":\n",
    "            labelFile = currDir + \"\\\\\" + childDir[1:] + \"\\\\\" + parentDir[1:] + \"_\" + os.path.basename(file)[:-4] + \"_drowsiness\" + '.txt'\n",
    "        elif parentDir[1:] == \"Evaluation Dataset\":\n",
    "            labelFile = currDir + \"\\\\\" + childDir[1:] + \"\\\\\" + os.path.basename(file)[:-4] + \"_drowsiness\" + '.txt'\n",
    "            labelFile = labelFile.replace(\"mix\", \"mixing\")\n",
    "            labelFile = labelFile.replace(\"nightno\", \"night_no\")\n",
    "        else:\n",
    "            scenario = \"\"\n",
    "            if \"sunglasses\" in os.path.basename(file)[:-4]:\n",
    "                scenario = \"sunglasses\"\n",
    "            elif \"night_noglasses\" in os.path.basename(file)[:-4]:\n",
    "                scenario = \"night_noglasses\"    \n",
    "            elif \"noglasses\" in os.path.basename(file)[:-4]:\n",
    "                scenario = \"noglasses\"\n",
    "            elif \"night_glasses\" in os.path.basename(file)[:-4]:\n",
    "                scenario = \"sunglasses\"\n",
    "            else:\n",
    "                scenario = \"glasses\"\n",
    "\n",
    "            labelFile = currDir + \"\\\\\" + childDir[1:] + \"\\\\\" + \"test_label_txt\\\\\" + scenario + \"\\\\\" + os.path.basename(file)[:-4] + \"_drowsiness\" + '.txt'\n",
    "\n",
    "            if not os.path.exists(labelFile):                    \n",
    "                labelFile = currDir + \"\\\\\" + childDir[1:] + \"\\\\\" + \"test_label_txt\\\\\" + \"wh\" + \"\\\\\" + os.path.basename(file)[:-4] + \"_drowsiness\" + '.txt'\n",
    "\n",
    "            labelFile = labelFile.replace(\"mix\", \"mixing\")\n",
    "\n",
    "            if not os.path.exists(labelFile): # if labels file is non-existent for the given video\n",
    "                continue\n",
    "\n",
    "        with open(labelFile) as f:\n",
    "            labels = f.readline()\n",
    "        try:\n",
    "\n",
    "            if datasetDir[1:] == \"Training Dataset\":\n",
    "                save_path = \"./data/Training/\" # specify where (in relation to root path to place new folder)\n",
    "                \n",
    "                file_name = save_path + childDir[1:] + \"_\" + parentDir[1:] + \"_\" + os.path.basename(file)[:-4] + \"_\" + str(current_frame) + \"_\" + labels[current_frame] + '.jpg'\n",
    "            elif parentDir[1:] == \"Evaluation Dataset\":\n",
    "                save_path = \"./data/Evaluation/\" # specify where (in relation to root path to place new folder)\n",
    "                \n",
    "                file_name = save_path + os.path.basename(file)[:-4] + \"_\" + str(current_frame) + \"_\" + labels[current_frame] + '.jpg'\n",
    "            else:\n",
    "                save_path = \"./data/Testing/\" # specify where (in relation to root path to place new folder)\n",
    "                \n",
    "                file_name = save_path + parentDir[1:] + os.path.basename(file)[:-4] + \"_\" + str(current_frame) + \"_\" + labels[current_frame] + '.jpg'\n",
    "    \n",
    "        except IndexError:\n",
    "            continue\n",
    "            \n",
    "        cv2.imwrite(file_name, frame)\n",
    "\n",
    "        img_file = Path(file_name) # for Retinex images, use replace() to get path of same named images in order to get post-retinex illum\n",
    "        \n",
    "        print(img_file)\n",
    "        print(os.path.basename(img_file))\n",
    "        \n",
    "        if save_path == \"./data/Training/\":\n",
    "            \n",
    "            data_fileName = os.path.basename(img_file)[:-4]\n",
    "            data_landmarks = generateLandmarks(os.path.dirname(img_file) + \"/\" + os.path.basename(img_file))\n",
    "            data_illuminance = illuminanceEstimation(img_file)\n",
    "            \n",
    "            retSave_path = \"./data/RetTraining/\"\n",
    "            retinexImplement(img_file, retSave_path)\n",
    "\n",
    "#           post-retinex functions\n",
    "\n",
    "            retSave_path = retSave_path + os.path.basename(img_file)[:-4] + \"_URetinexNet.jpg\"\n",
    "            data_illuminanceRet = illuminanceEstimation(retSave_path)\n",
    "            data_landmarksRet = generateLandmarks(retSave_path)\n",
    "            \n",
    "            exportTrain(data_fileName, data_landmarks, data_illuminance, data_illuminanceRet)\n",
    "\n",
    "        elif save_path == \"./data/Evaluation/\":\n",
    "        \n",
    "            data_fileName = os.path.basename(img_file)[:-4]\n",
    "            data_landmarks = generateLandmarks(os.path.dirname(img_file) + \"/\" + os.path.basename(img_file))\n",
    "            data_illuminance = illuminanceEstimation(img_file)\n",
    "\n",
    "            retSave_path = \"./data/RetEvaluation/\"\n",
    "            retinexImplement(img_file, retSave_path)\n",
    "\n",
    "#             post-retinex functions\n",
    "\n",
    "            retSave_path = retSave_path + os.path.basename(img_file)[:-4] + \"_URetinexNet.jpg\"\n",
    "            data_illuminanceRet = illuminanceEstimation(retSave_path)\n",
    "            data_landmarksRet = generateLandmarks(retSave_path)\n",
    "            \n",
    "            exportEval(data_fileName, data_landmarks, data_illuminance, data_illuminanceRet)\n",
    "\n",
    "        else:\n",
    "            data_fileName = os.path.basename(img_file)[:-4]\n",
    "            data_drowsiness = float(labels[current_frame])\n",
    "            print('Creating...' + file_name)\n",
    "            \n",
    "            exportTest(data_fileName, data_drowsiness)\n",
    "                \n",
    "        current_frame += 1\n",
    "        \n",
    "        if current_frame > int(cap.get(cv2.CAP_PROP_FRAME_COUNT)):  # Replace 'total_frames_in_current_video' with the actual number of frames in the video\n",
    "            start_new_video = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef9a360",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T08:02:18.461076Z",
     "start_time": "2023-11-25T08:02:18.447600Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Training Videos Path\n",
    "def trainingData_prep():\n",
    "    training_videos = Path(r\"NTHU Dataset\\Training_Evaluation_Dataset\\Training Dataset\") #AVIs\n",
    "    training_video_paths = []\n",
    "\n",
    "    # create data folder\n",
    "    try:\n",
    "        if not os.path.exists('data'):\n",
    "                os.makedirs('data')\n",
    "                os.mkdir(os.path.join('./data/', 'Training'))\n",
    "                os.mkdir(os.path.join('./data/', 'Evaluation'))\n",
    "                os.mkdir(os.path.join('./data/', 'Testing'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetTraining'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetEvaluation'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetTesting'))\n",
    "    except OSError:\n",
    "        print('Error: Creating directory of data')\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for driver in training_videos.glob(\"*\"):\n",
    "        for scenario in driver.glob(\"*\"):\n",
    "            for videos_file in sorted(scenario.glob(\"*.avi\")):\n",
    "\n",
    "    #         note: videos_file refers to direct path of current video file\n",
    "\n",
    "                datasetDir = os.path.dirname(os.path.dirname(os.path.dirname(videos_file)))\n",
    "                indexDataset = datasetDir.rfind('\\\\')\n",
    "                datasetDir = datasetDir[indexDataset:]\n",
    "\n",
    "                parentDir = os.path.dirname(os.path.dirname(videos_file)) # refers to <driver_num> e.g. \"001, 002\"\n",
    "                currDir = parentDir\n",
    "\n",
    "                childDir = os.path.dirname(videos_file) # refers to where the current video file is <scenario> e.g. noglasses, glasses, etc.\n",
    "                indexParent = parentDir.rfind('\\\\')\n",
    "                parentDir = parentDir[indexParent:] # refers to <scenario> only\n",
    "\n",
    "                indexChild = childDir.rfind('\\\\')\n",
    "                childDir = childDir[indexChild:]\n",
    "\n",
    "                if datasetDir[1:] == \"Training Dataset\":\n",
    "                    save_path = \"./data/Training/\" # specify where (in relation to root path to place new folder)\n",
    "#                     folder_name = parentDir[1:] + \"_\" + os.path.basename(videos_file)[:-4] # specify name of new folder\n",
    "                elif parentDir[1:] == \"Evaluation Dataset\":\n",
    "                    save_path = \"./data/Evaluation/\" # specify where (in relation to root path to place new folder)\n",
    "#                     folder_name = os.path.basename(videos_file)[:-4] # specify name of new folder\n",
    "\n",
    "#                 data_path = os.path.join(save_path, folder_name) # set path and folder name as input for os.path.join()\n",
    "\n",
    "                inputPath = save_path\n",
    "\n",
    "#                 if not os.path.exists(inputPath):\n",
    "#                     os.mkdir(input_path) # create new folder based on data_path\n",
    "\n",
    "                video_path = str(videos_file)\n",
    "                training_video_paths.append(video_path)\n",
    "                frame_capture(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732f5fa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T08:02:18.476587Z",
     "start_time": "2023-11-25T08:02:18.463102Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluation Videos Path\n",
    "def evalData_prep():\n",
    "    evaluation_videos = Path(r\"NTHU Dataset\\Training_Evaluation_Dataset\\Evaluation Dataset\") #AVIs\n",
    "    evaluation_video_paths = []\n",
    "\n",
    "    # create data folder\n",
    "    try:\n",
    "        if not os.path.exists('data'):\n",
    "                os.makedirs('data')\n",
    "                os.mkdir(os.path.join('./data/', 'Training'))\n",
    "                os.mkdir(os.path.join('./data/', 'Evaluation'))\n",
    "                os.mkdir(os.path.join('./data/', 'Testing'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetTraining'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetEvaluation'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetTesting'))\n",
    "    except OSError:\n",
    "        print('Error: Creating directory of data')\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for driver in evaluation_videos.glob(\"*\"):\n",
    "        for videos_file in sorted(driver.glob(\"*.mp4\")):\n",
    "\n",
    "    #         note: videos_file refers to direct path of current video file\n",
    "\n",
    "            datasetDir = os.path.dirname(os.path.dirname(os.path.dirname(videos_file)))\n",
    "            indexDataset = datasetDir.rfind('\\\\')\n",
    "            datasetDir = datasetDir[indexDataset:]\n",
    "\n",
    "            parentDir = os.path.dirname(os.path.dirname(videos_file)) # refers to <driver_num> e.g. \"001, 002\"\n",
    "            currDir = parentDir\n",
    "\n",
    "            childDir = os.path.dirname(videos_file) # refers to where the current video file is <scenario> e.g. noglasses, glasses, etc.\n",
    "            indexParent = parentDir.rfind('\\\\')\n",
    "            parentDir = parentDir[indexParent:] # refers to <scenario> only\n",
    "\n",
    "            indexChild = childDir.rfind('\\\\')\n",
    "            childDir = childDir[indexChild:]\n",
    "\n",
    "            if datasetDir[1:] == \"Training Dataset\":\n",
    "                save_path = \"./data/Training/\" # specify where (in relation to root path to place new folder)\n",
    "                folder_name = parentDir[1:] + \"_\" + os.path.basename(videos_file)[:-4] # specify name of new folder\n",
    "            elif parentDir[1:] == \"Evaluation Dataset\":\n",
    "                save_path = \"./data/Evaluation/\" # specify where (in relation to root path to place new folder)\n",
    "                folder_name = os.path.basename(videos_file)[:-4] # specify name of new folder\n",
    "\n",
    "            data_path = os.path.join(save_path, folder_name) # set path and folder name as input for os.path.join()\n",
    "\n",
    "#             inputPath = save_path + folder_name\n",
    "\n",
    "#             if not os.path.exists(inputPath):\n",
    "#                 os.mkdir(data_path) # create new folder based on data_path\n",
    "\n",
    "            video_path = str(videos_file)\n",
    "            evaluation_video_paths.append(video_path)\n",
    "            frame_capture(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2243a300",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T08:02:18.507018Z",
     "start_time": "2023-11-25T08:02:18.480090Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Testing Videos Path\n",
    "def testData_prep():\n",
    "    testing_videos = Path(r\"NTHU Dataset\\Testing_Dataset\") #MP4s\n",
    "    testing_video_paths = []\n",
    "\n",
    "    # create data folder\n",
    "    try:\n",
    "        if not os.path.exists('data'):\n",
    "                os.makedirs('data')\n",
    "                os.mkdir(os.path.join('./data/', 'Training'))\n",
    "                os.mkdir(os.path.join('./data/', 'Evaluation'))\n",
    "                os.mkdir(os.path.join('./data/', 'Testing'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetTraining'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetEvaluation'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetTesting'))\n",
    "    except OSError:\n",
    "        print('Error: Creating directory of data')\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for videos_file in testing_videos.glob(\"*.mp4\"):\n",
    "        print(videos_file)\n",
    "\n",
    "#         note: videos_file refers to direct path of current video file\n",
    "\n",
    "        print(os.path.dirname(videos_file))\n",
    "\n",
    "        datasetDir = os.path.dirname(videos_file)\n",
    "        indexDataset = datasetDir.rfind('\\\\')\n",
    "        datasetDir = datasetDir[indexDataset:]\n",
    "\n",
    "        save_path = \"./data/Testing/\" # specify where (in relation to root path to place new folder)\n",
    "        folder_name = os.path.basename(videos_file)[:-4] # specify name of new folder\n",
    "\n",
    "        print(save_path + folder_name)\n",
    "\n",
    "        data_path = os.path.join(save_path, folder_name) # set path and folder name as input for os.path.join()\n",
    "\n",
    "        inputPath = save_path + folder_name\n",
    "\n",
    "#         if not os.path.exists(inputPath):\n",
    "#             os.mkdir(data_path) # create new folder based on data_path\n",
    "\n",
    "        video_path = str(videos_file)\n",
    "        testing_video_paths.append(video_path)\n",
    "        frame_capture(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6f26ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T08:02:18.521978Z",
     "start_time": "2023-11-25T08:02:18.509013Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Landmarks Task\n",
    "\n",
    "def generateLandmarks(img):\n",
    "    img = cv2.imread(img)\n",
    "    output = detector.detect_faces(img)\n",
    "    \n",
    "    if len(output) < 1:\n",
    "        initial_values = 0\n",
    "\n",
    "        output = {\n",
    "            'box': [initial_values, initial_values, initial_values, initial_values],\n",
    "            'confidence': initial_values,\n",
    "            'keypoints': {\n",
    "                'left_eye': (initial_values, initial_values),\n",
    "                'right_eye': (initial_values, initial_values),\n",
    "                'nose': (initial_values, initial_values),\n",
    "                'mouth_left': (initial_values, initial_values),\n",
    "                'mouth_right': (initial_values, initial_values)\n",
    "            }\n",
    "        }\n",
    "        return output['keypoints']\n",
    "\n",
    "    return output[0]['keypoints']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0936f309",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T08:02:18.616269Z",
     "start_time": "2023-11-25T08:02:18.524972Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*- ILLUM EST FUNCTIONS\n",
    "\n",
    "illum_out = 0\n",
    "\n",
    "__all__ =['_HYPSPCIM_PATH','_HYPSPCIM_DEFAULT_IMAGE','render_image','xyz_to_rfl',\n",
    "          'get_superresolution_hsi','hsi_to_rgb','rfl_to_rgb','_CSF_NIKON_D700']             \n",
    "\n",
    "_HYPSPCIM_PATH = _PKG_PATH + _SEP + 'hypspcim' + _SEP\n",
    "_HYPSPCIM_DEFAULT_IMAGE = _PKG_PATH + _SEP + 'toolboxes' + _SEP + 'hypspcim' +  _SEP + 'data' + _SEP + 'testimage1.jpg'\n",
    "\n",
    "\n",
    "_ROUNDING = 6 # to speed up xyz_to_rfl search algorithm, increase if kernel dies!!!\n",
    "\n",
    "# Nikon D700 camera sensitivity functions:\n",
    "_CSF_NIKON_D700 = np.vstack((np.arange(400,710,10),\n",
    "                             np.array([[0.005, 0.007, 0.012, 0.015, 0.023, 0.025, 0.030, 0.026, 0.024, 0.019, 0.010, 0.004, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  0.000,  0.000,  0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000], \n",
    "                                       [0.000, 0.000, 0.000, 0.000, 0.000, 0.001, 0.002, 0.003, 0.005, 0.007, 0.012, 0.013, 0.015, 0.016, 0.017, 0.020, 0.013, 0.011, 0.009, 0.005,  0.001,  0.001,  0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.002, 0.002, 0.003],\n",
    "                                       [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.001, 0.003, 0.010, 0.012,  0.013,  0.022,  0.020, 0.020, 0.018, 0.017, 0.016, 0.016, 0.014, 0.014, 0.013]])[::-1]))\n",
    "\n",
    "\n",
    "def xyz_to_rfl(xyz, CSF = None, rfl = None, out = 'rfl_est', \\\n",
    "                 refspd = None, D = None, cieobs = _CIEOBS, \\\n",
    "                 cspace = 'xyz', cspace_tf = {},\\\n",
    "                 interp_type = 'nd', k_neighbours = 4, verbosity = 0,\n",
    "                 csf_based_rgb_rounding = _ROUNDING):\n",
    "   \n",
    "\n",
    "    # get rfl set:\n",
    "    if rfl is None: # use IESTM30['4880'] set \n",
    "        rfl = _CRI_RFL['ies-tm30']['4880']['5nm']\n",
    "    \n",
    "    wlr = rfl[0]\n",
    "    \n",
    "    # get Ref spd:\n",
    "    if refspd is None:\n",
    "        refspd = _CIE_ILLUMINANTS['D65'].copy()\n",
    "    refspd = cie_interp(refspd, wlr, kind = 'linear') # force spd to same wavelength range as rfl\n",
    "        \n",
    "    # Calculate rgb values of standard rfl set under refspd:\n",
    "    if CSF is None:\n",
    "        # Calculate lab coordinates:\n",
    "        xyz_rr, xyz_wr = spd_to_xyz(refspd, relative = True, rfl = rfl, cieobs = cieobs, out = 2)\n",
    "        cspace_tf_copy = cspace_tf.copy()\n",
    "        cspace_tf_copy['xyzw'] = xyz_wr # put correct white point in param. dict\n",
    "        lab_rr = colortf(xyz_rr, tf = cspace, fwtf = cspace_tf_copy, bwtf = cspace_tf_copy)[:,0,:]\n",
    "    else:\n",
    "        # Calculate rgb coordinates from camera sensitivity functions\n",
    "        rgb_rr = rfl_to_rgb(rfl, spd = refspd, CSF = CSF, wl = None)   \n",
    "        lab_rr = rgb_rr\n",
    "        xyz = xyz\n",
    "        lab_rr = np.round(lab_rr,csf_based_rgb_rounding) # speed up search\n",
    "        \n",
    "        global illum_out\n",
    "        illum_out = np.mean(lab_rr)\n",
    "        print(\"Illuminance: \" + str(np.mean(lab_rr)))\n",
    "        \n",
    "    # Convert xyz to lab-type values under refspd:\n",
    "    if CSF is None:\n",
    "        lab = colortf(xyz, tf = cspace, fwtf = cspace_tf_copy, bwtf = cspace_tf_copy)\n",
    "    else:\n",
    "        lab = xyz # xyz contained rgb values !!!\n",
    "        rgb = xyz\n",
    "        lab = np.round(lab,csf_based_rgb_rounding) # speed up search\n",
    "    \n",
    "    if interp_type == 'nearest':\n",
    "        # Find rfl (cfr. lab_rr) from rfl set that results in 'near' metameric \n",
    "        # color coordinates for each value in lab_ur (i.e. smallest DE):\n",
    "        # Construct cKDTree:\n",
    "        tree = sp.spatial.cKDTree(lab_rr, copy_data = True)\n",
    "        \n",
    "        # Interpolate rfls using k nearest neightbours and inverse distance weigthing:\n",
    "        d, inds = tree.query(lab, k = k_neighbours )\n",
    "        if k_neighbours  > 1:\n",
    "            d += _EPS\n",
    "            w = (1.0 / d**2)[:,:,None] # inverse distance weigthing\n",
    "            rfl_est = np.sum(w * rfl[inds+1,:], axis=1) / np.sum(w, axis=1)\n",
    "        else:\n",
    "            rfl_est = rfl[inds+1,:].copy()\n",
    "    elif interp_type == 'nd':\n",
    "\n",
    "        rfl_est = math.ndinterp1_scipy(lab_rr, rfl[1:], lab)\n",
    "            \n",
    "        _isnan = np.isnan(rfl_est[:,0]) \n",
    "\n",
    "        if (_isnan.any()): #do nearest neigbour method for those that fail using Delaunay (i.e. ndinterp1_scipy)\n",
    "\n",
    "            # Find rfl (cfr. lab_rr) from rfl set that results in 'near' metameric \n",
    "            # color coordinates for each value in lab_ur (i.e. smallest DE):\n",
    "            # Construct cKDTree:\n",
    "            tree = sp.spatial.cKDTree(lab_rr, copy_data = True)\n",
    "\n",
    "            # Interpolate rfls using k nearest neightbours and inverse distance weigthing:\n",
    "            d, inds = tree.query(lab[_isnan,...], k = k_neighbours )\n",
    "\n",
    "            if k_neighbours  > 1:\n",
    "                d += _EPS\n",
    "                w = (1.0 / d**2)[:,:,None] # inverse distance weigthing\n",
    "                rfl_est_isnan = np.sum(w * rfl[inds+1,:], axis=1) / np.sum(w, axis=1)\n",
    "            else:\n",
    "                rfl_est_isnan = rfl[inds+1,:].copy()\n",
    "            rfl_est[_isnan, :] = rfl_est_isnan\n",
    "\n",
    "    else:\n",
    "        raise Exception('xyz_to_rfl(): unsupported interp_type!')\n",
    "    \n",
    "    rfl_est[rfl_est<0] = 0 #can occur for points outside convexhull of standard rfl set.\n",
    "\n",
    "    rfl_est = np.vstack((rfl[0],rfl_est))\n",
    "        \n",
    "    if ((verbosity > 0) | ('xyz_est' in out.split(',')) | ('lab_est' in out.split(',')) | ('DEi_ab' in out.split(',')) | ('DEa_ab' in out.split(','))) & (CSF is None):\n",
    "        xyz_est, _ = spd_to_xyz(refspd, rfl = rfl_est, relative = True, cieobs = cieobs, out = 2)\n",
    "        cspace_tf_copy = cspace_tf.copy()\n",
    "        cspace_tf_copy['xyzw'] = xyz_wr # put correct white point in param. dict\n",
    "        lab_est = colortf(xyz_est, tf = cspace, fwtf = cspace_tf_copy)[:,0,:]\n",
    "        DEi_ab = np.sqrt(((lab_est[:,1:3]-lab[:,1:3])**2).sum(axis=1))\n",
    "        DEa_ab = DEi_ab.mean()\n",
    "    elif ((verbosity > 0) | ('xyz_est' in out.split(',')) | ('rgb_est' in out.split(',')) | ('DEi_rgb' in out.split(',')) | ('DEa_rgb' in out.split(','))) & (CSF is not None):\n",
    "        rgb_est = rfl_to_rgb(rfl_est[1:], spd = refspd, CSF = CSF, wl = wlr) \n",
    "        xyz_est = rgb_est\n",
    "        DEi_rgb = np.sqrt(((rgb_est - rgb)**2).sum(axis=1))\n",
    "        DEa_rgb = DEi_rgb.mean()\n",
    "\n",
    "        \n",
    "    if verbosity > 0:\n",
    "        if CSF is None:\n",
    "            ax = plot_color_data(lab[...,1], lab[...,2], z = lab[...,0], \\\n",
    "                            show = False, cieobs = cieobs, cspace = cspace, \\\n",
    "                            formatstr = 'ro', label = 'Original')\n",
    "            plot_color_data(lab_est[...,1], lab_est[...,2], z = lab_est[...,0], \\\n",
    "                            show = True, axh = ax, cieobs = cieobs, cspace = cspace, \\\n",
    "                            formatstr = 'bd', label = 'Rendered')\n",
    "        else:\n",
    "            n = 100 #min(rfl.shape[0]-1,rfl_est.shape[0]-1)\n",
    "            s = np.random.permutation(rfl.shape[0]-1)[:min(n,rfl.shape[0]-1)]\n",
    "            st = np.random.permutation(rfl_est.shape[0]-1)[:min(n,rfl_est.shape[0]-1)]\n",
    "            fig = plt.figure()\n",
    "            ax = np.zeros((3,),dtype=np.object)\n",
    "            ax[0] = fig.add_subplot(131)\n",
    "            ax[1] = fig.add_subplot(132)\n",
    "            ax[2] = fig.add_subplot(133,projection='3d')\n",
    "            ax[0].plot(rfl[0],rfl[1:][s].T, linestyle = '-')\n",
    "            ax[0].set_title('Original RFL set (random selection of all)')\n",
    "            ax[0].set_ylim([0,1])\n",
    "            ax[1].plot(rfl_est[0],rfl_est[1:][st].T, linestyle = '--')\n",
    "            ax[0].set_title('Estimated RFL set (random selection of targets)')\n",
    "            ax[1].set_ylim([0,1])\n",
    "            ax[2].plot(rgb[st,0],rgb[st,1],rgb[st,2],'ro', label = 'Original')\n",
    "            ax[2].plot(rgb_est[st,0],rgb_est[st,1],rgb_est[st,2],'bd', label = 'Rendered')\n",
    "            ax[2].legend()\n",
    "    if out == 'rfl_est':\n",
    "        return rfl_est\n",
    "    elif out == 'rfl_est,xyz_est':\n",
    "        return rfl_est, xyz_est\n",
    "    else:\n",
    "        return eval(out)\n",
    "\n",
    "\n",
    "\n",
    "def render_image(img = None, spd = None, rfl = None, out = 'img_hyp', \\\n",
    "                 refspd = None, D = None, cieobs = _CIEOBS, \\\n",
    "                 cspace = 'xyz', cspace_tf = {}, CSF = None,\\\n",
    "                 interp_type = 'nd', k_neighbours = 4, show = True,\n",
    "                 verbosity = 0, show_ref_img = True,\\\n",
    "                 stack_test_ref = 12,\\\n",
    "                 write_to_file = None,\\\n",
    "                 csf_based_rgb_rounding = _ROUNDING):\n",
    "    \n",
    "    # Get image:\n",
    "    #imread = lambda x: plt.imread(x) #matplotlib.pyplot\n",
    "   \n",
    "    if img is not None:\n",
    "        if isinstance(img,str):\n",
    "            img = plt.imread(img).copy() # use matplotlib.pyplot's imread\n",
    "    else:\n",
    "        img = plt.imread(_HYPSPCIM_DEFAULT_IMAGE).copy()\n",
    "    \n",
    "    if img.dtype == np.uint8: \n",
    "        img = img/255\n",
    "    elif img.dtype == np.uint16:\n",
    "        img = img/(2**16-1)\n",
    "    elif (img.dtype == np.float64) | (img.dtype == np.float32):\n",
    "        pass\n",
    "    else:\n",
    "        raise Exception('img input must be None, string or ndarray of (max = 1) float32 or float64 !')\n",
    "    if img.max() > 1.0: raise Exception('img input must be None, string or ndarray of (max = 1) float32 or float64 !')\n",
    "    \n",
    "    \n",
    "    # Convert to 2D format:\n",
    "    rgb = img.reshape(img.shape[0]*img.shape[1],3) # *1.0: make float\n",
    "    rgb[rgb==0] = _EPS # avoid division by zero for pure blacks.\n",
    "\n",
    "    \n",
    "    # Get unique rgb values and positions:\n",
    "    rgb_u, rgb_indices = np.unique(rgb, return_inverse=True, axis = 0)\n",
    "\n",
    "    \n",
    "    # get rfl set:\n",
    "    if rfl is None: # use IESTM30['4880'] set \n",
    "        rfl = _CRI_RFL['ies-tm30']['4880']['5nm']\n",
    "    wlr = rfl[0] # spectral reflectance set determines wavelength range for estimation (xyz_to_rfl())\n",
    "        \n",
    "    # get Ref spd:\n",
    "    if refspd is None:\n",
    "        refspd = _CIE_ILLUMINANTS['D65'].copy()\n",
    "    refspd = cie_interp(refspd, wlr, kind = 'linear') # force spd to same wavelength range as rfl\n",
    "\n",
    "\n",
    "    # Convert rgb_u to xyz and lab-type values under assumed refspd:\n",
    "    if CSF is None:\n",
    "        xyz_wr = spd_to_xyz(refspd, cieobs = cieobs, relative = True)\n",
    "        xyz_ur = colortf(rgb_u*255, tf = 'srgb>xyz')\n",
    "    else:\n",
    "        xyz_ur = rgb_u # for input in xyz_to_rfl (when CSF is not None: this functions assumes input is indeed rgb !!!)\n",
    "    \n",
    "    # Estimate rfl's for xyz_ur:\n",
    "    rfl_est, xyzri = xyz_to_rfl(xyz_ur, rfl = rfl, out = 'rfl_est,xyz_est', \\\n",
    "                 refspd = refspd, D = D, cieobs = cieobs, \\\n",
    "                 cspace = cspace, cspace_tf = cspace_tf, CSF = CSF,\\\n",
    "                 interp_type = interp_type, k_neighbours = k_neighbours, \n",
    "                 verbosity = verbosity,\n",
    "                 csf_based_rgb_rounding = csf_based_rgb_rounding)\n",
    "\n",
    "    # Get default test spd if none supplied:\n",
    "    if spd is None:\n",
    "        spd = _CIE_ILLUMINANTS['F4']\n",
    "        \n",
    "    if CSF is None:\n",
    "        # calculate xyz values under test spd:\n",
    "        xyzti, xyztw = spd_to_xyz(spd, rfl = rfl_est, cieobs = cieobs, out = 2)\n",
    "    \n",
    "        # Chromatic adaptation from test spd to refspd:\n",
    "        if D is not None:\n",
    "            xyzti = cat.apply(xyzti, xyzw1 = xyztw, xyzw2 = xyz_wr, D = D)\n",
    "    \n",
    "        # Convert xyzti under test spd to srgb:\n",
    "        rgbti = colortf(xyzti, tf = 'srgb')/255\n",
    "    else:\n",
    "        # Calculate rgb coordinates from camera sensitivity functions under spd:\n",
    "        rgbti = rfl_to_rgb(rfl_est, spd = spd, CSF = CSF, wl = None) \n",
    "        \n",
    "         # Chromatic adaptation from test spd to refspd:\n",
    "        if D is not None:\n",
    "            white = np.ones_like(spd)\n",
    "            white[0] = spd[0]\n",
    "            rgbwr = rfl_to_rgb(white, spd = refspd, CSF = CSF, wl = None)\n",
    "            rgbwt = rfl_to_rgb(white, spd = spd, CSF = CSF, wl = None)\n",
    "            rgbti = cat.apply_vonkries2(rgbti,rgbwt,rgbwr,xyzw0=np.array([[1.0,1.0,1.0]]), in_type='rgb',out_type= 'rgb',D=1)\n",
    "        \n",
    "    \n",
    "    # Reconstruct original locations for rendered image rgbs:\n",
    "    img_ren = rgbti[rgb_indices]\n",
    "    img_ren.shape = img.shape # reshape back to 3D size of original\n",
    "    img_ren = img_ren\n",
    "    \n",
    "    # For output:\n",
    "    if show_ref_img == True:\n",
    "        rgb_ref = colortf(xyzri, tf = 'srgb')/255 if (CSF is None) else xyzri # if CSF not None: xyzri contains rgbri !!!\n",
    "        img_ref = rgb_ref[rgb_indices]\n",
    "        img_ref.shape = img.shape # reshape back to 3D size of original\n",
    "        img_str = 'Rendered (under ref. spd)'\n",
    "        img = img_ref\n",
    "    else:\n",
    "        img_str = 'Original'\n",
    "        img = img\n",
    "       \n",
    "    \n",
    "    if (stack_test_ref > 0) | show == True:\n",
    "        if stack_test_ref == 21:\n",
    "            img_original_rendered = np.vstack((img_ren,np.ones((4,img.shape[1],3)),img))\n",
    "            img_original_rendered_str = 'Rendered (under test spd)\\n ' + img_str \n",
    "        elif stack_test_ref == 12:\n",
    "            img_original_rendered = np.hstack((img_ren,np.ones((img.shape[0],4,3)),img))\n",
    "            img_original_rendered_str = 'Rendered (under test spd) | ' + img_str \n",
    "        elif stack_test_ref == 1:\n",
    "            img_original_rendered = img_ren\n",
    "            img_original_rendered_str = 'Rendered (under test spd)' \n",
    "        elif stack_test_ref == 2:\n",
    "            img_original_rendered = img\n",
    "            img_original_rendered_str = img_str\n",
    "        elif stack_test_ref == 0:\n",
    "            img_original_rendered = img_ren\n",
    "            img_original_rendered_str =  'Rendered (under test spd)' \n",
    "            \n",
    "    if write_to_file is not None:\n",
    "        # Convert from RGB to BGR formatand write:\n",
    "        #print('Writing rendering results to image file: {}'.format(write_to_file))\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            imsave(write_to_file, img_original_rendered)\n",
    "            \n",
    "    if show == True:\n",
    "        # show images using pyplot.show():\n",
    "        plt.figure()\n",
    "        \n",
    "        plt.imshow(img_original_rendered)\n",
    "        plt.title(img_original_rendered_str)\n",
    "        plt.gca().get_xaxis().set_ticklabels([])\n",
    "        plt.gca().get_yaxis().set_ticklabels([])\n",
    "        \n",
    "        if stack_test_ref == 0:\n",
    "            plt.figure()\n",
    "            plt.imshow(img)\n",
    "            plt.title(img_str)\n",
    "            plt.axis('off')\n",
    "      \n",
    "    if 'img_hyp' in out.split(','):\n",
    "        # Create hyper_spectral image:\n",
    "        rfl_image_2D = rfl_est[rgb_indices+1,:] # create array with all rfls required for each pixel\n",
    "        img_hyp = rfl_image_2D.reshape(img.shape[0],img.shape[1],rfl_image_2D.shape[1])\n",
    "\n",
    "\n",
    "    # Setup output:\n",
    "    if out == 'img_hyp':\n",
    "        return img_hyp\n",
    "    elif out == 'img_ren':\n",
    "        return img_ren\n",
    "    else:\n",
    "        return eval(out)\n",
    "\n",
    "\n",
    "def rfl_to_rgb(rfl, spd = None, CSF = None, wl = None, normalize_to_white = True):\n",
    "\n",
    "    rfl_cp = rfl.copy()\n",
    "    if (wl is None): \n",
    "        wl = rfl_cp[0] \n",
    "        rfl_cp = rfl_cp[1:]\n",
    "    wlr = getwlr(wl)\n",
    "    if spd is not None:\n",
    "        spd = cie_interp(spd,wlr,kind='linear')[1:]\n",
    "    else:\n",
    "        spd = np.ones_like(wlr)\n",
    "    if CSF is None: CSF = _CSF_NIKON_D700\n",
    "    CSF = cie_interp(CSF,wlr,kind='linear')\n",
    "    CSF[1:] = CSF[1:]*spd\n",
    "    rgb = rfl_cp @ CSF[1:].T \n",
    "    if normalize_to_white:\n",
    "        white = np.ones_like(spd)\n",
    "        white = white/white.sum()*spd.sum()\n",
    "        rgbw = white @ CSF[1:].T  \n",
    "        rgb = rgb/rgbw.max(axis = 0,keepdims=True) \n",
    "    \n",
    "    return rgb\n",
    "\n",
    "    \n",
    "    \n",
    "def hsi_to_rgb(hsi, spd = None, cieobs = _CIEOBS, srgb = False, \n",
    "               linear_rgb = False, CSF = None, normalize_to_white = True, \n",
    "               wl = [380,780,1]):\n",
    "    \n",
    "    if spd is None:\n",
    "        spd = _CIE_E.copy()\n",
    "    wlr = getwlr(wl)\n",
    "    spd = cie_interp(spd,wl,kind='linear')\n",
    "    \n",
    "    hsi_2d = np.reshape(hsi,(hsi.shape[0]*hsi.shape[1],hsi.shape[2]))\n",
    "    if srgb:\n",
    "        xyz = spd_to_xyz(spd, cieobs = cieobs, relative = True, rfl = np.vstack((wlr,hsi_2d)))\n",
    "        gamma = 1 if linear_rgb else 2.4\n",
    "        rgb = xyz_to_srgb(xyz, gamma = gamma, use_linear_part = not linear_rgb)/255\n",
    "    else:\n",
    "        if CSF is None: CSF = _CSF_NIKON_D700\n",
    "        rgb = rfl_to_rgb(hsi_2d, spd = spd, CSF = CSF, wl = wl, normalize_to_white = normalize_to_white)        \n",
    "    return np.reshape(rgb,(hsi.shape[0],hsi.shape[1],3))\n",
    "\n",
    "       \n",
    "def get_superresolution_hsi(lrhsi, hrci, CSF, wl = [380,780,1], csf_based_rgb_rounding = _ROUNDING,\n",
    "                            interp_type = 'nd', k_neighbours = 4, verbosity = 0):\n",
    "    \n",
    "    wlr = getwlr(wl)\n",
    "    eew = np.vstack((wlr,np.ones_like(wlr)))\n",
    "    lrhsi_2d = np.vstack((wlr,np.reshape(lrhsi,(lrhsi.shape[0]*lrhsi.shape[1],lrhsi.shape[2])))) # create 2D rfl database\n",
    "    if CSF is None: CSF = _CSF_NIKON_D700\n",
    "    hrhsi = render_image(hrci, spd = eew,\n",
    "                         refspd = eew, rfl = lrhsi_2d, D = None,\n",
    "                         interp_type = interp_type, k_neighbours = k_neighbours,\n",
    "                         verbosity = verbosity, show = bool(verbosity),\n",
    "                         CSF = CSF, csf_based_rgb_rounding = csf_based_rgb_rounding) # render HR-hsi from HR-ci using LR-HSI rfls as database        \n",
    "    return hrhsi\n",
    "\n",
    "\n",
    "def illuminanceEstimation(input_img):\n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    "    # Example / test code for HSI simulation and rendering:\n",
    "    #--------------------------------------------------------------------------\n",
    "    # plt.close('all')\n",
    "    # from luxpy.toolboxes import spdbuild as spb\n",
    "    # S = spb.spd_builder(peakwl = [460,525,590],fwhm=[20,40,20],target=4000, tar_type = 'cct') \n",
    "    # img = _HYPSPCIM_DEFAULT_IMAGE\n",
    "    # img_hyp,img_ren = render_image(img = img, \n",
    "    #                                 cspace = 'Yuv',interp_type='nd',\n",
    "    #                                 spd = S, D=1, \n",
    "    #                                 show_ref_img = True,\n",
    "    #                                 stack_test_ref = 21,\n",
    "    #                                 out='img_hyp,img_ren',\n",
    "    #                                 write_to_file = 'test.jpg') \n",
    "    # raise Exception('')\n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    "    # Example / test code for super resolution:\n",
    "    #--------------------------------------------------------------------------\n",
    "    \n",
    "    np.random.seed(1)    \n",
    "    \n",
    "    # Set some default parameters:\n",
    "    #----------------------------\n",
    "    load_hsi = False # If True: load hrci and hrhsi from npy-file.\n",
    "    file = input_img\n",
    "\n",
    "    cieobs = '1931_2' # CIE CMF set\n",
    "    linear_rgb = 1 # only used when srgb in hsi_to_rgb == True !!!\n",
    "    verbosity = 0\n",
    "    \n",
    "    # Create HR-rgb image and HR-HSI for code testing: \n",
    "    #---------------------------------------------------\n",
    "    # get an image:\n",
    "    im = imageio.v2.imread(file)/255\n",
    "    \n",
    "    # rescale to n x dimensions of typical hyperspectral camera:\n",
    "    n = 2 # downscale factor\n",
    "    w, h = 1280, 960\n",
    "    cr,cc = np.array(im.shape[:2])//2\n",
    "    crop = lambda im,cr,cc,h,w:im[(cr-h//2):(cr+h//2),(cc-w//2):(cc+w//2),:].copy()\n",
    "    im = crop(im,cr,cc,h*n,w*n)\n",
    "#     print('New image shape:',im.shape)\n",
    "    \n",
    "    # simulate HR hyperspectral image:\n",
    "    hrhsi = render_image(im,show=False)\n",
    "    wlr = getwlr([380,780,1]) #  = wavelength range of default TM30 rfl set\n",
    "    wlr = wlr[20:-80:10] # wavelength range from 400nm-700nm every 10 nm\n",
    "    hrhsi = hrhsi[...,20:-80:10] # wavelength range from 400nm-700nm every 10 nm\n",
    "#     print('Simulated HR-HSI shape:',hrhsi.shape)\n",
    "    # np.save(file[:-4]+'.npy',{'hrhsi':hrhsi,'im':im, 'wlr':wlr})\n",
    "    \n",
    "    # Illumination spectrum of HSI:    \n",
    "    eew = np.vstack((wlr,np.ones_like(wlr)))\n",
    "        \n",
    "    # Create fig and axes for plots:\n",
    "    if verbosity > 0: fig, axs = plt.subplots(1,3)\n",
    "    \n",
    "    # convert HR hsi to HR rgb image:\n",
    "    hrci = hsi_to_rgb(hrhsi, spd = eew, cieobs = cieobs, wl = wlr, linear_rgb = linear_rgb)\n",
    "    if verbosity > 0:  axs[0].imshow(hrci)\n",
    "    \n",
    "    # create LR hsi image for testing:\n",
    "    dl = n \n",
    "    lrhsi = hrhsi[::dl,::dl,:]\n",
    "#     print('Simulated LR-HSI shape:',lrhsi.shape)\n",
    "    \n",
    "    # convert LR hsi to LR rgb image:\n",
    "    lrci = hsi_to_rgb(lrhsi, spd = eew, cieobs = cieobs, wl = wlr,linear_rgb = linear_rgb)\n",
    "    if verbosity > 0:  axs[1].imshow(lrci)\n",
    "    \n",
    "    # # Perform rgb guided super-resolution:\n",
    "    #hrci = lrci # for testing of estimation code\n",
    "    tic = time.time()\n",
    "    hrhsi_est = get_superresolution_hsi(lrhsi, hrci, CSF = _CSF_NIKON_D700, wl = wlr)\n",
    "#     print('Elapsed time (s): {:1.4f}'.format(time.time() - tic))\n",
    "    hrci_est = hsi_to_rgb(hrhsi_est, spd = eew, cieobs = cieobs, wl = wlr, linear_rgb = linear_rgb)\n",
    "\n",
    "    if verbosity > 0:  axs[2].imshow(hrci_est)\n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    "    # Plot some rfl to visually evaluate estimation accuracy:\n",
    "    \n",
    "    hsi_rmse = np.linalg.norm(hrhsi-hrhsi_est)/np.array(hrhsi.shape[:2]).prod()**0.5\n",
    "#     print('RMSE(ground-truth,estimate): {:1.4f}'.format(hsi_rmse))\n",
    "    \n",
    "    global illum_out\n",
    "    return illum_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9a8d5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T08:02:18.678133Z",
     "start_time": "2023-11-25T08:02:18.618266Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import URetinexNet\n",
    "\n",
    "def one2three(x):\n",
    "    return torch.cat([x, x, x], dim=1).to(x)\n",
    "\n",
    "class Inference(nn.Module):\n",
    "    #Class Inference Methods\n",
    "    def __init__(self, opts):\n",
    "        super().__init__()\n",
    "        self.opts = opts\n",
    "        # loading decomposition model \n",
    "        self.model_Decom_low = Decom()\n",
    "        self.model_Decom_low = load_initialize(self.model_Decom_low, self.opts.Decom_model_low_path)\n",
    "        # loading R; old_model_opts; and L model\n",
    "        self.unfolding_opts, self.model_R, self.model_L= load_unfolding(self.opts.unfolding_model_path)\n",
    "        # loading adjustment model\n",
    "        self.adjust_model = load_adjustment(self.opts.adjust_model_path)\n",
    "        self.P = P()\n",
    "        self.Q = Q()\n",
    "\n",
    "        transform = [\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "        self.transform = transforms.Compose(transform)\n",
    "#         print(self.model_Decom_low)\n",
    "#         print(self.model_R)\n",
    "#         print(self.model_L)\n",
    "#         print(self.adjust_model)\n",
    "        #time.sleep(8)\n",
    "\n",
    "    def unfolding(self, input_low_img):\n",
    "        for t in range(self.unfolding_opts.round):      \n",
    "            if t == 0: # initialize R0, L0\n",
    "                P, Q = self.model_Decom_low(input_low_img)\n",
    "            else: # update P and Q\n",
    "                w_p = (self.unfolding_opts.gamma + self.unfolding_opts.Roffset * t)\n",
    "                w_q = (self.unfolding_opts.lamda + self.unfolding_opts.Loffset * t)\n",
    "                P = self.P(I=input_low_img, Q=Q, R=R, gamma=w_p)\n",
    "                Q = self.Q(I=input_low_img, P=P, L=L, lamda=w_q) \n",
    "            R = self.model_R(r=P, l=Q)\n",
    "            L = self.model_L(l=Q)\n",
    "        return R, L\n",
    "    \n",
    "    def lllumination_adjust(self, L, ratio):\n",
    "        ratio = torch.ones(L.shape) * self.opts.ratio\n",
    "        return self.adjust_model(l=L, alpha=ratio)\n",
    "    \n",
    "    def forward(self, input_low_img):\n",
    "        if torch.cuda.is_available():\n",
    "            input_low_img = input_low_img\n",
    "        with torch.no_grad():\n",
    "            start = time.time()  \n",
    "            R, L = self.unfolding(input_low_img)\n",
    "            High_L = self.lllumination_adjust(L, self.opts.ratio)\n",
    "            I_enhance = High_L * R\n",
    "            p_time = (time.time() - start)\n",
    "        return I_enhance, p_time\n",
    "\n",
    "    def run(self, low_img_path):\n",
    "        file_name = os.path.basename(self.opts.img_path)\n",
    "        name = file_name.split('.')[0]\n",
    "        low_img = self.transform(Image.open(low_img_path)).unsqueeze(0)\n",
    "        enhance, p_time = self.forward(input_low_img=low_img)\n",
    "        if not os.path.exists(self.opts.output):\n",
    "            os.makedirs(self.opts.output)\n",
    "        save_path = os.path.join(self.opts.output, file_name.replace(name, \"%s_URetinexNet\"%(name)))\n",
    "        np_save_TensorImg(enhance, save_path)  \n",
    "        print(\"================================= time for %s: %f============================\"%(file_name, p_time))\n",
    "\n",
    "#         add to own function for input of img path\n",
    "def retinexImplement(img, outPath):\n",
    "    parser = argparse.ArgumentParser(description='Configure')\n",
    "    \n",
    "    # specify your data path here!\n",
    "    parser.add_argument('--img_path', type=str, default=img)\n",
    "    parser.add_argument('--output', type=str, default=outPath)\n",
    "    # ratio are recommended to be 3-5, bigger ratio will lead to over-exposure \n",
    "    parser.add_argument('--ratio', type=int, default=2)\n",
    "    # model path\n",
    "    parser.add_argument('--Decom_model_low_path', type=str, default=\"./URetinex_Net/ckpt/init_low.pth\")\n",
    "    parser.add_argument('--unfolding_model_path', type=str, default=\"./URetinex_Net/ckpt/unfolding.pth\")\n",
    "    parser.add_argument('--adjust_model_path', type=str, default=\"./URetinex_Net/ckpt/L_adjust.pth\")\n",
    "    parser.add_argument('--gpu_id', type=int, default=0)\n",
    "    \n",
    "#     opts = parser.parse_args() change parse_args() to parse_known_args\n",
    "    opts, _ = parser.parse_known_args()\n",
    "#     for k, v in vars(opts).items():\n",
    "#         print(k, v)\n",
    "    \n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(opts.gpu_id)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = Inference(opts)\n",
    "        print(\"CUDA (GPU) is available\")\n",
    "    else:\n",
    "        print(\"CUDA (GPU) is not available. Loading the model on CPU...\")\n",
    "        model = Inference(opts).to(torch.device('cpu'))\n",
    "    \n",
    "#    \n",
    "    model.run(opts.img_path)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5abb729",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T08:02:18.708869Z",
     "start_time": "2023-11-25T08:02:18.682122Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ===================== IMPORT/EXPORT FUNCTIONS =====================\n",
    "\n",
    "def exportTrain(trainFileNames, trainLandmarks, trainIllumsRaw, trainIllumsRet):\n",
    "    train_data = \"train_data.csv\"\n",
    "    \n",
    "    # Check if the file already exists\n",
    "    file_exists = Path(train_data).is_file()\n",
    "\n",
    "    with open(train_data, 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        # Write the column headers only if the file is newly created\n",
    "        if not file_exists:\n",
    "            writer.writerow(['Filename', 'LandmarksRaw', 'IlluminanceRaw', 'IlluminanceRet'])\n",
    "\n",
    "        row = [trainFileNames, trainLandmarks, trainIllumsRaw, trainIllumsRet]\n",
    "        writer.writerow(row)\n",
    "\n",
    "def exportEval(evalFileNames, evalLandmarks, evalIllumsRaw, evalIllumsRet):\n",
    "    eval_data = \"eval_data.csv\"\n",
    "\n",
    "    # Check if the file already exists\n",
    "    file_exists = Path(eval_data).is_file()\n",
    "\n",
    "    with open(eval_data, 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        # Write the column headers only if the file is newly created\n",
    "        if not file_exists:\n",
    "            writer.writerow(['Filename', 'LandmarksRaw', 'IlluminanceRaw', 'IlluminanceRet'])\n",
    "\n",
    "        row = [evalFileNames, evalLandmarks, evalIllumsRaw, evalIllumsRet]\n",
    "        writer.writerow(row)\n",
    "\n",
    "def exportTest(testFileNames, testDrowsiness):\n",
    "    test_data = \"test_data.csv\"\n",
    "    \n",
    "    # Check if the file already exists\n",
    "    file_exists = Path(test_data).is_file()\n",
    "\n",
    "    with open(test_data, 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        # Write the column headers (optional)\n",
    "        if not file_exists:\n",
    "            writer.writerow(['Filename', 'Drowsiness'])\n",
    "\n",
    "        row = [testFileNames, testDrowsiness]\n",
    "        writer.writerow(row)\n",
    "\n",
    "def importTrain():\n",
    "    with open(\"train_data.csv\", 'r') as file:\n",
    "        # Create a CSV reader object\n",
    "        csv_reader = csv.reader(file)\n",
    "\n",
    "        # Skip the header row\n",
    "        next(csv_reader)\n",
    "\n",
    "        # Iterate through the rows and append data to the respective lists\n",
    "        for row in csv_reader:\n",
    "            trainFileNames.append(row[0])\n",
    "\n",
    "            # Assuming the dictionary is stored as a string in the CSV file\n",
    "            data_dict = ast.literal_eval(row[1])\n",
    "\n",
    "            # Append the dictionary to the list\n",
    "            trainLandmarks.append(data_dict)\n",
    "            \n",
    "            trainIllumsRaw.append(float(row[2]))\n",
    "            trainIllumsRet.append(float(row[3]))\n",
    "\n",
    "def importEval():\n",
    "    with open(\"eval_data.csv\", 'r') as file:\n",
    "        # Create a CSV reader object\n",
    "        csv_reader = csv.reader(file)\n",
    "\n",
    "        # Skip the header row\n",
    "        next(csv_reader)\n",
    "\n",
    "        # Iterate through the rows and append data to the respective lists\n",
    "        for row in csv_reader:\n",
    "            evalFileNames.append(row[0])\n",
    "            \n",
    "            # Assuming the dictionary is stored as a string in the CSV file\n",
    "            data_dict = ast.literal_eval(row[1])\n",
    "\n",
    "            # Append the dictionary to the list\n",
    "            evalLandmarks.append(data_dict)\n",
    "            \n",
    "            evalIllumsRaw.append(float(row[2]))\n",
    "            evalIllumsRet.append(float(row[3]))\n",
    "            \n",
    "def importTest():\n",
    "    with open(\"test_data.csv\", 'r') as file:\n",
    "        # Create a CSV reader object\n",
    "        csv_reader = csv.reader(file)\n",
    "\n",
    "        # Skip the header row\n",
    "        next(csv_reader)\n",
    "\n",
    "        # Iterate through the rows and append data to the respective lists\n",
    "        for row in csv_reader:\n",
    "            testFileNames.append(row[0])\n",
    "\n",
    "            # Append the dictionary to the list\n",
    "            testDrowsiness.append(float(row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadb8e9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T08:02:18.724339Z",
     "start_time": "2023-11-25T08:02:18.711369Z"
    },
    "code_folding": [
     0,
     14
    ]
   },
   "outputs": [],
   "source": [
    "def historyToCsv():\n",
    "    # convert the history.history dict to a pandas DataFrame:     \n",
    "    hist_df = pd.DataFrame(history.history) \n",
    "    \n",
    "    current_datetime = datetime.now()\n",
    "\n",
    "    # Convert the datetime object to a string\n",
    "    filename_friendly_datetime_string = current_datetime.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    # save to csv: \n",
    "    hist_csv_file = 'history' + filename_friendly_datetime_string + '.csv'\n",
    "    with open(hist_csv_file, mode='w') as f:\n",
    "        hist_df.to_csv(f)\n",
    "\n",
    "def csvToHistory(csv_filename):\n",
    "    # Read the CSV file into a pandas DataFrame\n",
    "    hist_df = pd.read_csv(csv_filename, index_col=0)\n",
    "\n",
    "    # Convert the DataFrame to a dictionary\n",
    "    history_dict = hist_df.to_dict(orient='list')\n",
    "\n",
    "    return history_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "679d8739",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T09:31:29.930129Z",
     "start_time": "2023-11-25T09:31:29.061809Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MTCNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# function to setup entire dataset to be used for training, eval, and testing\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m detector \u001b[38;5;241m=\u001b[39m \u001b[43mMTCNN\u001b[49m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MTCNN' is not defined"
     ]
    }
   ],
   "source": [
    "# function to setup entire dataset to be used for training, eval, and testing\n",
    "\n",
    "detector = MTCNN()\n",
    "\n",
    "# evalData_prep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e4c5e20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T04:11:30.138564Z",
     "start_time": "2023-11-28T04:11:29.648443Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flattened_features (Flatten  (None, 25088)            0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " embedding (Dense)           (None, 512)               12845568  \n",
      "                                                                 \n",
      " additional_dense1 (Dense)   (None, 64)                32832     \n",
      "                                                                 \n",
      " additional_dense2 (Dense)   (None, 64)                4160      \n",
      "                                                                 \n",
      " landmark_output (Dense)     (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,597,898\n",
      "Trainable params: 27,597,898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# ===================== MULTITASK MODEL SETUP =====================\n",
    "base_model = tf.keras.applications.VGG16(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "\n",
    "flattened_features = tf.keras.layers.Flatten(name='flattened_features')(base_model.output)\n",
    "\n",
    "embedding_layer = tf.keras.layers.Dense(512, activation='linear', name='embedding')(flattened_features)\n",
    "\n",
    "additional_dense_layer1 = tf.keras.layers.Dense(64, name='additional_dense1')(embedding_layer)\n",
    "additional_dense_layer2 = tf.keras.layers.Dense(64, name='additional_dense2')(additional_dense_layer1)\n",
    "# additional_dense_layer3 = tf.keras.layers.Dense(64, activation='relu', name='additional_dense3')(flattened_features)\n",
    "\n",
    "landmarks = tf.keras.layers.Dense(10, activation='linear', name='landmark_output')(additional_dense_layer2)\n",
    "# illum = tf.keras.layers.Dense(1, activation='linear', name='previous_illuminance_output')(additional_dense_layer2)\n",
    "\n",
    "# Reshape layer to the desired shape\n",
    "# reshaped_features = tf.keras.layers.Reshape((8, 8, 8))(embedding_layer)\n",
    "\n",
    "# Upsampling layers\n",
    "# upsample1 = tf.keras.layers.UpSampling2D(size=(7, 7))(reshaped_features)\n",
    "# upsample2 = tf.keras.layers.UpSampling2D(size=(2, 2))(upsample1)\n",
    "# upsample3 = tf.keras.layers.UpSampling2D(size=(2, 2))(upsample2)\n",
    "\n",
    "# retIllum = tf.keras.layers.Conv2D(3, kernel_size=(3, 3), activation='relu', padding='same', name='image_retinex_output')(upsample3)\n",
    "\n",
    "task_outputs = None\n",
    "\n",
    "task_outputs = [landmarks]\n",
    "\n",
    "multi_task_model = tf.keras.Model(inputs=base_model.input, outputs=task_outputs)\n",
    "\n",
    "# Compile the model with specific loss functions and metrics for each task\n",
    "multi_task_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={\n",
    "        'landmark_output': 'mean_squared_error'\n",
    "#         'previous_illuminance_output': 'mean_squared_error',\n",
    "#         'image_retinex_output': 'mean_squared_error'\n",
    "    },\n",
    "    metrics = {\n",
    "        'landmark_output': ['mse', \"mae\", tf.keras.metrics.Precision(name='precision'), tf.keras.metrics.Recall(name='recall'), tfa.metrics.F1Score(num_classes=2, threshold=0.5)]\n",
    "#         'previous_illuminance_output': ['mse', \"mae\"],\n",
    "#         'image_retinex_output': ['mse', \"mae\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "# Summary of the multi-task model\n",
    "multi_task_model.summary()\n",
    "# plot_model(multi_task_model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e8ac7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T08:40:57.448205Z",
     "start_time": "2023-11-15T08:40:57.440869Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ===================== CALLBACK FUNCTION =====================\n",
    "class InputOutputShapeCallback(Callback):\n",
    "    def __init__(self, input_data, task_names):\n",
    "        super().__init__()\n",
    "        self.input_data = input_data\n",
    "        self.task_names = task_names\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # Get the model's first layer (input layer) and last layer (output layer)\n",
    "        input_layer = self.model.layers[0]\n",
    "        output_layers = self.model.layers[1:]  # Exclude the input layer\n",
    "\n",
    "        # Print the shapes of the input and output tensors of the model\n",
    "        print(f\"Input Data Shape: {self.input_data.shape}\")\n",
    "        print(f\"Input Layer Shape: {input_layer.input_shape}\")\n",
    "\n",
    "        # Print the shapes of the output tensors for each task\n",
    "        for task_name, output_layer in zip(self.task_names, output_layers):\n",
    "            print(f\"Output Layer Shape for {task_name}: {output_layer.output_shape}\")\n",
    "\n",
    "# List of task names\n",
    "task_names = ['landmark_output', 'previous_illuminance_output', 'image_retinex_output']\n",
    "\n",
    "# Create an instance of the callback with input data and task names\n",
    "shape_callback = InputOutputShapeCallback(input_data=imageTensor, task_names=task_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52600a91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T18:08:51.920238Z",
     "start_time": "2023-11-27T18:08:50.963809Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ===================== DATA PLOTS =====================\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Plot training & validation loss values\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# ===================== DATA PLOTS =====================\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['landmark_output_loss'])\n",
    "plt.plot(history.history['val_landmark_output_loss'])\n",
    "plt.title('Landmarks Task Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['landmark_output_mse'])\n",
    "plt.plot(history.history['val_landmark_output_mse'])\n",
    "plt.title('Landmarks Task MSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['landmark_output_mae'])\n",
    "plt.plot(history.history['val_landmark_output_mae'])\n",
    "plt.title('Landmarks Task MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['previous_illuminance_output_loss'])\n",
    "plt.plot(history.history['val_previous_illuminance_output_loss'])\n",
    "plt.title('Illuminance Task Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['previous_illuminance_output_mse'])\n",
    "plt.plot(history.history['val_previous_illuminance_output_mse'])\n",
    "plt.title('Illuminance Task MSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['previous_illuminance_output_mae'])\n",
    "plt.plot(history.history['val_previous_illuminance_output_mae'])\n",
    "plt.title('Illuminance Task MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['image_retinex_output_loss'])\n",
    "plt.plot(history.history['val_image_retinex_output_loss'])\n",
    "plt.title('Retinex Task Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['image_retinex_output_mse'])\n",
    "plt.plot(history.history['val_image_retinex_output_mse'])\n",
    "plt.title('Landmarks Task MSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['image_retinex_output_mae'])\n",
    "plt.plot(history.history['val_image_retinex_output_mae'])\n",
    "plt.title('Landmarks Task MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "211280a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T04:11:12.569346Z",
     "start_time": "2023-11-28T04:11:12.438424Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " block1_conv1 (Conv2D)          (None, 224, 224, 64  1792        ['input_6[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1_conv2 (Conv2D)          (None, 224, 224, 64  36928       ['block1_conv1[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1_pool (MaxPooling2D)     (None, 112, 112, 64  0           ['block1_conv2[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block2_conv1 (Conv2D)          (None, 112, 112, 12  73856       ['block1_pool[0][0]']            \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " block2_conv2 (Conv2D)          (None, 112, 112, 12  147584      ['block2_conv1[0][0]']           \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " block2_pool (MaxPooling2D)     (None, 56, 56, 128)  0           ['block2_conv2[0][0]']           \n",
      "                                                                                                  \n",
      " block3_conv1 (Conv2D)          (None, 56, 56, 256)  295168      ['block2_pool[0][0]']            \n",
      "                                                                                                  \n",
      " block3_conv2 (Conv2D)          (None, 56, 56, 256)  590080      ['block3_conv1[0][0]']           \n",
      "                                                                                                  \n",
      " block3_conv3 (Conv2D)          (None, 56, 56, 256)  590080      ['block3_conv2[0][0]']           \n",
      "                                                                                                  \n",
      " block3_pool (MaxPooling2D)     (None, 28, 28, 256)  0           ['block3_conv3[0][0]']           \n",
      "                                                                                                  \n",
      " block4_conv1 (Conv2D)          (None, 28, 28, 512)  1180160     ['block3_pool[0][0]']            \n",
      "                                                                                                  \n",
      " block4_conv2 (Conv2D)          (None, 28, 28, 512)  2359808     ['block4_conv1[0][0]']           \n",
      "                                                                                                  \n",
      " block4_conv3 (Conv2D)          (None, 28, 28, 512)  2359808     ['block4_conv2[0][0]']           \n",
      "                                                                                                  \n",
      " block4_pool (MaxPooling2D)     (None, 14, 14, 512)  0           ['block4_conv3[0][0]']           \n",
      "                                                                                                  \n",
      " block5_conv1 (Conv2D)          (None, 14, 14, 512)  2359808     ['block4_pool[0][0]']            \n",
      "                                                                                                  \n",
      " block5_conv2 (Conv2D)          (None, 14, 14, 512)  2359808     ['block5_conv1[0][0]']           \n",
      "                                                                                                  \n",
      " block5_conv3 (Conv2D)          (None, 14, 14, 512)  2359808     ['block5_conv2[0][0]']           \n",
      "                                                                                                  \n",
      " block5_pool (MaxPooling2D)     (None, 7, 7, 512)    0           ['block5_conv3[0][0]']           \n",
      "                                                                                                  \n",
      " flattened_features (Flatten)   (None, 25088)        0           ['block5_pool[0][0]']            \n",
      "                                                                                                  \n",
      " embedding (Dense)              (None, 512)          12845568    ['flattened_features[0][0]']     \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 16, 16, 2)    0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 16, 16, 2)    6           ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " softmax (Softmax)              (None, 16, 16, 2)    0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 16, 16, 2)    0           ['reshape[0][0]',                \n",
      "                                                                  'softmax[0][0]']                \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 2)           0           ['multiply[0][0]']               \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " drowsiness_output (Dense)      (None, 2)            6           ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 27,560,268\n",
      "Trainable params: 27,560,268\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# ===================== DROWSINESS MODEL =====================\n",
    "\n",
    "# retain weights and remove top layer\n",
    "output_layer = multi_task_model.get_layer('embedding').output\n",
    "\n",
    "drowsiness_model = Model(inputs=multi_task_model.input, outputs=output_layer)\n",
    "\n",
    "# drowsiness_model.summary()\n",
    "# tf.keras.utils.plot_model(drowsiness_model)\n",
    "from keras.utils.vis_utils import plot_model\n",
    "# plot_model(drowsiness_model, to_file='drowsinessModel_plot.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "existing_output = drowsiness_model.output\n",
    "\n",
    "reshaped_output = tf.keras.layers.Reshape((16, 16, 2))(existing_output)\n",
    "\n",
    "spatial_attention = tf.keras.layers.Conv2D(2, (1, 1), activation='sigmoid', padding='same')(reshaped_output)\n",
    "spatial_attention = tf.keras.layers.Softmax()(spatial_attention)\n",
    "output_tensor = tf.keras.layers.Multiply()([reshaped_output, spatial_attention])\n",
    "\n",
    "# Add Global Average Pooling layer\n",
    "output_tensor = tf.keras.layers.GlobalAveragePooling2D()(output_tensor)\n",
    "\n",
    "# Add output layer with two classes and softmax activation\n",
    "predictions = tf.keras.layers.Dense(2, activation='softmax', name='drowsiness_output')(output_tensor)\n",
    "\n",
    "# Create the new model with the modified top layers\n",
    "drowsiness_model = Model(inputs=drowsiness_model.input, outputs=predictions)\n",
    "\n",
    "drowsiness_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={\n",
    "        'drowsiness_output': 'binary_crossentropy'\n",
    "    },\n",
    "    metrics={\n",
    "        'drowsiness_output': [\"accuracy\", \n",
    "                            tf.keras.metrics.Precision(name='precision'),\n",
    "                            tf.keras.metrics.Recall(name='recall'),\n",
    "                            tfa.metrics.F1Score(num_classes=10, name='f1_score')]\n",
    "    }\n",
    ")\n",
    "\n",
    "drowsiness_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db3f0a4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T03:16:21.182363Z",
     "start_time": "2023-11-28T03:16:21.154807Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ===================== ORIGINAL DATA GEN CLASS =====================\n",
    "\n",
    "class CustomDataGen(tf.keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, df, X_col, y_col,\n",
    "                 batch_size,\n",
    "                 input_size=(224, 224, 3),\n",
    "                 shuffle=True):\n",
    "        \n",
    "        self.df = df.copy()\n",
    "        self.X_col = X_col\n",
    "        self.y_col = y_col\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.n = len(self.df)\n",
    "        self.n_coords = 2  # Assuming landmark coordinates are 2-dimensional\n",
    "        self.n_illuminance = 1  # Assuming a single illuminance value\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    def __get_input(self, path, target_size):\n",
    "    \n",
    "        image = tf.keras.preprocessing.image.load_img(path)\n",
    "        image_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "\n",
    "        image_arr = tf.image.resize(image_arr, (target_size[0], target_size[1])).numpy()\n",
    "\n",
    "        return image_arr / 255.\n",
    "    \n",
    "    def __get_output(self, label, output_type):\n",
    "        # Assuming output_type is 'coordinates', 'illuminance', or 'adjusted_image_path'\n",
    "        if output_type == 'coordinates':\n",
    "            # Assuming label is a string containing a dictionary-like structure\n",
    "            # Safely evaluate the string as a literal dictionary using ast.literal_eval\n",
    "            coordinates_dict = ast.literal_eval(label)\n",
    "            \n",
    "            # Extract x and y coordinates for each landmark\n",
    "            landmarks = ['left_eye', 'right_eye', 'nose', 'mouth_left', 'mouth_right']\n",
    "            coordinates_list = [coordinates_dict[landmark] for landmark in landmarks]\n",
    "            \n",
    "            # Flatten the list and convert to numpy array\n",
    "            coordinates_array = np.array([coord for landmark_coords in coordinates_list for coord in landmark_coords])\n",
    "            \n",
    "#             print(\"Shape of landmarks_array:\", coordinates_array.shape)\n",
    "            \n",
    "            # If there are exactly 10 values, return the array, otherwise raise an error\n",
    "            if len(coordinates_array) == 10:\n",
    "                return coordinates_array\n",
    "            else:\n",
    "                raise ValueError(\"Expected 10 coordinates, but found {}\".format(len(coordinates_array)))\n",
    "        elif output_type == 'illuminance':\n",
    "            # Convert the illuminance value to a float\n",
    "            return float(label)\n",
    "        elif output_type == 'adjusted_image_path':\n",
    "            # Assuming label is the path to the adjusted image\n",
    "            return self.__get_input(label, self.input_size)\n",
    "    \n",
    "    def __get_data(self, batches):\n",
    "        # Generates data containing batch_size samples\n",
    "\n",
    "        path_batch = batches[self.X_col['path']]\n",
    "        \n",
    "        coords_batch = batches[self.y_col['coordinates']]\n",
    "        illuminance_batch = batches[self.y_col['illuminance']]\n",
    "        adjusted_image_path_batch = batches[self.y_col['adjusted_image_path']]\n",
    "\n",
    "        X_batch = np.asarray([self.__get_input(x, self.input_size) for x in path_batch])\n",
    "\n",
    "        y0_batch = np.asarray([self.__get_output(y, 'coordinates') for y in coords_batch])\n",
    "        y1_batch = np.asarray([self.__get_output(y, 'illuminance') for y in illuminance_batch])\n",
    "        y2_batch = np.asarray([self.__get_output(y, 'adjusted_image_path') for y in adjusted_image_path_batch])\n",
    "\n",
    "        return X_batch, [y0_batch, y1_batch, y2_batch]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batches = self.df[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        X, y = self.__get_data(batches)\n",
    "\n",
    "        # Print a few examples of the data\n",
    "#         print(\"Sample X:\", tf.shape(X[0]))  # Print the first example in the batch\n",
    "#         print(\"Sample y[0] (landmarks):\", tf.shape(y[0][0]))  # Print the first example in the landmarks output\n",
    "#         print(\"Sample y[1] (illum):\", tf.shape(y[1][0]))  # Print the first example in the illum output\n",
    "#         print(\"Sample y[2] (adjusted_image_path):\", tf.shape(y[2][0]))  # Print the first example in the adjusted_image_path output\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07139b2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T04:05:46.037595Z",
     "start_time": "2023-11-28T04:05:46.022144Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ===================== LANDMARKS ONLY DATA GEN =====================\n",
    "\n",
    "class CustomDataGen(tf.keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, df, X_col, y_col,\n",
    "                 batch_size,\n",
    "                 input_size=(224, 224, 3),\n",
    "                 shuffle=True):\n",
    "        \n",
    "        self.df = df.copy()\n",
    "        self.X_col = X_col\n",
    "        self.y_col = y_col\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.n = len(self.df)\n",
    "        self.n_coords = 2  # Assuming landmark coordinates are 2-dimensional\n",
    "#         self.n_illuminance = 1  # Assuming a single illuminance value\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    def __get_input(self, path, target_size):\n",
    "    \n",
    "        image = tf.keras.preprocessing.image.load_img(path)\n",
    "        image_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "\n",
    "        image_arr = tf.image.resize(image_arr, (target_size[0], target_size[1])).numpy()\n",
    "\n",
    "        return image_arr / 255.\n",
    "    \n",
    "    def __get_output(self, label, output_type):\n",
    "        # Assuming output_type is 'coordinates', 'illuminance', or 'adjusted_image_path'\n",
    "        if output_type == 'coordinates':\n",
    "            # Assuming label is a string containing a dictionary-like structure\n",
    "            # Safely evaluate the string as a literal dictionary using ast.literal_eval\n",
    "            coordinates_dict = ast.literal_eval(label)\n",
    "            \n",
    "            # Extract x and y coordinates for each landmark\n",
    "            landmarks = ['left_eye', 'right_eye', 'nose', 'mouth_left', 'mouth_right']\n",
    "            coordinates_list = [coordinates_dict[landmark] for landmark in landmarks]\n",
    "            \n",
    "            # Flatten the list and convert to numpy array\n",
    "            coordinates_array = np.array([coord for landmark_coords in coordinates_list for coord in landmark_coords])\n",
    "            \n",
    "#             print(\"Shape of landmarks_array:\", coordinates_array.shape)\n",
    "            \n",
    "            # If there are exactly 10 values, return the array, otherwise raise an error\n",
    "            if len(coordinates_array) == 10:\n",
    "                return coordinates_array\n",
    "            else:\n",
    "                raise ValueError(\"Expected 10 coordinates, but found {}\".format(len(coordinates_array)))\n",
    "#         elif output_type == 'illuminance':\n",
    "#             # Convert the illuminance value to a float\n",
    "#             return float(label)\n",
    "#         elif output_type == 'adjusted_image_path':\n",
    "#             # Assuming label is the path to the adjusted image\n",
    "#             return self.__get_input(label, self.input_size)\n",
    "    \n",
    "    def __get_data(self, batches):\n",
    "        # Generates data containing batch_size samples\n",
    "\n",
    "        path_batch = batches[self.X_col['path']]\n",
    "        \n",
    "        coords_batch = batches[self.y_col['coordinates']]\n",
    "#         illuminance_batch = batches[self.y_col['illuminance']]\n",
    "#         adjusted_image_path_batch = batches[self.y_col['adjusted_image_path']]\n",
    "\n",
    "        X_batch = np.asarray([self.__get_input(x, self.input_size) for x in path_batch])\n",
    "\n",
    "        y0_batch = np.asarray([self.__get_output(y, 'coordinates') for y in coords_batch])\n",
    "#         y1_batch = np.asarray([self.__get_output(y, 'illuminance') for y in illuminance_batch])\n",
    "#         y2_batch = np.asarray([self.__get_output(y, 'adjusted_image_path') for y in adjusted_image_path_batch])\n",
    "\n",
    "        return X_batch, [y0_batch]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batches = self.df[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        X, y = self.__get_data(batches)\n",
    "\n",
    "        # Print a few examples of the data\n",
    "#         print(\"Sample X:\", tf.shape(X[0]))  # Print the first example in the batch\n",
    "#         print(\"Sample y[0] (landmarks):\", tf.shape(y[0][0]))  # Print the first example in the landmarks output\n",
    "#         print(\"Sample y[1] (illum):\", tf.shape(y[1][0]))  # Print the first example in the illum output\n",
    "#         print(\"Sample y[2] (adjusted_image_path):\", tf.shape(y[2][0]))  # Print the first example in the adjusted_image_path output\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0790f85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T06:43:18.091187Z",
     "start_time": "2023-11-28T06:43:17.171891Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ===================== DATA GEN SETUP (LANDMARKS ONLY TASK) =====================\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# path to train_data csv\u001b[39;00m\n\u001b[0;32m      4\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFilename\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/Evaluation/Evaluation/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFilename\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Define column indices or names for X and y\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# ===================== DATA GEN SETUP (LANDMARKS ONLY TASK) =====================\n",
    "\n",
    "df = pd.read_csv(\"eval_data.csv\") # path to train_data csv\n",
    "df[\"Filename\"] = \"./data/Evaluation/Evaluation/\" + df[\"Filename\"] + \".jpg\"\n",
    "\n",
    "# Define column indices or names for X and y\n",
    "X_col = {'path': 'Filename'}\n",
    "y_col = {'coordinates': 'LandmarksRaw'}\n",
    "\n",
    "# Create an instance of CustomDataGen\n",
    "train_gen = CustomDataGen(df, X_col, y_col, batch_size=32, input_size=(224, 224, 3))\n",
    "\n",
    "eval_df = pd.read_csv(\"eval_data.csv\") # path to eval_data csv\n",
    "eval_df[\"Filename\"] = \"./data/Evaluation/Evaluation/\" + eval_df[\"Filename\"] + \".jpg\"\n",
    "\n",
    "# Define column indices or names for X and y\n",
    "eval_X_col = {'path': 'Filename'}\n",
    "eval_y_col = {'coordinates': 'LandmarksRaw'}\n",
    "\n",
    "val_gen = CustomDataGen(eval_df, eval_X_col, eval_y_col, batch_size=32, input_size=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b12f56d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T04:11:37.180825Z",
     "start_time": "2023-11-28T04:11:36.224271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\michael\\miniconda3\\envs\\torch\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\michael\\miniconda3\\envs\\torch\\lib\\site-packages\\tensorflow_addons\\metrics\\f_scores.py\", line 160, in update_state  *\n        self.true_positives.assign_add(_weighted_sum(y_pred * y_true, sample_weight))\n\n    ValueError: Dimension 0 in both shapes must be equal, but are 2 and 10. Shapes are [2] and [10]. for '{{node AssignAddVariableOp_10}} = AssignAddVariableOp[dtype=DT_FLOAT](AssignAddVariableOp_10/resource, Sum_8)' with input shapes: [], [10].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_task_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_gen\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\torch\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file8os7ftpr.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filef15dd414.py:57\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__update_state\u001b[1;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fscope_1\u001b[38;5;241m.\u001b[39mret(retval__1, do_return_1)\n\u001b[1;32m---> 57\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mtrue_positives\u001b[38;5;241m.\u001b[39massign_add, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(_weighted_sum), (ag__\u001b[38;5;241m.\u001b[39mld(y_pred) \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(y_true), ag__\u001b[38;5;241m.\u001b[39mld(sample_weight)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     58\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfalse_positives\u001b[38;5;241m.\u001b[39massign_add, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(_weighted_sum), (ag__\u001b[38;5;241m.\u001b[39mld(y_pred) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(y_true)), ag__\u001b[38;5;241m.\u001b[39mld(sample_weight)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     59\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfalse_negatives\u001b[38;5;241m.\u001b[39massign_add, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(_weighted_sum), ((\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(y_pred)) \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(y_true), ag__\u001b[38;5;241m.\u001b[39mld(sample_weight)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\michael\\miniconda3\\envs\\torch\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\michael\\miniconda3\\envs\\torch\\lib\\site-packages\\tensorflow_addons\\metrics\\f_scores.py\", line 160, in update_state  *\n        self.true_positives.assign_add(_weighted_sum(y_pred * y_true, sample_weight))\n\n    ValueError: Dimension 0 in both shapes must be equal, but are 2 and 10. Shapes are [2] and [10]. for '{{node AssignAddVariableOp_10}} = AssignAddVariableOp[dtype=DT_FLOAT](AssignAddVariableOp_10/resource, Sum_8)' with input shapes: [], [10].\n"
     ]
    }
   ],
   "source": [
    "history = multi_task_model.fit(train_gen, epochs=50, validation_data=val_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c01102d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T03:32:08.577739Z",
     "start_time": "2023-11-28T03:32:07.904228Z"
    }
   },
   "outputs": [],
   "source": [
    "multi_task_model.save(\"multiTaskModel.keras\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.9 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
