{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f69663c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T18:33:37.040167Z",
     "start_time": "2023-10-23T18:33:37.036186Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from luxpy import (cat, colortf, _CIEOBS, _CIE_ILLUMINANTS, _CRI_RFL, _CIE_D65,_CIE_E,\n",
    "                   spd_to_xyz, plot_color_data, math, cie_interp, getwlr, xyz_to_srgb)\n",
    "from luxpy.utils import np, plt, sp, _PKG_PATH, _SEP, _EPS \n",
    "import warnings\n",
    "from imageio import imsave\n",
    "\n",
    "import tensorflow as tf\n",
    "from mtcnn import MTCNN\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import glob\n",
    "import cv2\n",
    "import csv\n",
    "import os\n",
    "import pydot\n",
    "import pydotplus\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43c1c7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T03:41:38.193563Z",
     "start_time": "2023-10-19T03:41:28.720711Z"
    }
   },
   "outputs": [],
   "source": [
    "pip install pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5bf89806",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T17:56:34.686855Z",
     "start_time": "2023-10-23T17:56:34.683234Z"
    }
   },
   "outputs": [],
   "source": [
    "trainLandmarks = []\n",
    "trainLandmarksRet = []\n",
    "trainFileNames = []\n",
    "trainIllumsRaw = []\n",
    "trainIllumsRet = []\n",
    "evalLandmarks = []\n",
    "evalLandmarksRet = []\n",
    "evalFileNames = []\n",
    "evalIllumsRaw = []\n",
    "evalIllumsRet = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "096c8392",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T15:56:18.864250Z",
     "start_time": "2023-10-23T15:56:18.848682Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def frame_capture(file):\n",
    "    \n",
    "    cap = cv2.VideoCapture(file)\n",
    "    \n",
    "    try:\n",
    "        if not os.path.exists('data'):\n",
    "            os.makedirs('data')\n",
    "    except OSError:\n",
    "        print('Error: Creating directory of data')\n",
    "            \n",
    "    current_frame = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "#         print(file)\n",
    "        if not ret:\n",
    "            current_frame = 0\n",
    "            break \n",
    "\n",
    "        if current_frame % 15 == 0:\n",
    "            \n",
    "            datasetDir = os.path.dirname(os.path.dirname(os.path.dirname(file)))\n",
    "            indexDataset = datasetDir.rfind('\\\\')\n",
    "            datasetDir = datasetDir[indexDataset:]\n",
    "            \n",
    "            parentDir = os.path.dirname(os.path.dirname(file)) # refers to <driver_num> e.g. \"001, 002\"\n",
    "            currDir = parentDir\n",
    "            \n",
    "            childDir = os.path.dirname(file) # refers to where the current video file is <scenario> e.g. noglasses, glasses, etc.\n",
    "            indexParent = parentDir.rfind('\\\\')\n",
    "            parentDir = parentDir[indexParent:] # refers to <scenario> only\n",
    "            \n",
    "            indexChild = childDir.rfind('\\\\')\n",
    "            childDir = childDir[indexChild:]\n",
    "            \n",
    "#             filename of txt document containing labels for current video\n",
    "            if datasetDir[1:] == \"Training Dataset\":\n",
    "                labelFile = currDir + \"\\\\\" + childDir[1:] + \"\\\\\" + parentDir[1:] + \"_\" + os.path.basename(file)[:-4] + \"_drowsiness\" + '.txt'\n",
    "            elif parentDir[1:] == \"Evaluation Dataset\":\n",
    "                labelFile = currDir + \"\\\\\" + childDir[1:] + \"\\\\\" + os.path.basename(file)[:-4] + \"_drowsiness\" + '.txt'\n",
    "                labelFile = labelFile.replace(\"mix\", \"mixing\")\n",
    "                labelFile = labelFile.replace(\"nightno\", \"night_no\")\n",
    "            else:\n",
    "                scenario = \"\"\n",
    "                if \"sunglasses\" in os.path.basename(file)[:-4]:\n",
    "                    scenario = \"sunglasses\"\n",
    "                elif \"night_noglasses\" in os.path.basename(file)[:-4]:\n",
    "                    scenario = \"night_noglasses\"    \n",
    "                elif \"noglasses\" in os.path.basename(file)[:-4]:\n",
    "                    scenario = \"noglasses\"\n",
    "                elif \"night_glasses\" in os.path.basename(file)[:-4]:\n",
    "                    scenario = \"sunglasses\"\n",
    "                else:\n",
    "                    scenario = \"glasses\"\n",
    "                \n",
    "                labelFile = currDir + \"\\\\\" + childDir[1:] + \"\\\\\" + \"test_label_txt\\\\\" + scenario + \"\\\\\" + os.path.basename(file)[:-4] + \"_drowsiness\" + '.txt'\n",
    "                \n",
    "                if not os.path.exists(labelFile):                    \n",
    "                    labelFile = currDir + \"\\\\\" + childDir[1:] + \"\\\\\" + \"test_label_txt\\\\\" + \"wh\" + \"\\\\\" + os.path.basename(file)[:-4] + \"_drowsiness\" + '.txt'\n",
    "                \n",
    "                labelFile = labelFile.replace(\"mix\", \"mixing\")\n",
    "                \n",
    "            with open(labelFile) as f:\n",
    "                labels = f.readline()\n",
    "            try:\n",
    "                \n",
    "                if datasetDir[1:] == \"Training Dataset\":\n",
    "                    save_path = \"./data/Training/\" # specify where (in relation to root path to place new folder)\n",
    "                    file_name = save_path + parentDir[1:] + \"_\" + os.path.basename(file)[:-4] + \"/\" + parentDir[1:] + \"_\" + os.path.basename(file)[:-4] + \"_\" + str(current_frame) + \"_\" + labels[current_frame] + '.jpg'\n",
    "                elif parentDir[1:] == \"Evaluation Dataset\":\n",
    "                    save_path = \"./data/Evaluation/\" # specify where (in relation to root path to place new folder)\n",
    "                    file_name = save_path + os.path.basename(file)[:-4] + \"/\" + os.path.basename(file)[:-4] + \"_\" + str(current_frame) + \"_\" + labels[current_frame] + '.jpg'\n",
    "                else:\n",
    "                    save_path = \"./data/Testing/\" # specify where (in relation to root path to place new folder)\n",
    "                    file_name = save_path + os.path.basename(file)[:-4] + \"/\" + os.path.basename(file)[:-4] + \"_\" + str(current_frame) + \"_\" + labels[current_frame] + '.jpg'\n",
    "                \n",
    "#                 Creating..../data/Testing/037_sunglasses_mix/037_sunglasses_mix_7110_1.jpg\n",
    "                \n",
    "            except IndexError:\n",
    "                continue\n",
    "            \n",
    "            print('Creating...' + file_name)\n",
    "            cv2.imwrite(file_name, frame)\n",
    "            \n",
    "            img_file = Path(file_name) # for Retinex images, use replace() to get path of same named images in order to get post-retinex illum\n",
    "            \n",
    "            if save_path == \"./data/Training/\":\n",
    "                trainFileNames.append(os.path.basename(img_file)[:-4])\n",
    "                trainLandmarks.append(generateLandmarks(os.path.dirname(img_file) + \"\\\\\" + os.path.basename(img_file)))\n",
    "                trainIllumsRaw.append(illuminanceEstimation(img_file))\n",
    "                \n",
    "                retSave_path = \"./data/RetTraining/\" + parentDir[1:] + \"_\" + os.path.basename(file)[:-4] + \"/\"\n",
    "                retinexImplement(img_file, retSave_path)\n",
    "                \n",
    "#               post-retinex functions\n",
    "                \n",
    "                retSave_path = retSave_path + os.path.basename(img_file)[:-4] + \"_URetinexNet.jpg\"\n",
    "                trainIllumsRet.append(illuminanceEstimation(retSave_path))\n",
    "            \n",
    "                trainLandmarksRet.append(generateLandmarks(retSave_path))\n",
    "                \n",
    "                \n",
    "\n",
    "            elif save_path == \"./data/Evaluation/\":\n",
    "                evalFileNames.append(os.path.basename(img_file)[:-4])\n",
    "                evalLandmarks.append(generateLandmarks(os.path.dirname(img_file) + \"\\\\\" + os.path.basename(img_file)))\n",
    "                evalIllumsRaw.append(illuminanceEstimation(img_file))\n",
    "                \n",
    "                retSave_path = \"./data/RetEvaluation/\" + parentDir[1:] + \"_\" + os.path.basename(file)[:-4] + \"/\"\n",
    "                retinexImplement(img_file, retSave_path)\n",
    "                \n",
    "#               post-retinex functions\n",
    "                \n",
    "                retSave_path = retSave_path + os.path.basename(img_file)[:-4] + \"_URetinexNet.jpg\"\n",
    "                evalIllumsRet.append(illuminanceEstimation(retSave_path))\n",
    "            \n",
    "                evalLandmarksRet.append(generateLandmarks(retSave_path))\n",
    "            \n",
    "            \n",
    "        current_frame += 1\n",
    "    \n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcf88578",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T15:56:05.033714Z",
     "start_time": "2023-10-23T15:56:05.020702Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Training Videos Path\n",
    "def trainingData_prep():\n",
    "    training_videos = Path(r\"NTHU Dataset\\Training_Evaluation_Dataset\\Training Dataset\") #AVIs\n",
    "    training_video_paths = []\n",
    "\n",
    "    # create data folder\n",
    "    try:\n",
    "        if not os.path.exists('data'):\n",
    "                os.makedirs('data')\n",
    "                os.mkdir(os.path.join('./data/', 'Training'))\n",
    "                os.mkdir(os.path.join('./data/', 'Evaluation'))\n",
    "                os.mkdir(os.path.join('./data/', 'Testing'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetTraining'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetEvaluation'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetTesting'))\n",
    "    except OSError:\n",
    "        print('Error: Creating directory of data')\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for driver in training_videos.glob(\"*\"):\n",
    "        for scenario in driver.glob(\"*\"):\n",
    "            for videos_file in scenario.glob(\"*.avi\"):\n",
    "\n",
    "    #         note: videos_file refers to direct path of current video file\n",
    "\n",
    "                datasetDir = os.path.dirname(os.path.dirname(os.path.dirname(videos_file)))\n",
    "                indexDataset = datasetDir.rfind('\\\\')\n",
    "                datasetDir = datasetDir[indexDataset:]\n",
    "\n",
    "                parentDir = os.path.dirname(os.path.dirname(videos_file)) # refers to <driver_num> e.g. \"001, 002\"\n",
    "                currDir = parentDir\n",
    "\n",
    "                childDir = os.path.dirname(videos_file) # refers to where the current video file is <scenario> e.g. noglasses, glasses, etc.\n",
    "                indexParent = parentDir.rfind('\\\\')\n",
    "                parentDir = parentDir[indexParent:] # refers to <scenario> only\n",
    "\n",
    "                indexChild = childDir.rfind('\\\\')\n",
    "                childDir = childDir[indexChild:]\n",
    "\n",
    "                if datasetDir[1:] == \"Training Dataset\":\n",
    "                    save_path = \"./data/Training/\" # specify where (in relation to root path to place new folder)\n",
    "                    folder_name = parentDir[1:] + \"_\" + os.path.basename(videos_file)[:-4] # specify name of new folder\n",
    "                elif parentDir[1:] == \"Evaluation Dataset\":\n",
    "                    save_path = \"./data/Evaluation/\" # specify where (in relation to root path to place new folder)\n",
    "                    folder_name = os.path.basename(videos_file)[:-4] # specify name of new folder\n",
    "\n",
    "                data_path = os.path.join(save_path, folder_name) # set path and folder name as input for os.path.join()\n",
    "\n",
    "                inputPath = save_path + folder_name\n",
    "\n",
    "                if not os.path.exists(inputPath):\n",
    "                    os.mkdir(data_path) # create new folder based on data_path\n",
    "\n",
    "                video_path = str(videos_file)\n",
    "                training_video_paths.append(video_path)\n",
    "                frame_capture(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "732f5fa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T15:56:05.344054Z",
     "start_time": "2023-10-23T15:56:05.336679Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluation Videos Path\n",
    "def evalData_prep():\n",
    "    evaluation_videos = Path(r\"NTHU Dataset\\Training_Evaluation_Dataset\\Evaluation Dataset\") #AVIs\n",
    "    evaluation_video_paths = []\n",
    "\n",
    "    # create data folder\n",
    "    try:\n",
    "        if not os.path.exists('data'):\n",
    "                os.makedirs('data')\n",
    "                os.mkdir(os.path.join('./data/', 'Training'))\n",
    "                os.mkdir(os.path.join('./data/', 'Evaluation'))\n",
    "                os.mkdir(os.path.join('./data/', 'Testing'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetTraining'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetEvaluation'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetTesting'))\n",
    "    except OSError:\n",
    "        print('Error: Creating directory of data')\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for driver in evaluation_videos.glob(\"*\"):\n",
    "        for videos_file in driver.glob(\"*.mp4\"):\n",
    "\n",
    "    #         note: videos_file refers to direct path of current video file\n",
    "\n",
    "            datasetDir = os.path.dirname(os.path.dirname(os.path.dirname(videos_file)))\n",
    "            indexDataset = datasetDir.rfind('\\\\')\n",
    "            datasetDir = datasetDir[indexDataset:]\n",
    "\n",
    "            parentDir = os.path.dirname(os.path.dirname(videos_file)) # refers to <driver_num> e.g. \"001, 002\"\n",
    "            currDir = parentDir\n",
    "\n",
    "            childDir = os.path.dirname(videos_file) # refers to where the current video file is <scenario> e.g. noglasses, glasses, etc.\n",
    "            indexParent = parentDir.rfind('\\\\')\n",
    "            parentDir = parentDir[indexParent:] # refers to <scenario> only\n",
    "\n",
    "            indexChild = childDir.rfind('\\\\')\n",
    "            childDir = childDir[indexChild:]\n",
    "\n",
    "            if datasetDir[1:] == \"Training Dataset\":\n",
    "                save_path = \"./data/Training/\" # specify where (in relation to root path to place new folder)\n",
    "                folder_name = parentDir[1:] + \"_\" + os.path.basename(videos_file)[:-4] # specify name of new folder\n",
    "            elif parentDir[1:] == \"Evaluation Dataset\":\n",
    "                save_path = \"./data/Evaluation/\" # specify where (in relation to root path to place new folder)\n",
    "                folder_name = os.path.basename(videos_file)[:-4] # specify name of new folder\n",
    "\n",
    "            data_path = os.path.join(save_path, folder_name) # set path and folder name as input for os.path.join()\n",
    "\n",
    "            inputPath = save_path + folder_name\n",
    "\n",
    "            if not os.path.exists(inputPath):\n",
    "                os.mkdir(data_path) # create new folder based on data_path\n",
    "\n",
    "            video_path = str(videos_file)\n",
    "            evaluation_video_paths.append(video_path)\n",
    "            frame_capture(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2243a300",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T15:56:05.618742Z",
     "start_time": "2023-10-23T15:56:05.612878Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Testing Videos Path\n",
    "def testData_prep():\n",
    "    testing_videos = Path(r\"NTHU Dataset\\Testing_Dataset\") #MP4s\n",
    "    testing_video_paths = []\n",
    "\n",
    "    # create data folder\n",
    "    try:\n",
    "        if not os.path.exists('data'):\n",
    "                os.makedirs('data')\n",
    "                os.mkdir(os.path.join('./data/', 'Training'))\n",
    "                os.mkdir(os.path.join('./data/', 'Evaluation'))\n",
    "                os.mkdir(os.path.join('./data/', 'Testing'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetTraining'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetEvaluation'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetTesting'))\n",
    "    except OSError:\n",
    "        print('Error: Creating directory of data')\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for videos_file in testing_videos.glob(\"*.mp4\"):\n",
    "        print(videos_file)\n",
    "\n",
    "#         note: videos_file refers to direct path of current video file\n",
    "\n",
    "        print(os.path.dirname(videos_file))\n",
    "\n",
    "        datasetDir = os.path.dirname(videos_file)\n",
    "        indexDataset = datasetDir.rfind('\\\\')\n",
    "        datasetDir = datasetDir[indexDataset:]\n",
    "\n",
    "        save_path = \"./data/Testing/\" # specify where (in relation to root path to place new folder)\n",
    "        folder_name = os.path.basename(videos_file)[:-4] # specify name of new folder\n",
    "\n",
    "        print(save_path + folder_name)\n",
    "\n",
    "        data_path = os.path.join(save_path, folder_name) # set path and folder name as input for os.path.join()\n",
    "\n",
    "        inputPath = save_path + folder_name\n",
    "\n",
    "        if not os.path.exists(inputPath):\n",
    "            os.mkdir(data_path) # create new folder based on data_path\n",
    "\n",
    "        video_path = str(videos_file)\n",
    "        testing_video_paths.append(video_path)\n",
    "        frame_capture(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0936f309",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T15:56:06.212040Z",
     "start_time": "2023-10-23T15:56:06.149675Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*- ILLUM EST FUNCTIONS\n",
    "\"\"\"\n",
    "Module for hyper spectral image simulation\n",
    "==========================================\n",
    "\n",
    " :_HYPSPCIM_PATH: path to module\n",
    "\n",
    " :_HYPSPCIM_DEFAULT_IMAGE: path + filename to default image\n",
    " \n",
    " :_CSF_NIKON_D700: Nikon D700 camera sensitivity functions\n",
    " \n",
    " :_ROUNDING: rounding of input to xyz_to_rfl() search algorithm for improved speed\n",
    "\n",
    " :xyz_to_rfl(): approximate spectral reflectance of xyz based on k nearest \n",
    "                neighbour interpolation of samples from a standard reflectance \n",
    "                set.\n",
    "\n",
    " :render_image(): Render image under specified light source spd.\n",
    "\n",
    " :get_superresolution_hsi(): Get a HighResolution HyperSpectral Image (super-resolution HSI) based on a LowResolution HSI and a HighResolution Color Image.\n",
    "\n",
    " :hsi_to_rgb(): Convert HyperSpectral Image to rgb\n",
    " \n",
    " :rfl_to_rgb(): Convert spectral reflectance functions (illuminated by spd) to Camera Sensitivity Functions.\n",
    "     \n",
    ".. codeauthor:: Kevin A.G. Smet (ksmet1977 at gmail.com)\n",
    "\"\"\"\n",
    "\n",
    "from luxpy import (cat, colortf, _CIEOBS, _CIE_ILLUMINANTS, _CRI_RFL, _CIE_D65,_CIE_E,\n",
    "                   spd_to_xyz, plot_color_data, math, cie_interp, getwlr, xyz_to_srgb)\n",
    "from luxpy.utils import np, plt, sp, _PKG_PATH, _SEP, _EPS \n",
    "\n",
    "import warnings\n",
    "from imageio import imsave\n",
    "\n",
    "illum_out = 0\n",
    "\n",
    "__all__ =['_HYPSPCIM_PATH','_HYPSPCIM_DEFAULT_IMAGE','render_image','xyz_to_rfl',\n",
    "          'get_superresolution_hsi','hsi_to_rgb','rfl_to_rgb','_CSF_NIKON_D700']             \n",
    "\n",
    "_HYPSPCIM_PATH = _PKG_PATH + _SEP + 'hypspcim' + _SEP\n",
    "_HYPSPCIM_DEFAULT_IMAGE = _PKG_PATH + _SEP + 'toolboxes' + _SEP + 'hypspcim' +  _SEP + 'data' + _SEP + 'testimage1.jpg'\n",
    "\n",
    "\n",
    "_ROUNDING = 6 # to speed up xyz_to_rfl search algorithm, increase if kernel dies!!!\n",
    "\n",
    "# Nikon D700 camera sensitivity functions:\n",
    "_CSF_NIKON_D700 = np.vstack((np.arange(400,710,10),\n",
    "                             np.array([[0.005, 0.007, 0.012, 0.015, 0.023, 0.025, 0.030, 0.026, 0.024, 0.019, 0.010, 0.004, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  0.000,  0.000,  0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000], \n",
    "                                       [0.000, 0.000, 0.000, 0.000, 0.000, 0.001, 0.002, 0.003, 0.005, 0.007, 0.012, 0.013, 0.015, 0.016, 0.017, 0.020, 0.013, 0.011, 0.009, 0.005,  0.001,  0.001,  0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.002, 0.002, 0.003],\n",
    "                                       [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.001, 0.003, 0.010, 0.012,  0.013,  0.022,  0.020, 0.020, 0.018, 0.017, 0.016, 0.016, 0.014, 0.014, 0.013]])[::-1]))\n",
    "\n",
    "\n",
    "def xyz_to_rfl(xyz, CSF = None, rfl = None, out = 'rfl_est', \\\n",
    "                 refspd = None, D = None, cieobs = _CIEOBS, \\\n",
    "                 cspace = 'xyz', cspace_tf = {},\\\n",
    "                 interp_type = 'nd', k_neighbours = 4, verbosity = 0,\n",
    "                 csf_based_rgb_rounding = _ROUNDING):\n",
    "    \"\"\"\n",
    "    Approximate spectral reflectance of xyz values based on nd-dimensional linear interpolation \n",
    "    or k nearest neighbour interpolation of samples from a standard reflectance set.\n",
    "    \n",
    "    Args:\n",
    "        :xyz: \n",
    "            | ndarray with xyz values of target points.\n",
    "        :CSF:\n",
    "            | None, optional\n",
    "            | RGB camera response functions.\n",
    "            | If None: input :xyz: contains raw rgb (float) values. Override :cspace:\n",
    "            | argument and perform estimation directly in raw rgb space!!!\n",
    "        :rfl: \n",
    "            | ndarray, optional\n",
    "            | Reflectance set for color coordinate to rfl mapping.\n",
    "        :out: \n",
    "            | 'rfl_est' or str, optional\n",
    "        :refspd: \n",
    "            | None, optional\n",
    "            | Refer ence spectrum for color coordinate to rfl mapping.\n",
    "            | None defaults to D65.\n",
    "        :cieobs:\n",
    "            | _CIEOBS, optional\n",
    "            | CMF set used for calculation of xyz from spectral data.\n",
    "        :cspace:\n",
    "            | 'xyz',  optional\n",
    "            | Color space for color coordinate to rfl mapping.\n",
    "            | Tip: Use linear space (e.g. 'xyz', 'Yuv',...) for (interp_type == 'nd'),\n",
    "            |      and perceptually uniform space (e.g. 'ipt') for (interp_type == 'nearest')\n",
    "        :cspace_tf:\n",
    "            | {}, optional\n",
    "            | Dict with parameters for xyz_to_cspace and cspace_to_xyz transform.\n",
    "        :interp_type:\n",
    "            | 'nd', optional\n",
    "            | Options:\n",
    "            | - 'nd': perform n-dimensional linear interpolation using Delaunay triangulation.\n",
    "            | - 'nearest': perform nearest neighbour interpolation. \n",
    "        :k_neighbours:\n",
    "            | 4 or int, optional\n",
    "            | Number of nearest neighbours for reflectance spectrum interpolation.\n",
    "            | Neighbours are found using scipy.spatial.cKDTree\n",
    "        :verbosity:\n",
    "            | 0, optional\n",
    "            | If > 0: make a plot of the color coordinates of original and \n",
    "            | rendered image pixels.\n",
    "        :csf_based_rgb_rounding:\n",
    "            | _ROUNDING, optional\n",
    "            | Int representing the number of decimals to round the RGB values (obtained from not-None CSF input) to before applying the search algorithm.\n",
    "            | Smaller values increase the search speed, but could cause fatal error that causes python kernel to die. If this happens increase the rounding int value.\n",
    "\n",
    "    Returns:\n",
    "        :returns: \n",
    "            | :rfl_est:\n",
    "            | ndarrays with estimated reflectance spectra.\n",
    "    \"\"\"\n",
    "\n",
    "    # get rfl set:\n",
    "    if rfl is None: # use IESTM30['4880'] set \n",
    "        rfl = _CRI_RFL['ies-tm30']['4880']['5nm']\n",
    "    \n",
    "    wlr = rfl[0]\n",
    "    \n",
    "    # get Ref spd:\n",
    "    if refspd is None:\n",
    "        refspd = _CIE_ILLUMINANTS['D65'].copy()\n",
    "    refspd = cie_interp(refspd, wlr, kind = 'linear') # force spd to same wavelength range as rfl\n",
    "        \n",
    "    # Calculate rgb values of standard rfl set under refspd:\n",
    "    if CSF is None:\n",
    "        # Calculate lab coordinates:\n",
    "        xyz_rr, xyz_wr = spd_to_xyz(refspd, relative = True, rfl = rfl, cieobs = cieobs, out = 2)\n",
    "        cspace_tf_copy = cspace_tf.copy()\n",
    "        cspace_tf_copy['xyzw'] = xyz_wr # put correct white point in param. dict\n",
    "        lab_rr = colortf(xyz_rr, tf = cspace, fwtf = cspace_tf_copy, bwtf = cspace_tf_copy)[:,0,:]\n",
    "    else:\n",
    "        # Calculate rgb coordinates from camera sensitivity functions\n",
    "        rgb_rr = rfl_to_rgb(rfl, spd = refspd, CSF = CSF, wl = None)   \n",
    "        lab_rr = rgb_rr\n",
    "        xyz = xyz\n",
    "        lab_rr = np.round(lab_rr,csf_based_rgb_rounding) # speed up search\n",
    "        \n",
    "        global illum_out\n",
    "        illum_out = np.mean(lab_rr)\n",
    "        print(\"Illuminance: \" + str(np.mean(lab_rr)))\n",
    "        \n",
    "    # Convert xyz to lab-type values under refspd:\n",
    "    if CSF is None:\n",
    "        lab = colortf(xyz, tf = cspace, fwtf = cspace_tf_copy, bwtf = cspace_tf_copy)\n",
    "    else:\n",
    "        lab = xyz # xyz contained rgb values !!!\n",
    "        rgb = xyz\n",
    "        lab = np.round(lab,csf_based_rgb_rounding) # speed up search\n",
    "    \n",
    "    if interp_type == 'nearest':\n",
    "        # Find rfl (cfr. lab_rr) from rfl set that results in 'near' metameric \n",
    "        # color coordinates for each value in lab_ur (i.e. smallest DE):\n",
    "        # Construct cKDTree:\n",
    "        tree = sp.spatial.cKDTree(lab_rr, copy_data = True)\n",
    "        \n",
    "        # Interpolate rfls using k nearest neightbours and inverse distance weigthing:\n",
    "        d, inds = tree.query(lab, k = k_neighbours )\n",
    "        if k_neighbours  > 1:\n",
    "            d += _EPS\n",
    "            w = (1.0 / d**2)[:,:,None] # inverse distance weigthing\n",
    "            rfl_est = np.sum(w * rfl[inds+1,:], axis=1) / np.sum(w, axis=1)\n",
    "        else:\n",
    "            rfl_est = rfl[inds+1,:].copy()\n",
    "    elif interp_type == 'nd':\n",
    "\n",
    "        rfl_est = math.ndinterp1_scipy(lab_rr, rfl[1:], lab)\n",
    "            \n",
    "        _isnan = np.isnan(rfl_est[:,0]) \n",
    "\n",
    "        if (_isnan.any()): #do nearest neigbour method for those that fail using Delaunay (i.e. ndinterp1_scipy)\n",
    "\n",
    "            # Find rfl (cfr. lab_rr) from rfl set that results in 'near' metameric \n",
    "            # color coordinates for each value in lab_ur (i.e. smallest DE):\n",
    "            # Construct cKDTree:\n",
    "            tree = sp.spatial.cKDTree(lab_rr, copy_data = True)\n",
    "\n",
    "            # Interpolate rfls using k nearest neightbours and inverse distance weigthing:\n",
    "            d, inds = tree.query(lab[_isnan,...], k = k_neighbours )\n",
    "\n",
    "            if k_neighbours  > 1:\n",
    "                d += _EPS\n",
    "                w = (1.0 / d**2)[:,:,None] # inverse distance weigthing\n",
    "                rfl_est_isnan = np.sum(w * rfl[inds+1,:], axis=1) / np.sum(w, axis=1)\n",
    "            else:\n",
    "                rfl_est_isnan = rfl[inds+1,:].copy()\n",
    "            rfl_est[_isnan, :] = rfl_est_isnan\n",
    "\n",
    "    else:\n",
    "        raise Exception('xyz_to_rfl(): unsupported interp_type!')\n",
    "    \n",
    "    rfl_est[rfl_est<0] = 0 #can occur for points outside convexhull of standard rfl set.\n",
    "\n",
    "    rfl_est = np.vstack((rfl[0],rfl_est))\n",
    "        \n",
    "    if ((verbosity > 0) | ('xyz_est' in out.split(',')) | ('lab_est' in out.split(',')) | ('DEi_ab' in out.split(',')) | ('DEa_ab' in out.split(','))) & (CSF is None):\n",
    "        xyz_est, _ = spd_to_xyz(refspd, rfl = rfl_est, relative = True, cieobs = cieobs, out = 2)\n",
    "        cspace_tf_copy = cspace_tf.copy()\n",
    "        cspace_tf_copy['xyzw'] = xyz_wr # put correct white point in param. dict\n",
    "        lab_est = colortf(xyz_est, tf = cspace, fwtf = cspace_tf_copy)[:,0,:]\n",
    "        DEi_ab = np.sqrt(((lab_est[:,1:3]-lab[:,1:3])**2).sum(axis=1))\n",
    "        DEa_ab = DEi_ab.mean()\n",
    "    elif ((verbosity > 0) | ('xyz_est' in out.split(',')) | ('rgb_est' in out.split(',')) | ('DEi_rgb' in out.split(',')) | ('DEa_rgb' in out.split(','))) & (CSF is not None):\n",
    "        rgb_est = rfl_to_rgb(rfl_est[1:], spd = refspd, CSF = CSF, wl = wlr) \n",
    "        xyz_est = rgb_est\n",
    "        DEi_rgb = np.sqrt(((rgb_est - rgb)**2).sum(axis=1))\n",
    "        DEa_rgb = DEi_rgb.mean()\n",
    "\n",
    "        \n",
    "    if verbosity > 0:\n",
    "        if CSF is None:\n",
    "            ax = plot_color_data(lab[...,1], lab[...,2], z = lab[...,0], \\\n",
    "                            show = False, cieobs = cieobs, cspace = cspace, \\\n",
    "                            formatstr = 'ro', label = 'Original')\n",
    "            plot_color_data(lab_est[...,1], lab_est[...,2], z = lab_est[...,0], \\\n",
    "                            show = True, axh = ax, cieobs = cieobs, cspace = cspace, \\\n",
    "                            formatstr = 'bd', label = 'Rendered')\n",
    "        else:\n",
    "            n = 100 #min(rfl.shape[0]-1,rfl_est.shape[0]-1)\n",
    "            s = np.random.permutation(rfl.shape[0]-1)[:min(n,rfl.shape[0]-1)]\n",
    "            st = np.random.permutation(rfl_est.shape[0]-1)[:min(n,rfl_est.shape[0]-1)]\n",
    "            fig = plt.figure()\n",
    "            ax = np.zeros((3,),dtype=np.object)\n",
    "            ax[0] = fig.add_subplot(131)\n",
    "            ax[1] = fig.add_subplot(132)\n",
    "            ax[2] = fig.add_subplot(133,projection='3d')\n",
    "            ax[0].plot(rfl[0],rfl[1:][s].T, linestyle = '-')\n",
    "            ax[0].set_title('Original RFL set (random selection of all)')\n",
    "            ax[0].set_ylim([0,1])\n",
    "            ax[1].plot(rfl_est[0],rfl_est[1:][st].T, linestyle = '--')\n",
    "            ax[0].set_title('Estimated RFL set (random selection of targets)')\n",
    "            ax[1].set_ylim([0,1])\n",
    "            ax[2].plot(rgb[st,0],rgb[st,1],rgb[st,2],'ro', label = 'Original')\n",
    "            ax[2].plot(rgb_est[st,0],rgb_est[st,1],rgb_est[st,2],'bd', label = 'Rendered')\n",
    "            ax[2].legend()\n",
    "    if out == 'rfl_est':\n",
    "        return rfl_est\n",
    "    elif out == 'rfl_est,xyz_est':\n",
    "        return rfl_est, xyz_est\n",
    "    else:\n",
    "        return eval(out)\n",
    "\n",
    "\n",
    "\n",
    "def render_image(img = None, spd = None, rfl = None, out = 'img_hyp', \\\n",
    "                 refspd = None, D = None, cieobs = _CIEOBS, \\\n",
    "                 cspace = 'xyz', cspace_tf = {}, CSF = None,\\\n",
    "                 interp_type = 'nd', k_neighbours = 4, show = True,\n",
    "                 verbosity = 0, show_ref_img = True,\\\n",
    "                 stack_test_ref = 12,\\\n",
    "                 write_to_file = None,\\\n",
    "                 csf_based_rgb_rounding = _ROUNDING):\n",
    "    \"\"\"\n",
    "    Render image under specified light source spd.\n",
    "    \n",
    "    Args:\n",
    "        :img: \n",
    "            | None or str or ndarray with float (max = 1) rgb image.\n",
    "            | None load a default image.\n",
    "        :spd: \n",
    "            | ndarray, optional\n",
    "            | Light source spectrum for rendering\n",
    "            | If None: use CIE illuminant F4\n",
    "        :rfl: \n",
    "            | ndarray, optional\n",
    "            | Reflectance set for color coordinate to rfl mapping.\n",
    "        :out: \n",
    "            | 'img_hyp' or str, optional\n",
    "            |  (other option: 'img_ren': rendered image under :spd:)\n",
    "        :refspd:\n",
    "            | None, optional\n",
    "            | Reference spectrum for color coordinate to rfl mapping.\n",
    "            | None defaults to D65 (srgb has a D65 white point)\n",
    "        :D: \n",
    "            | None, optional\n",
    "            | Degree of (von Kries) adaptation from spd to refspd. \n",
    "        :cieobs:\n",
    "            | _CIEOBS, optional\n",
    "            | CMF set for calculation of xyz from spectral data.\n",
    "        :cspace:\n",
    "            | 'xyz',  optional\n",
    "            | Color space for color coordinate to rfl mapping.\n",
    "            | Tip: Use linear space (e.g. 'xyz', 'Yuv',...) for (interp_type == 'nd'),\n",
    "            |      and perceptually uniform space (e.g. 'ipt') for (interp_type == 'nearest')\n",
    "        :cspace_tf:\n",
    "            | {}, optional\n",
    "            | Dict with parameters for xyz_to_cspace and cspace_to_xyz transform.\n",
    "        :CSF:\n",
    "            | None, optional\n",
    "            | RGB camera response functions.\n",
    "            | If None: input :xyz: contains raw rgb values. Override :cspace:\n",
    "            | argument and perform estimation directly in raw rgb space!!!\n",
    "        :interp_type:\n",
    "            | 'nd', optional\n",
    "            | Options:\n",
    "            | - 'nd': perform n-dimensional linear interpolation using Delaunay triangulation.\n",
    "            | - 'nearest': perform nearest neighbour interpolation. \n",
    "        :k_neighbours:\n",
    "            | 4 or int, optional\n",
    "            | Number of nearest neighbours for reflectance spectrum interpolation.\n",
    "            | Neighbours are found using scipy.spatial.cKDTree\n",
    "        :show: \n",
    "            | True, optional\n",
    "            |  Show images.\n",
    "        :verbosity:\n",
    "            | 0, optional\n",
    "            | If > 0: make a plot of the color coordinates of original and \n",
    "              rendered image pixels.\n",
    "        :show_ref_img:\n",
    "            | True, optional\n",
    "            | True: shows rendered image under reference spd. False: shows\n",
    "            |  original image.\n",
    "        :write_to_file:\n",
    "            | None, optional\n",
    "            | None: do nothing, else: write to filename(+path) in :write_to_file:\n",
    "        :stack_test_ref: \n",
    "            | 12, optional\n",
    "            |   - 12: left (test), right (ref) format for show and imwrite\n",
    "            |   - 21: top (test), bottom (ref)\n",
    "            |   - 1: only show/write test\n",
    "            |   - 2: only show/write ref\n",
    "            |   - 0: show both, write test\n",
    "        :csf_based_rgb_rounding:\n",
    "            | _ROUNDING, optional\n",
    "            | Int representing the number of decimals to round the RGB values (obtained from not-None CSF input) to before applying the search algorithm.\n",
    "            | Smaller values increase the search speed, but could cause fatal error that causes python kernel to die. If this happens increase the rounding int value.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        :returns: \n",
    "            | img_hyp, img_ren, \n",
    "            | ndarrays with float hyperspectral image and rendered images \n",
    "    \"\"\"\n",
    "    \n",
    "    # Get image:\n",
    "    #imread = lambda x: plt.imread(x) #matplotlib.pyplot\n",
    "   \n",
    "    if img is not None:\n",
    "        if isinstance(img,str):\n",
    "            img = plt.imread(img).copy() # use matplotlib.pyplot's imread\n",
    "    else:\n",
    "        img = plt.imread(_HYPSPCIM_DEFAULT_IMAGE).copy()\n",
    "    \n",
    "    if img.dtype == np.uint8: \n",
    "        img = img/255\n",
    "    elif img.dtype == np.uint16:\n",
    "        img = img/(2**16-1)\n",
    "    elif (img.dtype == np.float64) | (img.dtype == np.float32):\n",
    "        pass\n",
    "    else:\n",
    "        raise Exception('img input must be None, string or ndarray of (max = 1) float32 or float64 !')\n",
    "    if img.max() > 1.0: raise Exception('img input must be None, string or ndarray of (max = 1) float32 or float64 !')\n",
    "    \n",
    "    \n",
    "    # Convert to 2D format:\n",
    "    rgb = img.reshape(img.shape[0]*img.shape[1],3) # *1.0: make float\n",
    "    rgb[rgb==0] = _EPS # avoid division by zero for pure blacks.\n",
    "\n",
    "    \n",
    "    # Get unique rgb values and positions:\n",
    "    rgb_u, rgb_indices = np.unique(rgb, return_inverse=True, axis = 0)\n",
    "\n",
    "    \n",
    "    # get rfl set:\n",
    "    if rfl is None: # use IESTM30['4880'] set \n",
    "        rfl = _CRI_RFL['ies-tm30']['4880']['5nm']\n",
    "    wlr = rfl[0] # spectral reflectance set determines wavelength range for estimation (xyz_to_rfl())\n",
    "        \n",
    "    # get Ref spd:\n",
    "    if refspd is None:\n",
    "        refspd = _CIE_ILLUMINANTS['D65'].copy()\n",
    "    refspd = cie_interp(refspd, wlr, kind = 'linear') # force spd to same wavelength range as rfl\n",
    "\n",
    "\n",
    "    # Convert rgb_u to xyz and lab-type values under assumed refspd:\n",
    "    if CSF is None:\n",
    "        xyz_wr = spd_to_xyz(refspd, cieobs = cieobs, relative = True)\n",
    "        xyz_ur = colortf(rgb_u*255, tf = 'srgb>xyz')\n",
    "    else:\n",
    "        xyz_ur = rgb_u # for input in xyz_to_rfl (when CSF is not None: this functions assumes input is indeed rgb !!!)\n",
    "    \n",
    "    # Estimate rfl's for xyz_ur:\n",
    "    rfl_est, xyzri = xyz_to_rfl(xyz_ur, rfl = rfl, out = 'rfl_est,xyz_est', \\\n",
    "                 refspd = refspd, D = D, cieobs = cieobs, \\\n",
    "                 cspace = cspace, cspace_tf = cspace_tf, CSF = CSF,\\\n",
    "                 interp_type = interp_type, k_neighbours = k_neighbours, \n",
    "                 verbosity = verbosity,\n",
    "                 csf_based_rgb_rounding = csf_based_rgb_rounding)\n",
    "\n",
    "    # Get default test spd if none supplied:\n",
    "    if spd is None:\n",
    "        spd = _CIE_ILLUMINANTS['F4']\n",
    "        \n",
    "    if CSF is None:\n",
    "        # calculate xyz values under test spd:\n",
    "        xyzti, xyztw = spd_to_xyz(spd, rfl = rfl_est, cieobs = cieobs, out = 2)\n",
    "    \n",
    "        # Chromatic adaptation from test spd to refspd:\n",
    "        if D is not None:\n",
    "            xyzti = cat.apply(xyzti, xyzw1 = xyztw, xyzw2 = xyz_wr, D = D)\n",
    "    \n",
    "        # Convert xyzti under test spd to srgb:\n",
    "        rgbti = colortf(xyzti, tf = 'srgb')/255\n",
    "    else:\n",
    "        # Calculate rgb coordinates from camera sensitivity functions under spd:\n",
    "        rgbti = rfl_to_rgb(rfl_est, spd = spd, CSF = CSF, wl = None) \n",
    "        \n",
    "         # Chromatic adaptation from test spd to refspd:\n",
    "        if D is not None:\n",
    "            white = np.ones_like(spd)\n",
    "            white[0] = spd[0]\n",
    "            rgbwr = rfl_to_rgb(white, spd = refspd, CSF = CSF, wl = None)\n",
    "            rgbwt = rfl_to_rgb(white, spd = spd, CSF = CSF, wl = None)\n",
    "            rgbti = cat.apply_vonkries2(rgbti,rgbwt,rgbwr,xyzw0=np.array([[1.0,1.0,1.0]]), in_type='rgb',out_type= 'rgb',D=1)\n",
    "        \n",
    "    \n",
    "    # Reconstruct original locations for rendered image rgbs:\n",
    "    img_ren = rgbti[rgb_indices]\n",
    "    img_ren.shape = img.shape # reshape back to 3D size of original\n",
    "    img_ren = img_ren\n",
    "    \n",
    "    # For output:\n",
    "    if show_ref_img == True:\n",
    "        rgb_ref = colortf(xyzri, tf = 'srgb')/255 if (CSF is None) else xyzri # if CSF not None: xyzri contains rgbri !!!\n",
    "        img_ref = rgb_ref[rgb_indices]\n",
    "        img_ref.shape = img.shape # reshape back to 3D size of original\n",
    "        img_str = 'Rendered (under ref. spd)'\n",
    "        img = img_ref\n",
    "    else:\n",
    "        img_str = 'Original'\n",
    "        img = img\n",
    "       \n",
    "    \n",
    "    if (stack_test_ref > 0) | show == True:\n",
    "        if stack_test_ref == 21:\n",
    "            img_original_rendered = np.vstack((img_ren,np.ones((4,img.shape[1],3)),img))\n",
    "            img_original_rendered_str = 'Rendered (under test spd)\\n ' + img_str \n",
    "        elif stack_test_ref == 12:\n",
    "            img_original_rendered = np.hstack((img_ren,np.ones((img.shape[0],4,3)),img))\n",
    "            img_original_rendered_str = 'Rendered (under test spd) | ' + img_str \n",
    "        elif stack_test_ref == 1:\n",
    "            img_original_rendered = img_ren\n",
    "            img_original_rendered_str = 'Rendered (under test spd)' \n",
    "        elif stack_test_ref == 2:\n",
    "            img_original_rendered = img\n",
    "            img_original_rendered_str = img_str\n",
    "        elif stack_test_ref == 0:\n",
    "            img_original_rendered = img_ren\n",
    "            img_original_rendered_str =  'Rendered (under test spd)' \n",
    "            \n",
    "    if write_to_file is not None:\n",
    "        # Convert from RGB to BGR formatand write:\n",
    "        #print('Writing rendering results to image file: {}'.format(write_to_file))\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            imsave(write_to_file, img_original_rendered)\n",
    "            \n",
    "    if show == True:\n",
    "        # show images using pyplot.show():\n",
    "        plt.figure()\n",
    "        \n",
    "        plt.imshow(img_original_rendered)\n",
    "        plt.title(img_original_rendered_str)\n",
    "        plt.gca().get_xaxis().set_ticklabels([])\n",
    "        plt.gca().get_yaxis().set_ticklabels([])\n",
    "        \n",
    "        if stack_test_ref == 0:\n",
    "            plt.figure()\n",
    "            plt.imshow(img)\n",
    "            plt.title(img_str)\n",
    "            plt.axis('off')\n",
    "      \n",
    "    if 'img_hyp' in out.split(','):\n",
    "        # Create hyper_spectral image:\n",
    "        rfl_image_2D = rfl_est[rgb_indices+1,:] # create array with all rfls required for each pixel\n",
    "        img_hyp = rfl_image_2D.reshape(img.shape[0],img.shape[1],rfl_image_2D.shape[1])\n",
    "\n",
    "\n",
    "    # Setup output:\n",
    "    if out == 'img_hyp':\n",
    "        return img_hyp\n",
    "    elif out == 'img_ren':\n",
    "        return img_ren\n",
    "    else:\n",
    "        return eval(out)\n",
    "\n",
    "\n",
    "def rfl_to_rgb(rfl, spd = None, CSF = None, wl = None, normalize_to_white = True):\n",
    "    \"\"\" \n",
    "    Convert spectral reflectance functions (illuminated by spd) to Camera Sensitivity Functions.\n",
    "    \n",
    "    Args:\n",
    "        :rfl:\n",
    "            | ndarray with spectral reflectance functions (1st row is wavelengths if wl is None).\n",
    "        :spd:\n",
    "            | None, optional\n",
    "            | ndarray with illumination spectrum\n",
    "        :CSF:\n",
    "            | None, optional\n",
    "            | ndarray with camera sensitivity functions \n",
    "            | If None: use Nikon D700\n",
    "        :normalize_to_white:\n",
    "            | True, optional\n",
    "            | If True: white-balance output rgb to a perfect white diffuser.\n",
    "    \n",
    "    Returns:\n",
    "        :rgb:\n",
    "            | ndarray with rgb values for each spectral reflectance functions\n",
    "    \"\"\"\n",
    "    rfl_cp = rfl.copy()\n",
    "    if (wl is None): \n",
    "        wl = rfl_cp[0] \n",
    "        rfl_cp = rfl_cp[1:]\n",
    "    wlr = getwlr(wl)\n",
    "    if spd is not None:\n",
    "        spd = cie_interp(spd,wlr,kind='linear')[1:]\n",
    "    else:\n",
    "        spd = np.ones_like(wlr)\n",
    "    if CSF is None: CSF = _CSF_NIKON_D700\n",
    "    CSF = cie_interp(CSF,wlr,kind='linear')\n",
    "    CSF[1:] = CSF[1:]*spd\n",
    "    rgb = rfl_cp @ CSF[1:].T \n",
    "    if normalize_to_white:\n",
    "        white = np.ones_like(spd)\n",
    "        white = white/white.sum()*spd.sum()\n",
    "        rgbw = white @ CSF[1:].T  \n",
    "        rgb = rgb/rgbw.max(axis = 0,keepdims=True) \n",
    "    \n",
    "    return rgb\n",
    "\n",
    "    \n",
    "    \n",
    "def hsi_to_rgb(hsi, spd = None, cieobs = _CIEOBS, srgb = False, \n",
    "               linear_rgb = False, CSF = None, normalize_to_white = True, \n",
    "               wl = [380,780,1]):\n",
    "    \"\"\" \n",
    "    Convert HyperSpectral Image to rgb.\n",
    "    \n",
    "    Args:\n",
    "        :hsi:\n",
    "            | ndarray with hyperspectral image [M,N,L]\n",
    "        :spd:\n",
    "            | None, optional\n",
    "            | ndarray with illumination spectrum\n",
    "        :cieobs:\n",
    "            | _CIEOBS, optional\n",
    "            | CMF set to convert spectral data to xyz tristimulus values.\n",
    "        :srgb:\n",
    "            | False, optional\n",
    "            | If False: Use xyz_to_srgb(spd_to_xyz(...)) to convert to srgb values\n",
    "            | If True: use camera sensitivity functions.\n",
    "        :linear_rgb:\n",
    "            | False, optional\n",
    "            | If False: use gamma = 2.4 in xyz_to_srgb, if False: use gamma = 1 and set :use_linear_part: to False.\n",
    "        :CSF:\n",
    "            | None, optional\n",
    "            | ndarray with camera sensitivity functions \n",
    "            | If None: use Nikon D700\n",
    "        :normalize_to_white:\n",
    "            | True, optional\n",
    "            | If True & CSF is not None: white-balance output rgb to a perfect white diffuser.\n",
    "        :wl:\n",
    "            | [380,780,1], optional\n",
    "            | Wavelength range and spacing or ndarray with wavelengths of HSI image.\n",
    "    \n",
    "    Returns:\n",
    "        :rgb:\n",
    "            | ndarray with rgb image [M,N,3]\n",
    "    \"\"\"\n",
    "    if spd is None:\n",
    "        spd = _CIE_E.copy()\n",
    "    wlr = getwlr(wl)\n",
    "    spd = cie_interp(spd,wl,kind='linear')\n",
    "    \n",
    "    hsi_2d = np.reshape(hsi,(hsi.shape[0]*hsi.shape[1],hsi.shape[2]))\n",
    "    if srgb:\n",
    "        xyz = spd_to_xyz(spd, cieobs = cieobs, relative = True, rfl = np.vstack((wlr,hsi_2d)))\n",
    "        gamma = 1 if linear_rgb else 2.4\n",
    "        rgb = xyz_to_srgb(xyz, gamma = gamma, use_linear_part = not linear_rgb)/255\n",
    "    else:\n",
    "        if CSF is None: CSF = _CSF_NIKON_D700\n",
    "        rgb = rfl_to_rgb(hsi_2d, spd = spd, CSF = CSF, wl = wl, normalize_to_white = normalize_to_white)        \n",
    "    return np.reshape(rgb,(hsi.shape[0],hsi.shape[1],3))\n",
    "\n",
    "       \n",
    "def get_superresolution_hsi(lrhsi, hrci, CSF, wl = [380,780,1], csf_based_rgb_rounding = _ROUNDING,\n",
    "                            interp_type = 'nd', k_neighbours = 4, verbosity = 0):\n",
    "    \"\"\" \n",
    "    Get a HighResolution HyperSpectral Image (super-resolution HSI) based on a LowResolution HSI and a HighResolution Color Image.\n",
    "    \n",
    "    Args:\n",
    "        :lrhsi:\n",
    "            | ndarray with float (max = 1) LowResolution HSI [m,m,L].\n",
    "        :hrci:\n",
    "            | ndarray with float (max = 1) HighResolution HSI [M,N,3].\n",
    "        :CSF:\n",
    "            | None, optional\n",
    "            | ndarray with camera sensitivity functions \n",
    "            | If None: use Nikon D700\n",
    "        :wl:\n",
    "            | [380,780,1], optional\n",
    "            | Wavelength range and spacing or ndarray with wavelengths of HSI image.\n",
    "        :interp_type:\n",
    "            | 'nd', optional\n",
    "            | Options:\n",
    "            | - 'nd': perform n-dimensional linear interpolation using Delaunay triangulation.\n",
    "            | - 'nearest': perform nearest neighbour interpolation. \n",
    "        :k_neighbours:\n",
    "            | 4 or int, optional\n",
    "            | Number of nearest neighbours for reflectance spectrum interpolation.\n",
    "            | Neighbours are found using scipy.spatial.cKDTree\n",
    "        :verbosity:\n",
    "            | 0, optional\n",
    "            | Verbosity level for sub-call to render_image().\n",
    "            | If > 0: make a plot of the color coordinates of original and \n",
    "            | rendered image pixels.\n",
    "        :csf_based_rgb_rounding:\n",
    "            | _ROUNDING, optional\n",
    "            | Int representing the number of decimals to round the RGB values (obtained from not-None CSF input) to before applying the search algorithm.\n",
    "            | Smaller values increase the search speed, but could cause fatal error that causes python kernel to die. If this happens increase the rounding int value.\n",
    "\n",
    "    Returns:\n",
    "        :hrhsi:\n",
    "            | ndarray with HighResolution HSI [M,N,L].\n",
    "        \n",
    "    Procedure:\n",
    "        | Call render_image(hrci, rfl = lrhsi_2, CSF = ...) to estimate a hyperspectral image\n",
    "        | from the high-resolution color image hrci with the reflectance spectra \n",
    "        | in the low-resolution hyper-spectral image as database for the estimation.\n",
    "        | Estimation is done in raw RGB space with the lrhsi converted using the\n",
    "        | camera sensitivity functions in CSF.\n",
    "    \"\"\"\n",
    "    wlr = getwlr(wl)\n",
    "    eew = np.vstack((wlr,np.ones_like(wlr)))\n",
    "    lrhsi_2d = np.vstack((wlr,np.reshape(lrhsi,(lrhsi.shape[0]*lrhsi.shape[1],lrhsi.shape[2])))) # create 2D rfl database\n",
    "    if CSF is None: CSF = _CSF_NIKON_D700\n",
    "    hrhsi = render_image(hrci, spd = eew,\n",
    "                         refspd = eew, rfl = lrhsi_2d, D = None,\n",
    "                         interp_type = interp_type, k_neighbours = k_neighbours,\n",
    "                         verbosity = verbosity, show = bool(verbosity),\n",
    "                         CSF = CSF, csf_based_rgb_rounding = csf_based_rgb_rounding) # render HR-hsi from HR-ci using LR-HSI rfls as database        \n",
    "    return hrhsi\n",
    "\n",
    "\n",
    "def illuminanceEstimation(input_img):\n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    "    # Example / test code for HSI simulation and rendering:\n",
    "    #--------------------------------------------------------------------------\n",
    "    # plt.close('all')\n",
    "    # from luxpy.toolboxes import spdbuild as spb\n",
    "    # S = spb.spd_builder(peakwl = [460,525,590],fwhm=[20,40,20],target=4000, tar_type = 'cct') \n",
    "    # img = _HYPSPCIM_DEFAULT_IMAGE\n",
    "    # img_hyp,img_ren = render_image(img = img, \n",
    "    #                                 cspace = 'Yuv',interp_type='nd',\n",
    "    #                                 spd = S, D=1, \n",
    "    #                                 show_ref_img = True,\n",
    "    #                                 stack_test_ref = 21,\n",
    "    #                                 out='img_hyp,img_ren',\n",
    "    #                                 write_to_file = 'test.jpg') \n",
    "    # raise Exception('')\n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    "    # Example / test code for super resolution:\n",
    "    #--------------------------------------------------------------------------\n",
    "    import time\n",
    "    import luxpy as lx\n",
    "    import matplotlib.pyplot as plt\n",
    "    from skimage import transform\n",
    "    import imageio\n",
    "    from skimage.transform import rescale,resize\n",
    "    \n",
    "    np.random.seed(1)    \n",
    "    \n",
    "    # Set some default parameters:\n",
    "    #----------------------------\n",
    "    load_hsi = False # If True: load hrci and hrhsi from npy-file.\n",
    "    file = input_img\n",
    "\n",
    "    cieobs = '1931_2' # CIE CMF set\n",
    "    linear_rgb = 1 # only used when srgb in hsi_to_rgb == True !!!\n",
    "    verbosity = 0\n",
    "    \n",
    "    # Create HR-rgb image and HR-HSI for code testing: \n",
    "    #---------------------------------------------------\n",
    "    # get an image:\n",
    "    im = imageio.v2.imread(file)/255\n",
    "    \n",
    "    # rescale to n x dimensions of typical hyperspectral camera:\n",
    "    n = 2 # downscale factor\n",
    "    w, h = 1280, 960\n",
    "    cr,cc = np.array(im.shape[:2])//2\n",
    "    crop = lambda im,cr,cc,h,w:im[(cr-h//2):(cr+h//2),(cc-w//2):(cc+w//2),:].copy()\n",
    "    im = crop(im,cr,cc,h*n,w*n)\n",
    "#     print('New image shape:',im.shape)\n",
    "    \n",
    "    # simulate HR hyperspectral image:\n",
    "    hrhsi = render_image(im,show=False)\n",
    "    wlr = getwlr([380,780,1]) #  = wavelength range of default TM30 rfl set\n",
    "    wlr = wlr[20:-80:10] # wavelength range from 400nm-700nm every 10 nm\n",
    "    hrhsi = hrhsi[...,20:-80:10] # wavelength range from 400nm-700nm every 10 nm\n",
    "#     print('Simulated HR-HSI shape:',hrhsi.shape)\n",
    "    # np.save(file[:-4]+'.npy',{'hrhsi':hrhsi,'im':im, 'wlr':wlr})\n",
    "    \n",
    "    # Illumination spectrum of HSI:    \n",
    "    eew = np.vstack((wlr,np.ones_like(wlr)))\n",
    "        \n",
    "    # Create fig and axes for plots:\n",
    "    if verbosity > 0: fig, axs = plt.subplots(1,3)\n",
    "    \n",
    "    # convert HR hsi to HR rgb image:\n",
    "    hrci = hsi_to_rgb(hrhsi, spd = eew, cieobs = cieobs, wl = wlr, linear_rgb = linear_rgb)\n",
    "    if verbosity > 0:  axs[0].imshow(hrci)\n",
    "    \n",
    "    # create LR hsi image for testing:\n",
    "    dl = n \n",
    "    lrhsi = hrhsi[::dl,::dl,:]\n",
    "#     print('Simulated LR-HSI shape:',lrhsi.shape)\n",
    "    \n",
    "    # convert LR hsi to LR rgb image:\n",
    "    lrci = hsi_to_rgb(lrhsi, spd = eew, cieobs = cieobs, wl = wlr,linear_rgb = linear_rgb)\n",
    "    if verbosity > 0:  axs[1].imshow(lrci)\n",
    "    \n",
    "    # # Perform rgb guided super-resolution:\n",
    "    #hrci = lrci # for testing of estimation code\n",
    "    tic = time.time()\n",
    "    hrhsi_est = get_superresolution_hsi(lrhsi, hrci, CSF = _CSF_NIKON_D700, wl = wlr)\n",
    "#     print('Elapsed time (s): {:1.4f}'.format(time.time() - tic))\n",
    "    hrci_est = hsi_to_rgb(hrhsi_est, spd = eew, cieobs = cieobs, wl = wlr, linear_rgb = linear_rgb)\n",
    "\n",
    "    if verbosity > 0:  axs[2].imshow(hrci_est)\n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    "    # Plot some rfl to visually evaluate estimation accuracy:\n",
    "    \n",
    "    hsi_rmse = np.linalg.norm(hrhsi-hrhsi_est)/np.array(hrhsi.shape[:2]).prod()**0.5\n",
    "#     print('RMSE(ground-truth,estimate): {:1.4f}'.format(hsi_rmse))\n",
    "    \n",
    "    global illum_out\n",
    "    return illum_out\n",
    "    \n",
    "#     fig, axs = plt.subplots(1,4, figsize=(22,5))\n",
    "    \n",
    "#     axs[0].imshow(transform.rescale(lrci,dl,order=0,multichannel=True),aspect='auto')\n",
    "#     axs[0].set_title('Color image of LR-HSI\\n(HR-to-LR scale factor = {:1.2f})'.format(1/dl))\n",
    "#     axs[0].axis('off')\n",
    "#     axs[1].imshow(hrci_est,aspect='auto')\n",
    "#     axs[1].set_title('Color image of estimated HR-HSI')\n",
    "#     axs[1].axis('off')\n",
    "    \n",
    "#     px_rmse = ((hrhsi_est-hrhsi)**2).sum(axis=-1)**0.5 # rmse per pixel\n",
    "#     axs[2].set_title('RMSE(ground-truth, estimated) HR-HSI\\nRMSE = {:1.4f}, max = {:1.4f}'.format((px_rmse**2).mean()**0.5,px_rmse.max()))\n",
    "#     im = axs[2].imshow(px_rmse, cmap = 'jet',aspect='auto') # rmse per pixel\n",
    "#     cbar = axs[2].figure.colorbar(im, ax=axs[2])\n",
    "#     cbar.ax.set_ylabel('RMSE', rotation=-90, va=\"bottom\")\n",
    "    \n",
    "    \n",
    "#     psorted = np.unravel_index(np.argsort(px_rmse, axis=None), px_rmse.shape) # index of pixels sorted by px_rmse\n",
    "#     np.random.seed(1)\n",
    "#     pxs = np.random.permutation(min(hrhsi.shape[:2]))[:12].reshape(2,3,2)\n",
    "#     iis = np.hstack((pxs[...,0].ravel(),psorted[0][-3:]))\n",
    "#     jjs = np.hstack((pxs[...,1].ravel(),psorted[1][-3:]))\n",
    "#     colors = np.array(['m','b','c','g','y','r','k','lightgrey','grey'])\n",
    "#     for t in range(len(iis)):\n",
    "#         ii,jj = iis[t],jjs[t]\n",
    "#         axs[1].plot(jj,ii,color = 'none', marker = 'o', mec = colors[t])\n",
    "#         axs[2].plot(jj,ii,color = 'none', marker = 'o', mec = colors[t])\n",
    "#         axs[3].plot(wlr,hrhsi[ii,jj,:],color = colors[t], linestyle ='-',label='ground-truth (r{:1.0f},c{:1.0f})'.format(ii,jj))\n",
    "#         axs[3].plot(wlr,hrhsi_est[ii,jj,:],color = colors[t], linestyle = '--',label='estimate (r{:1.0f},c{:1.0f})'.format(ii,jj))\n",
    "#     axs[3].legend(bbox_to_anchor=(1.05, 1))   \n",
    "#     axs[3].set_xlabel('Wavelengths (nm)')\n",
    "#     axs[3].set_ylabel('Spectral Reflectance')\n",
    "#     plt.subplots_adjust(right=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "eb1ae358",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T18:35:08.435020Z",
     "start_time": "2023-10-23T18:35:08.431685Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def illumEstOneFolder_prep():\n",
    "    images = Path(r\"data\\Training\\001_nonsleepyCombination\") #imgs\n",
    "#     landmarks = []\n",
    "#     fileNames = []\n",
    "#     illums = []\n",
    "    \n",
    "#     for img_file in images.glob(\"*.jpg\"):\n",
    "\n",
    "#         print(os.path.basename(img_file)[:-4])\n",
    "#         fileNames.append(os.path.basename(img_file)[:-4])\n",
    "#         landmarks.append(generateLandmarks(os.path.dirname(img_file) + \"\\\\\" + os.path.basename(img_file)))\n",
    "#         illums.append(illuminanceEstimation(img_file))\n",
    "#         retinexImplement(img_file)\n",
    "    \n",
    "#     ilu_dis = pd.DataFrame({'File Names': fileNames, 'Landmark Confidence': landmarks, 'Illuminance': illums})\n",
    "    \n",
    "#     print(ilu_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b6f26ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T15:56:07.572203Z",
     "start_time": "2023-10-23T15:56:07.566192Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def generateLandmarks(img):\n",
    "    img = cv2.imread(img)\n",
    "    detector = MTCNN()\n",
    "    output = detector.detect_faces(img)\n",
    "#     print(output[0]['confidence'])\n",
    "    \n",
    "    return output[0]['keypoints']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6a15479",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T15:56:08.074567Z",
     "start_time": "2023-10-23T15:56:08.069152Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# notes:\n",
    "\n",
    "# average 2.67s per image for generating landmarks and illuminance estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f9a8d5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T15:56:11.607598Z",
     "start_time": "2023-10-23T15:56:08.917214Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import URetinexNet\n",
    "from URetinex_Net import *\n",
    "\n",
    "# Import Arch_network\n",
    "import sys\n",
    "sys.path.append(r\"URetinex_Net\")\n",
    "\n",
    "# testing of URetinex.Net\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from network.Math_Module import P, Q\n",
    "from network.decom import Decom\n",
    "import os\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import time\n",
    "from utils import *\n",
    "\n",
    "def one2three(x):\n",
    "    return torch.cat([x, x, x], dim=1).to(x)\n",
    "\n",
    "class Inference(nn.Module):\n",
    "    #Class Inference Methods\n",
    "    def __init__(self, opts):\n",
    "        super().__init__()\n",
    "        self.opts = opts\n",
    "        # loading decomposition model \n",
    "        self.model_Decom_low = Decom()\n",
    "        self.model_Decom_low = load_initialize(self.model_Decom_low, self.opts.Decom_model_low_path)\n",
    "        # loading R; old_model_opts; and L model\n",
    "        self.unfolding_opts, self.model_R, self.model_L= load_unfolding(self.opts.unfolding_model_path)\n",
    "        # loading adjustment model\n",
    "        self.adjust_model = load_adjustment(self.opts.adjust_model_path)\n",
    "        self.P = P()\n",
    "        self.Q = Q()\n",
    "\n",
    "        transform = [\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "        self.transform = transforms.Compose(transform)\n",
    "        print(self.model_Decom_low)\n",
    "        print(self.model_R)\n",
    "        print(self.model_L)\n",
    "        print(self.adjust_model)\n",
    "        #time.sleep(8)\n",
    "\n",
    "    def unfolding(self, input_low_img):\n",
    "        for t in range(self.unfolding_opts.round):      \n",
    "            if t == 0: # initialize R0, L0\n",
    "                P, Q = self.model_Decom_low(input_low_img)\n",
    "            else: # update P and Q\n",
    "                w_p = (self.unfolding_opts.gamma + self.unfolding_opts.Roffset * t)\n",
    "                w_q = (self.unfolding_opts.lamda + self.unfolding_opts.Loffset * t)\n",
    "                P = self.P(I=input_low_img, Q=Q, R=R, gamma=w_p)\n",
    "                Q = self.Q(I=input_low_img, P=P, L=L, lamda=w_q) \n",
    "            R = self.model_R(r=P, l=Q)\n",
    "            L = self.model_L(l=Q)\n",
    "        return R, L\n",
    "    \n",
    "    def lllumination_adjust(self, L, ratio):\n",
    "        ratio = torch.ones(L.shape) * self.opts.ratio\n",
    "        return self.adjust_model(l=L, alpha=ratio)\n",
    "    \n",
    "    def forward(self, input_low_img):\n",
    "        if torch.cuda.is_available():\n",
    "            input_low_img = input_low_img\n",
    "        with torch.no_grad():\n",
    "            start = time.time()  \n",
    "            R, L = self.unfolding(input_low_img)\n",
    "            High_L = self.lllumination_adjust(L, self.opts.ratio)\n",
    "            I_enhance = High_L * R\n",
    "            p_time = (time.time() - start)\n",
    "        return I_enhance, p_time\n",
    "\n",
    "    def run(self, low_img_path):\n",
    "        file_name = os.path.basename(self.opts.img_path)\n",
    "        name = file_name.split('.')[0]\n",
    "        low_img = self.transform(Image.open(low_img_path)).unsqueeze(0)\n",
    "        enhance, p_time = self.forward(input_low_img=low_img)\n",
    "        if not os.path.exists(self.opts.output):\n",
    "            os.makedirs(self.opts.output)\n",
    "        save_path = os.path.join(self.opts.output, file_name.replace(name, \"%s_URetinexNet\"%(name)))\n",
    "        np_save_TensorImg(enhance, save_path)  \n",
    "        print(\"================================= time for %s: %f============================\"%(file_name, p_time))\n",
    "\n",
    "#         add to own function for input of img path\n",
    "def retinexImplement(img, outPath):\n",
    "    parser = argparse.ArgumentParser(description='Configure')\n",
    "    \n",
    "    # specify your data path here!\n",
    "    parser.add_argument('--img_path', type=str, default=img)\n",
    "    parser.add_argument('--output', type=str, default=outPath)\n",
    "    # ratio are recommended to be 3-5, bigger ratio will lead to over-exposure \n",
    "    parser.add_argument('--ratio', type=int, default=5)\n",
    "    # model path\n",
    "    parser.add_argument('--Decom_model_low_path', type=str, default=\"./URetinex_Net/ckpt/init_low.pth\")\n",
    "    parser.add_argument('--unfolding_model_path', type=str, default=\"./URetinex_Net/ckpt/unfolding.pth\")\n",
    "    parser.add_argument('--adjust_model_path', type=str, default=\"./URetinex_Net/ckpt/L_adjust.pth\")\n",
    "    parser.add_argument('--gpu_id', type=int, default=0)\n",
    "    \n",
    "#     opts = parser.parse_args() change parse_args() to parse_known_args\n",
    "    opts, _ = parser.parse_known_args()\n",
    "    for k, v in vars(opts).items():\n",
    "        print(k, v)\n",
    "    \n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(opts.gpu_id)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = Inference(opts)\n",
    "        print(\"CUDA (GPU) is available\")\n",
    "    else:\n",
    "        print(\"CUDA (GPU) is not available. Loading the model on CPU...\")\n",
    "        model = Inference(opts).to(torch.device('cpu'))\n",
    "    \n",
    "#    \n",
    "    model.run(opts.img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "679d8739",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T15:56:11.612008Z",
     "start_time": "2023-10-23T15:56:11.609590Z"
    }
   },
   "outputs": [],
   "source": [
    "# evalData_prep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e4da41d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T17:58:48.290986Z",
     "start_time": "2023-10-23T17:56:41.036472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_0_0.jpg\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "Illuminance: 0.29567792373263885\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_0_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 5\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "================================= time for 001_nonsleepyCombination_0_0.jpg: 3.803934============================\n",
      "Illuminance: 0.516026363376736\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_15_0.jpg\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "Illuminance: 0.2929762817534723\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_15_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 5\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================= time for 001_nonsleepyCombination_15_0.jpg: 4.013279============================\n",
      "Illuminance: 0.513045514422743\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_30_0.jpg\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "Illuminance: 0.29606498448350704\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_30_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 5\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "================================= time for 001_nonsleepyCombination_30_0.jpg: 3.665618============================\n",
      "Illuminance: 0.5178306256293403\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_45_0.jpg\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "Illuminance: 0.2991066410894097\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_45_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 5\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================= time for 001_nonsleepyCombination_45_0.jpg: 3.744013============================\n",
      "Illuminance: 0.5205153679947918\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_60_0.jpg\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "Illuminance: 0.29778815756076393\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_60_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 5\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "================================= time for 001_nonsleepyCombination_60_0.jpg: 3.775848============================\n",
      "Illuminance: 0.5188656798350694\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_75_0.jpg\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "Illuminance: 0.29540649650173617\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_75_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 5\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================= time for 001_nonsleepyCombination_75_0.jpg: 3.676301============================\n",
      "Illuminance: 0.5162518392838541\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_90_0.jpg\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "Illuminance: 0.2979124167057292\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_90_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 5\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "================================= time for 001_nonsleepyCombination_90_0.jpg: 3.692700============================\n",
      "Illuminance: 0.5190531985807292\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_105_0.jpg\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "Illuminance: 0.29994796240885413\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_105_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 5\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================= time for 001_nonsleepyCombination_105_0.jpg: 3.827613============================\n",
      "Illuminance: 0.5218593519053819\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_120_0.jpg\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "Illuminance: 0.29637356141927085\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_120_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 5\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "================================= time for 001_nonsleepyCombination_120_0.jpg: 3.645616============================\n",
      "Illuminance: 0.5163899465885416\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_135_0.jpg\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "Illuminance: 0.2969686096050347\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_135_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 5\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================= time for 001_nonsleepyCombination_135_0.jpg: 3.750235============================\n",
      "Illuminance: 0.5168914892708333\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_150_0.jpg\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "Illuminance: 0.2955887041015627\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_150_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 5\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainingData_prep()\n",
      "Cell \u001b[1;32mIn[7], line 57\u001b[0m, in \u001b[0;36mtrainingData_prep\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(videos_file)\n\u001b[0;32m     56\u001b[0m training_video_paths\u001b[38;5;241m.\u001b[39mappend(video_path)\n\u001b[1;32m---> 57\u001b[0m frame_capture(video_path)\n",
      "Cell \u001b[1;32mIn[17], line 93\u001b[0m, in \u001b[0;36mframe_capture\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m     90\u001b[0m                 trainIllumsRaw\u001b[38;5;241m.\u001b[39mappend(illuminanceEstimation(img_file))\n\u001b[0;32m     92\u001b[0m                 retSave_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/RetTraining/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m parentDir[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(file)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 93\u001b[0m                 retinexImplement(img_file, retSave_path)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m#               post-retinex functions\u001b[39;00m\n\u001b[0;32m     97\u001b[0m                 retSave_path \u001b[38;5;241m=\u001b[39m retSave_path \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(img_file)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_URetinexNet.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[14], line 118\u001b[0m, in \u001b[0;36mretinexImplement\u001b[1;34m(img, outPath)\u001b[0m\n\u001b[0;32m    115\u001b[0m         model \u001b[38;5;241m=\u001b[39m Inference(opts)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m#    \u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m     model\u001b[38;5;241m.\u001b[39mrun(opts\u001b[38;5;241m.\u001b[39mimg_path)\n",
      "Cell \u001b[1;32mIn[14], line 81\u001b[0m, in \u001b[0;36mInference.run\u001b[1;34m(self, low_img_path)\u001b[0m\n\u001b[0;32m     79\u001b[0m name \u001b[38;5;241m=\u001b[39m file_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     80\u001b[0m low_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(Image\u001b[38;5;241m.\u001b[39mopen(low_img_path))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 81\u001b[0m enhance, p_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(input_low_img\u001b[38;5;241m=\u001b[39mlow_img)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopts\u001b[38;5;241m.\u001b[39moutput):\n\u001b[0;32m     83\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopts\u001b[38;5;241m.\u001b[39moutput)\n",
      "Cell \u001b[1;32mIn[14], line 71\u001b[0m, in \u001b[0;36mInference.forward\u001b[1;34m(self, input_low_img)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     70\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \n\u001b[1;32m---> 71\u001b[0m     R, L \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munfolding(input_low_img)\n\u001b[0;32m     72\u001b[0m     High_L \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlllumination_adjust(L, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopts\u001b[38;5;241m.\u001b[39mratio)\n\u001b[0;32m     73\u001b[0m     I_enhance \u001b[38;5;241m=\u001b[39m High_L \u001b[38;5;241m*\u001b[39m R\n",
      "Cell \u001b[1;32mIn[14], line 52\u001b[0m, in \u001b[0;36mInference.unfolding\u001b[1;34m(self, input_low_img)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munfolding_opts\u001b[38;5;241m.\u001b[39mround):      \n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m# initialize R0, L0\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m         P, Q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_Decom_low(input_low_img)\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m# update P and Q\u001b[39;00m\n\u001b[0;32m     54\u001b[0m         w_p \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munfolding_opts\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munfolding_opts\u001b[38;5;241m.\u001b[39mRoffset \u001b[38;5;241m*\u001b[39m t)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\BriAN-D3\\URetinex_Net\\network\\decom.py:20\u001b[0m, in \u001b[0;36mDecom.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m---> 20\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecom(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m     21\u001b[0m     R \u001b[38;5;241m=\u001b[39m output[:, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m3\u001b[39m, :, :]\n\u001b[0;32m     22\u001b[0m     L \u001b[38;5;241m=\u001b[39m output[:, \u001b[38;5;241m3\u001b[39m:\u001b[38;5;241m4\u001b[39m, :, :]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:777\u001b[0m, in \u001b[0;36mLeakyReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 777\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mleaky_relu(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnegative_slope, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:1630\u001b[0m, in \u001b[0;36mleaky_relu\u001b[1;34m(input, negative_slope, inplace)\u001b[0m\n\u001b[0;32m   1628\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(leaky_relu, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, negative_slope\u001b[38;5;241m=\u001b[39mnegative_slope, inplace\u001b[38;5;241m=\u001b[39minplace)\n\u001b[0;32m   1629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m-> 1630\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mleaky_relu_(\u001b[38;5;28minput\u001b[39m, negative_slope)\n\u001b[0;32m   1631\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1632\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mleaky_relu(\u001b[38;5;28minput\u001b[39m, negative_slope)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainingData_prep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8df106c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T17:01:12.383036Z",
     "start_time": "2023-10-23T17:01:09.784468Z"
    }
   },
   "outputs": [],
   "source": [
    "# requires images converted to feature array\n",
    "\n",
    "# base_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "#               loss=tf.keras.losses.MeanSquaredError(),\n",
    "#               metrics=[tf.keras.metrics.Accuracy(),\n",
    "#                        tf.keras.metrics.Precision(),\n",
    "#                        tf.keras.metrics.Recall(),\n",
    "#                        tf.keras.metrics.F1Score()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e4c5e20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T17:16:30.639846Z",
     "start_time": "2023-10-23T17:16:30.144222Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_681\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_691 (InputLayer)      [(None, 224, 224, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)         1792      ['input_691[0][0]']           \n",
      "                                                                                                  \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)         36928     ['block1_conv1[0][0]']        \n",
      "                                                                                                  \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)         0         ['block1_conv2[0][0]']        \n",
      "                                                                                                  \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)        73856     ['block1_pool[0][0]']         \n",
      "                                                                                                  \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)        147584    ['block2_conv1[0][0]']        \n",
      "                                                                                                  \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)          0         ['block2_conv2[0][0]']        \n",
      "                                                                                                  \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)          295168    ['block2_pool[0][0]']         \n",
      "                                                                                                  \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)          590080    ['block3_conv1[0][0]']        \n",
      "                                                                                                  \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)          590080    ['block3_conv2[0][0]']        \n",
      "                                                                                                  \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)          0         ['block3_conv3[0][0]']        \n",
      "                                                                                                  \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)          1180160   ['block3_pool[0][0]']         \n",
      "                                                                                                  \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)          2359808   ['block4_conv1[0][0]']        \n",
      "                                                                                                  \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)          2359808   ['block4_conv2[0][0]']        \n",
      "                                                                                                  \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)          0         ['block4_conv3[0][0]']        \n",
      "                                                                                                  \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)          2359808   ['block4_pool[0][0]']         \n",
      "                                                                                                  \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)          2359808   ['block5_conv1[0][0]']        \n",
      "                                                                                                  \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)          2359808   ['block5_conv2[0][0]']        \n",
      "                                                                                                  \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)            0         ['block5_conv3[0][0]']        \n",
      "                                                                                                  \n",
      " additional_dense (Dense)    (None, 7, 7, 64)             32832     ['block5_pool[0][0]']         \n",
      "                                                                                                  \n",
      " illuminance_retinex_output  (None, 7, 7, 1)              65        ['additional_dense[0][0]']    \n",
      "  (Dense)                                                                                         \n",
      "                                                                                                  \n",
      " landmark_output (Dense)     (None, 7, 7, 2)              130       ['additional_dense[0][0]']    \n",
      "                                                                                                  \n",
      " previous_illuminance_outpu  (None, 7, 7, 1)              65        ['additional_dense[0][0]']    \n",
      " t (Dense)                                                                                        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14747780 (56.26 MB)\n",
      "Trainable params: 14747780 (56.26 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = tf.keras.applications.VGG16(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "\n",
    "additional_dense_layer = tf.keras.layers.Dense(64, activation='relu', name='additional_dense')(base_model.output)\n",
    "\n",
    "task_outputs = {\n",
    "    'landmark_detection': tf.keras.layers.Dense(2, activation='relu', name='landmark_output')(additional_dense_layer),\n",
    "    'previous_illuminance': tf.keras.layers.Dense(1, activation='relu', name='previous_illuminance_output')(additional_dense_layer),\n",
    "    'illuminance_retinex': tf.keras.layers.Dense(1, activation='relu', name='illuminance_retinex_output')(additional_dense_layer)\n",
    "}\n",
    "\n",
    "inputData = [trainLandmarks, trainIllumsRaw, trainIllumsRet]\n",
    "\n",
    "multi_task_model = tf.keras.Model(inputs=base_model.input, outputs=task_outputs)\n",
    "\n",
    "# Compile the model with specific loss functions and metrics for each task\n",
    "multi_task_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={\n",
    "        'landmark_detection': 'mean_squared_error',\n",
    "        'previous_illuminance': 'mean_squared_error',\n",
    "        'illuminance_retinex': 'mean_squared_error'\n",
    "    },\n",
    "    metrics={\n",
    "        'landmark_detection': 'mse',\n",
    "        'previous_illuminance': 'mse',\n",
    "        'illuminance_retinex': 'mse'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Summary of the multi-task model\n",
    "multi_task_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f6d29847",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T18:49:09.014941Z",
     "start_time": "2023-10-23T18:49:09.010178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "10\n",
      "[2.9568e-01 2.9298e-01 2.9606e-01 2.9911e-01 2.9779e-01 2.9541e-01\n",
      " 2.9791e-01 2.9995e-01 2.9637e-01 2.9697e-01]\n",
      "[5.1603e-01 5.1305e-01 5.1783e-01 5.2052e-01 5.1887e-01 5.1625e-01\n",
      " 5.1905e-01 5.2186e-01 5.1639e-01 5.1689e-01]\n"
     ]
    }
   ],
   "source": [
    "# multi_task_model.fit()\n",
    "# trainLandmarks = trainLandmarks[:-1]\n",
    "# trainIllumsRaw = trainIllumsRaw[:-1]\n",
    "print(len(trainLandmarks))\n",
    "print(len(trainIllumsRaw))\n",
    "print(len(trainIllumsRet))\n",
    "\n",
    "trainIllumsRawArray = np.array(trainIllumsRaw)\n",
    "trainIllumsRetArray = np.array(trainIllumsRet)\n",
    "\n",
    "print(trainIllumsRawArray)\n",
    "print(trainIllumsRetArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d888c6eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T18:19:53.156417Z",
     "start_time": "2023-10-23T18:19:53.151670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'left_eye': (213, 232), 'right_eye': (306, 222), 'nose': (239, 283), 'mouth_left': (227, 342), 'mouth_right': (298, 332)}\n",
      "  0.29567792373263885 0.516026363376736]\n",
      " [{'left_eye': (215, 230), 'right_eye': (307, 222), 'nose': (238, 285), 'mouth_left': (228, 339), 'mouth_right': (299, 333)}\n",
      "  0.2929762817534723 0.513045514422743]\n",
      " [{'left_eye': (207, 242), 'right_eye': (299, 228), 'nose': (236, 291), 'mouth_left': (226, 352), 'mouth_right': (296, 340)}\n",
      "  0.29606498448350704 0.5178306256293403]\n",
      " [{'left_eye': (218, 240), 'right_eye': (313, 232), 'nose': (245, 291), 'mouth_left': (231, 352), 'mouth_right': (304, 345)}\n",
      "  0.2991066410894097 0.5205153679947918]\n",
      " [{'left_eye': (223, 246), 'right_eye': (304, 238), 'nose': (242, 295), 'mouth_left': (234, 350), 'mouth_right': (302, 344)}\n",
      "  0.29778815756076393 0.5188656798350694]\n",
      " [{'left_eye': (214, 237), 'right_eye': (306, 233), 'nose': (241, 295), 'mouth_left': (227, 352), 'mouth_right': (298, 348)}\n",
      "  0.29540649650173617 0.5162518392838541]\n",
      " [{'left_eye': (216, 241), 'right_eye': (309, 230), 'nose': (245, 288), 'mouth_left': (231, 351), 'mouth_right': (303, 342)}\n",
      "  0.2979124167057292 0.5190531985807292]\n",
      " [{'left_eye': (225, 235), 'right_eye': (317, 232), 'nose': (249, 291), 'mouth_left': (235, 346), 'mouth_right': (305, 343)}\n",
      "  0.29994796240885413 0.5218593519053819]\n",
      " [{'left_eye': (214, 240), 'right_eye': (306, 231), 'nose': (243, 293), 'mouth_left': (229, 350), 'mouth_right': (300, 342)}\n",
      "  0.29637356141927085 0.5163899465885416]\n",
      " [{'left_eye': (214, 239), 'right_eye': (305, 232), 'nose': (240, 294), 'mouth_left': (227, 352), 'mouth_right': (299, 347)}\n",
      "  0.2969686096050347 0.5168914892708333]]\n"
     ]
    }
   ],
   "source": [
    "Y_train = np.vstack((trainLandmarks, trainIllumsRaw, trainIllumsRet)).T\n",
    "print(Y_train)\n",
    "\n",
    "# Sample 2D NumPy array\n",
    "data = Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6eff25d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T18:49:51.083904Z",
     "start_time": "2023-10-23T18:49:51.074617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[213 232 306 222 239 283 227 342 298 332]\n",
      " [215 230 307 222 238 285 228 339 299 333]\n",
      " [207 242 299 228 236 291 226 352 296 340]\n",
      " [218 240 313 232 245 291 231 352 304 345]\n",
      " [223 246 304 238 242 295 234 350 302 344]\n",
      " [214 237 306 233 241 295 227 352 298 348]\n",
      " [216 241 309 230 245 288 231 351 303 342]\n",
      " [225 235 317 232 249 291 235 346 305 343]\n",
      " [214 240 306 231 243 293 229 350 300 342]\n",
      " [214 239 305 232 240 294 227 352 299 347]]\n"
     ]
    }
   ],
   "source": [
    "# Sample 2D NumPy array\n",
    "data = Y_train\n",
    "\n",
    "# Extract numerical values from the dictionaries\n",
    "numerical_values = []\n",
    "\n",
    "for row in data:\n",
    "    row_values = []\n",
    "    for element in row:\n",
    "        if isinstance(element, dict):\n",
    "            # Extract numerical values from the dictionary\n",
    "            dict_values = [val for key, val in element.items() if isinstance(val, tuple)]\n",
    "            for tpl in dict_values:\n",
    "                for value in tpl:\n",
    "                    if isinstance(value, (int, float)):\n",
    "                        row_values.append(value)\n",
    "    numerical_values.append(row_values)\n",
    "\n",
    "# Print the numerical values\n",
    "# for row in numerical_values:\n",
    "#     print(row)\n",
    "\n",
    "numerical_values = np.array(numerical_values)\n",
    "print(numerical_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8a40ff52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T18:54:51.718265Z",
     "start_time": "2023-10-23T18:54:51.699035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255)  # Rescale pixel values to [0, 1]\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(r'data\\Training', target_size=(224, 224), batch_size=32, class_mode='input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "655648b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T18:54:54.657012Z",
     "start_time": "2023-10-23T18:54:54.622185Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'dict'> containing {\"<class 'str'>\"} keys and {\"<class 'keras.src.preprocessing.image.DirectoryIterator'>\"} values), (<class 'dict'> containing {\"<class 'str'>\"} keys and {\"<class 'numpy.ndarray'>\"} values)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[113], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m multi_task_model\u001b[38;5;241m.\u001b[39mfit({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_layer\u001b[39m\u001b[38;5;124m'\u001b[39m: train_generator},\n\u001b[0;32m      2\u001b[0m                                {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_task1\u001b[39m\u001b[38;5;124m'\u001b[39m: numerical_values, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_task2\u001b[39m\u001b[38;5;124m'\u001b[39m: trainIllumsRawArray, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_task3\u001b[39m\u001b[38;5;124m'\u001b[39m: trainIllumsRetArray},\n\u001b[0;32m      3\u001b[0m                                epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m      4\u001b[0m                                batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1105\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1102\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mcls\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ALL_ADAPTER_CLS \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mcan_handle(x, y)]\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m adapter_cls:\n\u001b[0;32m   1104\u001b[0m     \u001b[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[1;32m-> 1105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to find data adapter that can handle input: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1107\u001b[0m             _type_name(x), _type_name(y)\n\u001b[0;32m   1108\u001b[0m         )\n\u001b[0;32m   1109\u001b[0m     )\n\u001b[0;32m   1110\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(adapter_cls) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData adapters should be mutually exclusive for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhandling inputs. Found multiple adapters \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m to handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001b[0;32m   1115\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'dict'> containing {\"<class 'str'>\"} keys and {\"<class 'keras.src.preprocessing.image.DirectoryIterator'>\"} values), (<class 'dict'> containing {\"<class 'str'>\"} keys and {\"<class 'numpy.ndarray'>\"} values)"
     ]
    }
   ],
   "source": [
    "history = multi_task_model.fit({'input_layer': train_generator},\n",
    "                               {'output_task1': numerical_values, 'output_task2': trainIllumsRawArray, 'output_task3': trainIllumsRetArray},\n",
    "                               epochs=10,\n",
    "                               batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fab1e751",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T18:52:04.662969Z",
     "start_time": "2023-10-23T18:52:04.658486Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.1603e-01, 5.1305e-01, 5.1783e-01, 5.2052e-01, 5.1887e-01,\n",
       "       5.1625e-01, 5.1905e-01, 5.2186e-01, 5.1639e-01, 5.1689e-01])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_values\n",
    "trainIllumsRawArray\n",
    "trainIllumsRetArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6f354310",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T18:52:10.066732Z",
     "start_time": "2023-10-23T18:52:10.058482Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.9568e-01, 2.9298e-01, 2.9606e-01, 2.9911e-01, 2.9779e-01,\n",
       "       2.9541e-01, 2.9791e-01, 2.9995e-01, 2.9637e-01, 2.9697e-01])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainIllumsRawArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9e7e661e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T18:52:15.534992Z",
     "start_time": "2023-10-23T18:52:15.526848Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.1603e-01, 5.1305e-01, 5.1783e-01, 5.2052e-01, 5.1887e-01,\n",
       "       5.1625e-01, 5.1905e-01, 5.2186e-01, 5.1639e-01, 5.1689e-01])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainIllumsRetArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8295d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
