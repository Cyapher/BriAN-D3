{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f69663c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T08:04:18.173444Z",
     "start_time": "2023-10-24T08:04:07.927533Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from luxpy import (cat, colortf, _CIEOBS, _CIE_ILLUMINANTS, _CRI_RFL, _CIE_D65,_CIE_E,\n",
    "                   spd_to_xyz, plot_color_data, math, cie_interp, getwlr, xyz_to_srgb)\n",
    "from luxpy.utils import np, plt, sp, _PKG_PATH, _SEP, _EPS \n",
    "import warnings\n",
    "from imageio import imsave\n",
    "\n",
    "import tensorflow as tf\n",
    "from mtcnn import MTCNN\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import glob\n",
    "import cv2\n",
    "import csv\n",
    "import os\n",
    "import pydot\n",
    "import pydotplus\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f43c1c7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T08:04:25.745588Z",
     "start_time": "2023-10-24T08:04:18.176192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydotplus in c:\\users\\michael\\anaconda3\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.1 in c:\\users\\michael\\anaconda3\\lib\\site-packages (from pydotplus) (3.0.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bf89806",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T08:04:25.750851Z",
     "start_time": "2023-10-24T08:04:25.746629Z"
    }
   },
   "outputs": [],
   "source": [
    "trainLandmarks = []\n",
    "trainLandmarksRet = []\n",
    "trainFileNames = []\n",
    "trainIllumsRaw = []\n",
    "trainIllumsRet = []\n",
    "evalLandmarks = []\n",
    "evalLandmarksRet = []\n",
    "evalFileNames = []\n",
    "evalIllumsRaw = []\n",
    "evalIllumsRet = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "096c8392",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T08:04:25.777396Z",
     "start_time": "2023-10-24T08:04:25.755113Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def frame_capture(file):\n",
    "    \n",
    "    cap = cv2.VideoCapture(file)\n",
    "    \n",
    "    try:\n",
    "        if not os.path.exists('data'):\n",
    "            os.makedirs('data')\n",
    "    except OSError:\n",
    "        print('Error: Creating directory of data')\n",
    "            \n",
    "    current_frame = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "#         print(file)\n",
    "        if not ret:\n",
    "            current_frame = 0\n",
    "            break \n",
    "\n",
    "        if current_frame % 15 == 0:\n",
    "            \n",
    "            datasetDir = os.path.dirname(os.path.dirname(os.path.dirname(file)))\n",
    "            indexDataset = datasetDir.rfind('\\\\')\n",
    "            datasetDir = datasetDir[indexDataset:]\n",
    "            \n",
    "            parentDir = os.path.dirname(os.path.dirname(file)) # refers to <driver_num> e.g. \"001, 002\"\n",
    "            currDir = parentDir\n",
    "            \n",
    "            childDir = os.path.dirname(file) # refers to where the current video file is <scenario> e.g. noglasses, glasses, etc.\n",
    "            indexParent = parentDir.rfind('\\\\')\n",
    "            parentDir = parentDir[indexParent:] # refers to <scenario> only\n",
    "            \n",
    "            indexChild = childDir.rfind('\\\\')\n",
    "            childDir = childDir[indexChild:]\n",
    "            \n",
    "#             filename of txt document containing labels for current video\n",
    "            if datasetDir[1:] == \"Training Dataset\":\n",
    "                labelFile = currDir + \"\\\\\" + childDir[1:] + \"\\\\\" + parentDir[1:] + \"_\" + os.path.basename(file)[:-4] + \"_drowsiness\" + '.txt'\n",
    "            elif parentDir[1:] == \"Evaluation Dataset\":\n",
    "                labelFile = currDir + \"\\\\\" + childDir[1:] + \"\\\\\" + os.path.basename(file)[:-4] + \"_drowsiness\" + '.txt'\n",
    "                labelFile = labelFile.replace(\"mix\", \"mixing\")\n",
    "                labelFile = labelFile.replace(\"nightno\", \"night_no\")\n",
    "            else:\n",
    "                scenario = \"\"\n",
    "                if \"sunglasses\" in os.path.basename(file)[:-4]:\n",
    "                    scenario = \"sunglasses\"\n",
    "                elif \"night_noglasses\" in os.path.basename(file)[:-4]:\n",
    "                    scenario = \"night_noglasses\"    \n",
    "                elif \"noglasses\" in os.path.basename(file)[:-4]:\n",
    "                    scenario = \"noglasses\"\n",
    "                elif \"night_glasses\" in os.path.basename(file)[:-4]:\n",
    "                    scenario = \"sunglasses\"\n",
    "                else:\n",
    "                    scenario = \"glasses\"\n",
    "                \n",
    "                labelFile = currDir + \"\\\\\" + childDir[1:] + \"\\\\\" + \"test_label_txt\\\\\" + scenario + \"\\\\\" + os.path.basename(file)[:-4] + \"_drowsiness\" + '.txt'\n",
    "                \n",
    "                if not os.path.exists(labelFile):                    \n",
    "                    labelFile = currDir + \"\\\\\" + childDir[1:] + \"\\\\\" + \"test_label_txt\\\\\" + \"wh\" + \"\\\\\" + os.path.basename(file)[:-4] + \"_drowsiness\" + '.txt'\n",
    "                \n",
    "                labelFile = labelFile.replace(\"mix\", \"mixing\")\n",
    "                \n",
    "            with open(labelFile) as f:\n",
    "                labels = f.readline()\n",
    "            try:\n",
    "                \n",
    "                if datasetDir[1:] == \"Training Dataset\":\n",
    "                    save_path = \"./data/Training/\" # specify where (in relation to root path to place new folder)\n",
    "                    file_name = save_path + parentDir[1:] + \"_\" + os.path.basename(file)[:-4] + \"/\" + parentDir[1:] + \"_\" + os.path.basename(file)[:-4] + \"_\" + str(current_frame) + \"_\" + labels[current_frame] + '.jpg'\n",
    "                elif parentDir[1:] == \"Evaluation Dataset\":\n",
    "                    save_path = \"./data/Evaluation/\" # specify where (in relation to root path to place new folder)\n",
    "                    file_name = save_path + os.path.basename(file)[:-4] + \"/\" + os.path.basename(file)[:-4] + \"_\" + str(current_frame) + \"_\" + labels[current_frame] + '.jpg'\n",
    "                else:\n",
    "                    save_path = \"./data/Testing/\" # specify where (in relation to root path to place new folder)\n",
    "                    file_name = save_path + os.path.basename(file)[:-4] + \"/\" + os.path.basename(file)[:-4] + \"_\" + str(current_frame) + \"_\" + labels[current_frame] + '.jpg'\n",
    "                \n",
    "#                 Creating..../data/Testing/037_sunglasses_mix/037_sunglasses_mix_7110_1.jpg\n",
    "                \n",
    "            except IndexError:\n",
    "                continue\n",
    "            \n",
    "            print('Creating...' + file_name)\n",
    "            cv2.imwrite(file_name, frame)\n",
    "            \n",
    "            img_file = Path(file_name) # for Retinex images, use replace() to get path of same named images in order to get post-retinex illum\n",
    "            \n",
    "            if save_path == \"./data/Training/\":\n",
    "                trainFileNames.append(os.path.basename(img_file)[:-4])\n",
    "                trainLandmarks.append(generateLandmarks(os.path.dirname(img_file) + \"\\\\\" + os.path.basename(img_file)))\n",
    "                trainIllumsRaw.append(illuminanceEstimation(img_file))\n",
    "                \n",
    "                retSave_path = \"./data/RetTraining/\" + parentDir[1:] + \"_\" + os.path.basename(file)[:-4] + \"/\"\n",
    "                retinexImplement(img_file, retSave_path)\n",
    "                \n",
    "#               post-retinex functions\n",
    "                \n",
    "                retSave_path = retSave_path + os.path.basename(img_file)[:-4] + \"_URetinexNet.jpg\"\n",
    "                trainIllumsRet.append(illuminanceEstimation(retSave_path))\n",
    "            \n",
    "                trainLandmarksRet.append(generateLandmarks(retSave_path))\n",
    "                \n",
    "                \n",
    "\n",
    "            elif save_path == \"./data/Evaluation/\":\n",
    "                evalFileNames.append(os.path.basename(img_file)[:-4])\n",
    "                evalLandmarks.append(generateLandmarks(os.path.dirname(img_file) + \"\\\\\" + os.path.basename(img_file)))\n",
    "                evalIllumsRaw.append(illuminanceEstimation(img_file))\n",
    "                \n",
    "                retSave_path = \"./data/RetEvaluation/\" + parentDir[1:] + \"_\" + os.path.basename(file)[:-4] + \"/\"\n",
    "                retinexImplement(img_file, retSave_path)\n",
    "                \n",
    "#               post-retinex functions\n",
    "                \n",
    "                retSave_path = retSave_path + os.path.basename(img_file)[:-4] + \"_URetinexNet.jpg\"\n",
    "                evalIllumsRet.append(illuminanceEstimation(retSave_path))\n",
    "            \n",
    "                evalLandmarksRet.append(generateLandmarks(retSave_path))\n",
    "            \n",
    "            \n",
    "        current_frame += 1\n",
    "    \n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcf88578",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T08:04:25.789640Z",
     "start_time": "2023-10-24T08:04:25.777396Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Training Videos Path\n",
    "def trainingData_prep():\n",
    "    training_videos = Path(r\"NTHU Dataset\\Training_Evaluation_Dataset\\Training Dataset\") #AVIs\n",
    "    training_video_paths = []\n",
    "\n",
    "    # create data folder\n",
    "    try:\n",
    "        if not os.path.exists('data'):\n",
    "                os.makedirs('data')\n",
    "                os.mkdir(os.path.join('./data/', 'Training'))\n",
    "                os.mkdir(os.path.join('./data/', 'Evaluation'))\n",
    "                os.mkdir(os.path.join('./data/', 'Testing'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetTraining'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetEvaluation'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetTesting'))\n",
    "    except OSError:\n",
    "        print('Error: Creating directory of data')\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for driver in training_videos.glob(\"*\"):\n",
    "        for scenario in driver.glob(\"*\"):\n",
    "            for videos_file in scenario.glob(\"*.avi\"):\n",
    "\n",
    "    #         note: videos_file refers to direct path of current video file\n",
    "\n",
    "                datasetDir = os.path.dirname(os.path.dirname(os.path.dirname(videos_file)))\n",
    "                indexDataset = datasetDir.rfind('\\\\')\n",
    "                datasetDir = datasetDir[indexDataset:]\n",
    "\n",
    "                parentDir = os.path.dirname(os.path.dirname(videos_file)) # refers to <driver_num> e.g. \"001, 002\"\n",
    "                currDir = parentDir\n",
    "\n",
    "                childDir = os.path.dirname(videos_file) # refers to where the current video file is <scenario> e.g. noglasses, glasses, etc.\n",
    "                indexParent = parentDir.rfind('\\\\')\n",
    "                parentDir = parentDir[indexParent:] # refers to <scenario> only\n",
    "\n",
    "                indexChild = childDir.rfind('\\\\')\n",
    "                childDir = childDir[indexChild:]\n",
    "\n",
    "                if datasetDir[1:] == \"Training Dataset\":\n",
    "                    save_path = \"./data/Training/\" # specify where (in relation to root path to place new folder)\n",
    "                    folder_name = parentDir[1:] + \"_\" + os.path.basename(videos_file)[:-4] # specify name of new folder\n",
    "                elif parentDir[1:] == \"Evaluation Dataset\":\n",
    "                    save_path = \"./data/Evaluation/\" # specify where (in relation to root path to place new folder)\n",
    "                    folder_name = os.path.basename(videos_file)[:-4] # specify name of new folder\n",
    "\n",
    "                data_path = os.path.join(save_path, folder_name) # set path and folder name as input for os.path.join()\n",
    "\n",
    "                inputPath = save_path + folder_name\n",
    "\n",
    "                if not os.path.exists(inputPath):\n",
    "                    os.mkdir(data_path) # create new folder based on data_path\n",
    "\n",
    "                video_path = str(videos_file)\n",
    "                training_video_paths.append(video_path)\n",
    "                frame_capture(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "732f5fa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T08:04:25.802587Z",
     "start_time": "2023-10-24T08:04:25.789640Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluation Videos Path\n",
    "def evalData_prep():\n",
    "    evaluation_videos = Path(r\"NTHU Dataset\\Training_Evaluation_Dataset\\Evaluation Dataset\") #AVIs\n",
    "    evaluation_video_paths = []\n",
    "\n",
    "    # create data folder\n",
    "    try:\n",
    "        if not os.path.exists('data'):\n",
    "                os.makedirs('data')\n",
    "                os.mkdir(os.path.join('./data/', 'Training'))\n",
    "                os.mkdir(os.path.join('./data/', 'Evaluation'))\n",
    "                os.mkdir(os.path.join('./data/', 'Testing'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetTraining'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetEvaluation'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetTesting'))\n",
    "    except OSError:\n",
    "        print('Error: Creating directory of data')\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for driver in evaluation_videos.glob(\"*\"):\n",
    "        for videos_file in driver.glob(\"*.mp4\"):\n",
    "\n",
    "    #         note: videos_file refers to direct path of current video file\n",
    "\n",
    "            datasetDir = os.path.dirname(os.path.dirname(os.path.dirname(videos_file)))\n",
    "            indexDataset = datasetDir.rfind('\\\\')\n",
    "            datasetDir = datasetDir[indexDataset:]\n",
    "\n",
    "            parentDir = os.path.dirname(os.path.dirname(videos_file)) # refers to <driver_num> e.g. \"001, 002\"\n",
    "            currDir = parentDir\n",
    "\n",
    "            childDir = os.path.dirname(videos_file) # refers to where the current video file is <scenario> e.g. noglasses, glasses, etc.\n",
    "            indexParent = parentDir.rfind('\\\\')\n",
    "            parentDir = parentDir[indexParent:] # refers to <scenario> only\n",
    "\n",
    "            indexChild = childDir.rfind('\\\\')\n",
    "            childDir = childDir[indexChild:]\n",
    "\n",
    "            if datasetDir[1:] == \"Training Dataset\":\n",
    "                save_path = \"./data/Training/\" # specify where (in relation to root path to place new folder)\n",
    "                folder_name = parentDir[1:] + \"_\" + os.path.basename(videos_file)[:-4] # specify name of new folder\n",
    "            elif parentDir[1:] == \"Evaluation Dataset\":\n",
    "                save_path = \"./data/Evaluation/\" # specify where (in relation to root path to place new folder)\n",
    "                folder_name = os.path.basename(videos_file)[:-4] # specify name of new folder\n",
    "\n",
    "            data_path = os.path.join(save_path, folder_name) # set path and folder name as input for os.path.join()\n",
    "\n",
    "            inputPath = save_path + folder_name\n",
    "\n",
    "            if not os.path.exists(inputPath):\n",
    "                os.mkdir(data_path) # create new folder based on data_path\n",
    "\n",
    "            video_path = str(videos_file)\n",
    "            evaluation_video_paths.append(video_path)\n",
    "            frame_capture(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2243a300",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T08:04:25.812831Z",
     "start_time": "2023-10-24T08:04:25.805597Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Testing Videos Path\n",
    "def testData_prep():\n",
    "    testing_videos = Path(r\"NTHU Dataset\\Testing_Dataset\") #MP4s\n",
    "    testing_video_paths = []\n",
    "\n",
    "    # create data folder\n",
    "    try:\n",
    "        if not os.path.exists('data'):\n",
    "                os.makedirs('data')\n",
    "                os.mkdir(os.path.join('./data/', 'Training'))\n",
    "                os.mkdir(os.path.join('./data/', 'Evaluation'))\n",
    "                os.mkdir(os.path.join('./data/', 'Testing'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetTraining'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetEvaluation'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetTesting'))\n",
    "    except OSError:\n",
    "        print('Error: Creating directory of data')\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for videos_file in testing_videos.glob(\"*.mp4\"):\n",
    "        print(videos_file)\n",
    "\n",
    "#         note: videos_file refers to direct path of current video file\n",
    "\n",
    "        print(os.path.dirname(videos_file))\n",
    "\n",
    "        datasetDir = os.path.dirname(videos_file)\n",
    "        indexDataset = datasetDir.rfind('\\\\')\n",
    "        datasetDir = datasetDir[indexDataset:]\n",
    "\n",
    "        save_path = \"./data/Testing/\" # specify where (in relation to root path to place new folder)\n",
    "        folder_name = os.path.basename(videos_file)[:-4] # specify name of new folder\n",
    "\n",
    "        print(save_path + folder_name)\n",
    "\n",
    "        data_path = os.path.join(save_path, folder_name) # set path and folder name as input for os.path.join()\n",
    "\n",
    "        inputPath = save_path + folder_name\n",
    "\n",
    "        if not os.path.exists(inputPath):\n",
    "            os.mkdir(data_path) # create new folder based on data_path\n",
    "\n",
    "        video_path = str(videos_file)\n",
    "        testing_video_paths.append(video_path)\n",
    "        frame_capture(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0936f309",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T08:04:25.876980Z",
     "start_time": "2023-10-24T08:04:25.816001Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*- ILLUM EST FUNCTIONS\n",
    "\"\"\"\n",
    "Module for hyper spectral image simulation\n",
    "==========================================\n",
    "\n",
    " :_HYPSPCIM_PATH: path to module\n",
    "\n",
    " :_HYPSPCIM_DEFAULT_IMAGE: path + filename to default image\n",
    " \n",
    " :_CSF_NIKON_D700: Nikon D700 camera sensitivity functions\n",
    " \n",
    " :_ROUNDING: rounding of input to xyz_to_rfl() search algorithm for improved speed\n",
    "\n",
    " :xyz_to_rfl(): approximate spectral reflectance of xyz based on k nearest \n",
    "                neighbour interpolation of samples from a standard reflectance \n",
    "                set.\n",
    "\n",
    " :render_image(): Render image under specified light source spd.\n",
    "\n",
    " :get_superresolution_hsi(): Get a HighResolution HyperSpectral Image (super-resolution HSI) based on a LowResolution HSI and a HighResolution Color Image.\n",
    "\n",
    " :hsi_to_rgb(): Convert HyperSpectral Image to rgb\n",
    " \n",
    " :rfl_to_rgb(): Convert spectral reflectance functions (illuminated by spd) to Camera Sensitivity Functions.\n",
    "     \n",
    ".. codeauthor:: Kevin A.G. Smet (ksmet1977 at gmail.com)\n",
    "\"\"\"\n",
    "\n",
    "from luxpy import (cat, colortf, _CIEOBS, _CIE_ILLUMINANTS, _CRI_RFL, _CIE_D65,_CIE_E,\n",
    "                   spd_to_xyz, plot_color_data, math, cie_interp, getwlr, xyz_to_srgb)\n",
    "from luxpy.utils import np, plt, sp, _PKG_PATH, _SEP, _EPS \n",
    "\n",
    "import warnings\n",
    "from imageio import imsave\n",
    "\n",
    "illum_out = 0\n",
    "\n",
    "__all__ =['_HYPSPCIM_PATH','_HYPSPCIM_DEFAULT_IMAGE','render_image','xyz_to_rfl',\n",
    "          'get_superresolution_hsi','hsi_to_rgb','rfl_to_rgb','_CSF_NIKON_D700']             \n",
    "\n",
    "_HYPSPCIM_PATH = _PKG_PATH + _SEP + 'hypspcim' + _SEP\n",
    "_HYPSPCIM_DEFAULT_IMAGE = _PKG_PATH + _SEP + 'toolboxes' + _SEP + 'hypspcim' +  _SEP + 'data' + _SEP + 'testimage1.jpg'\n",
    "\n",
    "\n",
    "_ROUNDING = 6 # to speed up xyz_to_rfl search algorithm, increase if kernel dies!!!\n",
    "\n",
    "# Nikon D700 camera sensitivity functions:\n",
    "_CSF_NIKON_D700 = np.vstack((np.arange(400,710,10),\n",
    "                             np.array([[0.005, 0.007, 0.012, 0.015, 0.023, 0.025, 0.030, 0.026, 0.024, 0.019, 0.010, 0.004, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  0.000,  0.000,  0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000], \n",
    "                                       [0.000, 0.000, 0.000, 0.000, 0.000, 0.001, 0.002, 0.003, 0.005, 0.007, 0.012, 0.013, 0.015, 0.016, 0.017, 0.020, 0.013, 0.011, 0.009, 0.005,  0.001,  0.001,  0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.002, 0.002, 0.003],\n",
    "                                       [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.001, 0.003, 0.010, 0.012,  0.013,  0.022,  0.020, 0.020, 0.018, 0.017, 0.016, 0.016, 0.014, 0.014, 0.013]])[::-1]))\n",
    "\n",
    "\n",
    "def xyz_to_rfl(xyz, CSF = None, rfl = None, out = 'rfl_est', \\\n",
    "                 refspd = None, D = None, cieobs = _CIEOBS, \\\n",
    "                 cspace = 'xyz', cspace_tf = {},\\\n",
    "                 interp_type = 'nd', k_neighbours = 4, verbosity = 0,\n",
    "                 csf_based_rgb_rounding = _ROUNDING):\n",
    "    \"\"\"\n",
    "    Approximate spectral reflectance of xyz values based on nd-dimensional linear interpolation \n",
    "    or k nearest neighbour interpolation of samples from a standard reflectance set.\n",
    "    \n",
    "    Args:\n",
    "        :xyz: \n",
    "            | ndarray with xyz values of target points.\n",
    "        :CSF:\n",
    "            | None, optional\n",
    "            | RGB camera response functions.\n",
    "            | If None: input :xyz: contains raw rgb (float) values. Override :cspace:\n",
    "            | argument and perform estimation directly in raw rgb space!!!\n",
    "        :rfl: \n",
    "            | ndarray, optional\n",
    "            | Reflectance set for color coordinate to rfl mapping.\n",
    "        :out: \n",
    "            | 'rfl_est' or str, optional\n",
    "        :refspd: \n",
    "            | None, optional\n",
    "            | Refer ence spectrum for color coordinate to rfl mapping.\n",
    "            | None defaults to D65.\n",
    "        :cieobs:\n",
    "            | _CIEOBS, optional\n",
    "            | CMF set used for calculation of xyz from spectral data.\n",
    "        :cspace:\n",
    "            | 'xyz',  optional\n",
    "            | Color space for color coordinate to rfl mapping.\n",
    "            | Tip: Use linear space (e.g. 'xyz', 'Yuv',...) for (interp_type == 'nd'),\n",
    "            |      and perceptually uniform space (e.g. 'ipt') for (interp_type == 'nearest')\n",
    "        :cspace_tf:\n",
    "            | {}, optional\n",
    "            | Dict with parameters for xyz_to_cspace and cspace_to_xyz transform.\n",
    "        :interp_type:\n",
    "            | 'nd', optional\n",
    "            | Options:\n",
    "            | - 'nd': perform n-dimensional linear interpolation using Delaunay triangulation.\n",
    "            | - 'nearest': perform nearest neighbour interpolation. \n",
    "        :k_neighbours:\n",
    "            | 4 or int, optional\n",
    "            | Number of nearest neighbours for reflectance spectrum interpolation.\n",
    "            | Neighbours are found using scipy.spatial.cKDTree\n",
    "        :verbosity:\n",
    "            | 0, optional\n",
    "            | If > 0: make a plot of the color coordinates of original and \n",
    "            | rendered image pixels.\n",
    "        :csf_based_rgb_rounding:\n",
    "            | _ROUNDING, optional\n",
    "            | Int representing the number of decimals to round the RGB values (obtained from not-None CSF input) to before applying the search algorithm.\n",
    "            | Smaller values increase the search speed, but could cause fatal error that causes python kernel to die. If this happens increase the rounding int value.\n",
    "\n",
    "    Returns:\n",
    "        :returns: \n",
    "            | :rfl_est:\n",
    "            | ndarrays with estimated reflectance spectra.\n",
    "    \"\"\"\n",
    "\n",
    "    # get rfl set:\n",
    "    if rfl is None: # use IESTM30['4880'] set \n",
    "        rfl = _CRI_RFL['ies-tm30']['4880']['5nm']\n",
    "    \n",
    "    wlr = rfl[0]\n",
    "    \n",
    "    # get Ref spd:\n",
    "    if refspd is None:\n",
    "        refspd = _CIE_ILLUMINANTS['D65'].copy()\n",
    "    refspd = cie_interp(refspd, wlr, kind = 'linear') # force spd to same wavelength range as rfl\n",
    "        \n",
    "    # Calculate rgb values of standard rfl set under refspd:\n",
    "    if CSF is None:\n",
    "        # Calculate lab coordinates:\n",
    "        xyz_rr, xyz_wr = spd_to_xyz(refspd, relative = True, rfl = rfl, cieobs = cieobs, out = 2)\n",
    "        cspace_tf_copy = cspace_tf.copy()\n",
    "        cspace_tf_copy['xyzw'] = xyz_wr # put correct white point in param. dict\n",
    "        lab_rr = colortf(xyz_rr, tf = cspace, fwtf = cspace_tf_copy, bwtf = cspace_tf_copy)[:,0,:]\n",
    "    else:\n",
    "        # Calculate rgb coordinates from camera sensitivity functions\n",
    "        rgb_rr = rfl_to_rgb(rfl, spd = refspd, CSF = CSF, wl = None)   \n",
    "        lab_rr = rgb_rr\n",
    "        xyz = xyz\n",
    "        lab_rr = np.round(lab_rr,csf_based_rgb_rounding) # speed up search\n",
    "        \n",
    "        global illum_out\n",
    "        illum_out = np.mean(lab_rr)\n",
    "        print(\"Illuminance: \" + str(np.mean(lab_rr)))\n",
    "        \n",
    "    # Convert xyz to lab-type values under refspd:\n",
    "    if CSF is None:\n",
    "        lab = colortf(xyz, tf = cspace, fwtf = cspace_tf_copy, bwtf = cspace_tf_copy)\n",
    "    else:\n",
    "        lab = xyz # xyz contained rgb values !!!\n",
    "        rgb = xyz\n",
    "        lab = np.round(lab,csf_based_rgb_rounding) # speed up search\n",
    "    \n",
    "    if interp_type == 'nearest':\n",
    "        # Find rfl (cfr. lab_rr) from rfl set that results in 'near' metameric \n",
    "        # color coordinates for each value in lab_ur (i.e. smallest DE):\n",
    "        # Construct cKDTree:\n",
    "        tree = sp.spatial.cKDTree(lab_rr, copy_data = True)\n",
    "        \n",
    "        # Interpolate rfls using k nearest neightbours and inverse distance weigthing:\n",
    "        d, inds = tree.query(lab, k = k_neighbours )\n",
    "        if k_neighbours  > 1:\n",
    "            d += _EPS\n",
    "            w = (1.0 / d**2)[:,:,None] # inverse distance weigthing\n",
    "            rfl_est = np.sum(w * rfl[inds+1,:], axis=1) / np.sum(w, axis=1)\n",
    "        else:\n",
    "            rfl_est = rfl[inds+1,:].copy()\n",
    "    elif interp_type == 'nd':\n",
    "\n",
    "        rfl_est = math.ndinterp1_scipy(lab_rr, rfl[1:], lab)\n",
    "            \n",
    "        _isnan = np.isnan(rfl_est[:,0]) \n",
    "\n",
    "        if (_isnan.any()): #do nearest neigbour method for those that fail using Delaunay (i.e. ndinterp1_scipy)\n",
    "\n",
    "            # Find rfl (cfr. lab_rr) from rfl set that results in 'near' metameric \n",
    "            # color coordinates for each value in lab_ur (i.e. smallest DE):\n",
    "            # Construct cKDTree:\n",
    "            tree = sp.spatial.cKDTree(lab_rr, copy_data = True)\n",
    "\n",
    "            # Interpolate rfls using k nearest neightbours and inverse distance weigthing:\n",
    "            d, inds = tree.query(lab[_isnan,...], k = k_neighbours )\n",
    "\n",
    "            if k_neighbours  > 1:\n",
    "                d += _EPS\n",
    "                w = (1.0 / d**2)[:,:,None] # inverse distance weigthing\n",
    "                rfl_est_isnan = np.sum(w * rfl[inds+1,:], axis=1) / np.sum(w, axis=1)\n",
    "            else:\n",
    "                rfl_est_isnan = rfl[inds+1,:].copy()\n",
    "            rfl_est[_isnan, :] = rfl_est_isnan\n",
    "\n",
    "    else:\n",
    "        raise Exception('xyz_to_rfl(): unsupported interp_type!')\n",
    "    \n",
    "    rfl_est[rfl_est<0] = 0 #can occur for points outside convexhull of standard rfl set.\n",
    "\n",
    "    rfl_est = np.vstack((rfl[0],rfl_est))\n",
    "        \n",
    "    if ((verbosity > 0) | ('xyz_est' in out.split(',')) | ('lab_est' in out.split(',')) | ('DEi_ab' in out.split(',')) | ('DEa_ab' in out.split(','))) & (CSF is None):\n",
    "        xyz_est, _ = spd_to_xyz(refspd, rfl = rfl_est, relative = True, cieobs = cieobs, out = 2)\n",
    "        cspace_tf_copy = cspace_tf.copy()\n",
    "        cspace_tf_copy['xyzw'] = xyz_wr # put correct white point in param. dict\n",
    "        lab_est = colortf(xyz_est, tf = cspace, fwtf = cspace_tf_copy)[:,0,:]\n",
    "        DEi_ab = np.sqrt(((lab_est[:,1:3]-lab[:,1:3])**2).sum(axis=1))\n",
    "        DEa_ab = DEi_ab.mean()\n",
    "    elif ((verbosity > 0) | ('xyz_est' in out.split(',')) | ('rgb_est' in out.split(',')) | ('DEi_rgb' in out.split(',')) | ('DEa_rgb' in out.split(','))) & (CSF is not None):\n",
    "        rgb_est = rfl_to_rgb(rfl_est[1:], spd = refspd, CSF = CSF, wl = wlr) \n",
    "        xyz_est = rgb_est\n",
    "        DEi_rgb = np.sqrt(((rgb_est - rgb)**2).sum(axis=1))\n",
    "        DEa_rgb = DEi_rgb.mean()\n",
    "\n",
    "        \n",
    "    if verbosity > 0:\n",
    "        if CSF is None:\n",
    "            ax = plot_color_data(lab[...,1], lab[...,2], z = lab[...,0], \\\n",
    "                            show = False, cieobs = cieobs, cspace = cspace, \\\n",
    "                            formatstr = 'ro', label = 'Original')\n",
    "            plot_color_data(lab_est[...,1], lab_est[...,2], z = lab_est[...,0], \\\n",
    "                            show = True, axh = ax, cieobs = cieobs, cspace = cspace, \\\n",
    "                            formatstr = 'bd', label = 'Rendered')\n",
    "        else:\n",
    "            n = 100 #min(rfl.shape[0]-1,rfl_est.shape[0]-1)\n",
    "            s = np.random.permutation(rfl.shape[0]-1)[:min(n,rfl.shape[0]-1)]\n",
    "            st = np.random.permutation(rfl_est.shape[0]-1)[:min(n,rfl_est.shape[0]-1)]\n",
    "            fig = plt.figure()\n",
    "            ax = np.zeros((3,),dtype=np.object)\n",
    "            ax[0] = fig.add_subplot(131)\n",
    "            ax[1] = fig.add_subplot(132)\n",
    "            ax[2] = fig.add_subplot(133,projection='3d')\n",
    "            ax[0].plot(rfl[0],rfl[1:][s].T, linestyle = '-')\n",
    "            ax[0].set_title('Original RFL set (random selection of all)')\n",
    "            ax[0].set_ylim([0,1])\n",
    "            ax[1].plot(rfl_est[0],rfl_est[1:][st].T, linestyle = '--')\n",
    "            ax[0].set_title('Estimated RFL set (random selection of targets)')\n",
    "            ax[1].set_ylim([0,1])\n",
    "            ax[2].plot(rgb[st,0],rgb[st,1],rgb[st,2],'ro', label = 'Original')\n",
    "            ax[2].plot(rgb_est[st,0],rgb_est[st,1],rgb_est[st,2],'bd', label = 'Rendered')\n",
    "            ax[2].legend()\n",
    "    if out == 'rfl_est':\n",
    "        return rfl_est\n",
    "    elif out == 'rfl_est,xyz_est':\n",
    "        return rfl_est, xyz_est\n",
    "    else:\n",
    "        return eval(out)\n",
    "\n",
    "\n",
    "\n",
    "def render_image(img = None, spd = None, rfl = None, out = 'img_hyp', \\\n",
    "                 refspd = None, D = None, cieobs = _CIEOBS, \\\n",
    "                 cspace = 'xyz', cspace_tf = {}, CSF = None,\\\n",
    "                 interp_type = 'nd', k_neighbours = 4, show = True,\n",
    "                 verbosity = 0, show_ref_img = True,\\\n",
    "                 stack_test_ref = 12,\\\n",
    "                 write_to_file = None,\\\n",
    "                 csf_based_rgb_rounding = _ROUNDING):\n",
    "    \"\"\"\n",
    "    Render image under specified light source spd.\n",
    "    \n",
    "    Args:\n",
    "        :img: \n",
    "            | None or str or ndarray with float (max = 1) rgb image.\n",
    "            | None load a default image.\n",
    "        :spd: \n",
    "            | ndarray, optional\n",
    "            | Light source spectrum for rendering\n",
    "            | If None: use CIE illuminant F4\n",
    "        :rfl: \n",
    "            | ndarray, optional\n",
    "            | Reflectance set for color coordinate to rfl mapping.\n",
    "        :out: \n",
    "            | 'img_hyp' or str, optional\n",
    "            |  (other option: 'img_ren': rendered image under :spd:)\n",
    "        :refspd:\n",
    "            | None, optional\n",
    "            | Reference spectrum for color coordinate to rfl mapping.\n",
    "            | None defaults to D65 (srgb has a D65 white point)\n",
    "        :D: \n",
    "            | None, optional\n",
    "            | Degree of (von Kries) adaptation from spd to refspd. \n",
    "        :cieobs:\n",
    "            | _CIEOBS, optional\n",
    "            | CMF set for calculation of xyz from spectral data.\n",
    "        :cspace:\n",
    "            | 'xyz',  optional\n",
    "            | Color space for color coordinate to rfl mapping.\n",
    "            | Tip: Use linear space (e.g. 'xyz', 'Yuv',...) for (interp_type == 'nd'),\n",
    "            |      and perceptually uniform space (e.g. 'ipt') for (interp_type == 'nearest')\n",
    "        :cspace_tf:\n",
    "            | {}, optional\n",
    "            | Dict with parameters for xyz_to_cspace and cspace_to_xyz transform.\n",
    "        :CSF:\n",
    "            | None, optional\n",
    "            | RGB camera response functions.\n",
    "            | If None: input :xyz: contains raw rgb values. Override :cspace:\n",
    "            | argument and perform estimation directly in raw rgb space!!!\n",
    "        :interp_type:\n",
    "            | 'nd', optional\n",
    "            | Options:\n",
    "            | - 'nd': perform n-dimensional linear interpolation using Delaunay triangulation.\n",
    "            | - 'nearest': perform nearest neighbour interpolation. \n",
    "        :k_neighbours:\n",
    "            | 4 or int, optional\n",
    "            | Number of nearest neighbours for reflectance spectrum interpolation.\n",
    "            | Neighbours are found using scipy.spatial.cKDTree\n",
    "        :show: \n",
    "            | True, optional\n",
    "            |  Show images.\n",
    "        :verbosity:\n",
    "            | 0, optional\n",
    "            | If > 0: make a plot of the color coordinates of original and \n",
    "              rendered image pixels.\n",
    "        :show_ref_img:\n",
    "            | True, optional\n",
    "            | True: shows rendered image under reference spd. False: shows\n",
    "            |  original image.\n",
    "        :write_to_file:\n",
    "            | None, optional\n",
    "            | None: do nothing, else: write to filename(+path) in :write_to_file:\n",
    "        :stack_test_ref: \n",
    "            | 12, optional\n",
    "            |   - 12: left (test), right (ref) format for show and imwrite\n",
    "            |   - 21: top (test), bottom (ref)\n",
    "            |   - 1: only show/write test\n",
    "            |   - 2: only show/write ref\n",
    "            |   - 0: show both, write test\n",
    "        :csf_based_rgb_rounding:\n",
    "            | _ROUNDING, optional\n",
    "            | Int representing the number of decimals to round the RGB values (obtained from not-None CSF input) to before applying the search algorithm.\n",
    "            | Smaller values increase the search speed, but could cause fatal error that causes python kernel to die. If this happens increase the rounding int value.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        :returns: \n",
    "            | img_hyp, img_ren, \n",
    "            | ndarrays with float hyperspectral image and rendered images \n",
    "    \"\"\"\n",
    "    \n",
    "    # Get image:\n",
    "    #imread = lambda x: plt.imread(x) #matplotlib.pyplot\n",
    "   \n",
    "    if img is not None:\n",
    "        if isinstance(img,str):\n",
    "            img = plt.imread(img).copy() # use matplotlib.pyplot's imread\n",
    "    else:\n",
    "        img = plt.imread(_HYPSPCIM_DEFAULT_IMAGE).copy()\n",
    "    \n",
    "    if img.dtype == np.uint8: \n",
    "        img = img/255\n",
    "    elif img.dtype == np.uint16:\n",
    "        img = img/(2**16-1)\n",
    "    elif (img.dtype == np.float64) | (img.dtype == np.float32):\n",
    "        pass\n",
    "    else:\n",
    "        raise Exception('img input must be None, string or ndarray of (max = 1) float32 or float64 !')\n",
    "    if img.max() > 1.0: raise Exception('img input must be None, string or ndarray of (max = 1) float32 or float64 !')\n",
    "    \n",
    "    \n",
    "    # Convert to 2D format:\n",
    "    rgb = img.reshape(img.shape[0]*img.shape[1],3) # *1.0: make float\n",
    "    rgb[rgb==0] = _EPS # avoid division by zero for pure blacks.\n",
    "\n",
    "    \n",
    "    # Get unique rgb values and positions:\n",
    "    rgb_u, rgb_indices = np.unique(rgb, return_inverse=True, axis = 0)\n",
    "\n",
    "    \n",
    "    # get rfl set:\n",
    "    if rfl is None: # use IESTM30['4880'] set \n",
    "        rfl = _CRI_RFL['ies-tm30']['4880']['5nm']\n",
    "    wlr = rfl[0] # spectral reflectance set determines wavelength range for estimation (xyz_to_rfl())\n",
    "        \n",
    "    # get Ref spd:\n",
    "    if refspd is None:\n",
    "        refspd = _CIE_ILLUMINANTS['D65'].copy()\n",
    "    refspd = cie_interp(refspd, wlr, kind = 'linear') # force spd to same wavelength range as rfl\n",
    "\n",
    "\n",
    "    # Convert rgb_u to xyz and lab-type values under assumed refspd:\n",
    "    if CSF is None:\n",
    "        xyz_wr = spd_to_xyz(refspd, cieobs = cieobs, relative = True)\n",
    "        xyz_ur = colortf(rgb_u*255, tf = 'srgb>xyz')\n",
    "    else:\n",
    "        xyz_ur = rgb_u # for input in xyz_to_rfl (when CSF is not None: this functions assumes input is indeed rgb !!!)\n",
    "    \n",
    "    # Estimate rfl's for xyz_ur:\n",
    "    rfl_est, xyzri = xyz_to_rfl(xyz_ur, rfl = rfl, out = 'rfl_est,xyz_est', \\\n",
    "                 refspd = refspd, D = D, cieobs = cieobs, \\\n",
    "                 cspace = cspace, cspace_tf = cspace_tf, CSF = CSF,\\\n",
    "                 interp_type = interp_type, k_neighbours = k_neighbours, \n",
    "                 verbosity = verbosity,\n",
    "                 csf_based_rgb_rounding = csf_based_rgb_rounding)\n",
    "\n",
    "    # Get default test spd if none supplied:\n",
    "    if spd is None:\n",
    "        spd = _CIE_ILLUMINANTS['F4']\n",
    "        \n",
    "    if CSF is None:\n",
    "        # calculate xyz values under test spd:\n",
    "        xyzti, xyztw = spd_to_xyz(spd, rfl = rfl_est, cieobs = cieobs, out = 2)\n",
    "    \n",
    "        # Chromatic adaptation from test spd to refspd:\n",
    "        if D is not None:\n",
    "            xyzti = cat.apply(xyzti, xyzw1 = xyztw, xyzw2 = xyz_wr, D = D)\n",
    "    \n",
    "        # Convert xyzti under test spd to srgb:\n",
    "        rgbti = colortf(xyzti, tf = 'srgb')/255\n",
    "    else:\n",
    "        # Calculate rgb coordinates from camera sensitivity functions under spd:\n",
    "        rgbti = rfl_to_rgb(rfl_est, spd = spd, CSF = CSF, wl = None) \n",
    "        \n",
    "         # Chromatic adaptation from test spd to refspd:\n",
    "        if D is not None:\n",
    "            white = np.ones_like(spd)\n",
    "            white[0] = spd[0]\n",
    "            rgbwr = rfl_to_rgb(white, spd = refspd, CSF = CSF, wl = None)\n",
    "            rgbwt = rfl_to_rgb(white, spd = spd, CSF = CSF, wl = None)\n",
    "            rgbti = cat.apply_vonkries2(rgbti,rgbwt,rgbwr,xyzw0=np.array([[1.0,1.0,1.0]]), in_type='rgb',out_type= 'rgb',D=1)\n",
    "        \n",
    "    \n",
    "    # Reconstruct original locations for rendered image rgbs:\n",
    "    img_ren = rgbti[rgb_indices]\n",
    "    img_ren.shape = img.shape # reshape back to 3D size of original\n",
    "    img_ren = img_ren\n",
    "    \n",
    "    # For output:\n",
    "    if show_ref_img == True:\n",
    "        rgb_ref = colortf(xyzri, tf = 'srgb')/255 if (CSF is None) else xyzri # if CSF not None: xyzri contains rgbri !!!\n",
    "        img_ref = rgb_ref[rgb_indices]\n",
    "        img_ref.shape = img.shape # reshape back to 3D size of original\n",
    "        img_str = 'Rendered (under ref. spd)'\n",
    "        img = img_ref\n",
    "    else:\n",
    "        img_str = 'Original'\n",
    "        img = img\n",
    "       \n",
    "    \n",
    "    if (stack_test_ref > 0) | show == True:\n",
    "        if stack_test_ref == 21:\n",
    "            img_original_rendered = np.vstack((img_ren,np.ones((4,img.shape[1],3)),img))\n",
    "            img_original_rendered_str = 'Rendered (under test spd)\\n ' + img_str \n",
    "        elif stack_test_ref == 12:\n",
    "            img_original_rendered = np.hstack((img_ren,np.ones((img.shape[0],4,3)),img))\n",
    "            img_original_rendered_str = 'Rendered (under test spd) | ' + img_str \n",
    "        elif stack_test_ref == 1:\n",
    "            img_original_rendered = img_ren\n",
    "            img_original_rendered_str = 'Rendered (under test spd)' \n",
    "        elif stack_test_ref == 2:\n",
    "            img_original_rendered = img\n",
    "            img_original_rendered_str = img_str\n",
    "        elif stack_test_ref == 0:\n",
    "            img_original_rendered = img_ren\n",
    "            img_original_rendered_str =  'Rendered (under test spd)' \n",
    "            \n",
    "    if write_to_file is not None:\n",
    "        # Convert from RGB to BGR formatand write:\n",
    "        #print('Writing rendering results to image file: {}'.format(write_to_file))\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            imsave(write_to_file, img_original_rendered)\n",
    "            \n",
    "    if show == True:\n",
    "        # show images using pyplot.show():\n",
    "        plt.figure()\n",
    "        \n",
    "        plt.imshow(img_original_rendered)\n",
    "        plt.title(img_original_rendered_str)\n",
    "        plt.gca().get_xaxis().set_ticklabels([])\n",
    "        plt.gca().get_yaxis().set_ticklabels([])\n",
    "        \n",
    "        if stack_test_ref == 0:\n",
    "            plt.figure()\n",
    "            plt.imshow(img)\n",
    "            plt.title(img_str)\n",
    "            plt.axis('off')\n",
    "      \n",
    "    if 'img_hyp' in out.split(','):\n",
    "        # Create hyper_spectral image:\n",
    "        rfl_image_2D = rfl_est[rgb_indices+1,:] # create array with all rfls required for each pixel\n",
    "        img_hyp = rfl_image_2D.reshape(img.shape[0],img.shape[1],rfl_image_2D.shape[1])\n",
    "\n",
    "\n",
    "    # Setup output:\n",
    "    if out == 'img_hyp':\n",
    "        return img_hyp\n",
    "    elif out == 'img_ren':\n",
    "        return img_ren\n",
    "    else:\n",
    "        return eval(out)\n",
    "\n",
    "\n",
    "def rfl_to_rgb(rfl, spd = None, CSF = None, wl = None, normalize_to_white = True):\n",
    "    \"\"\" \n",
    "    Convert spectral reflectance functions (illuminated by spd) to Camera Sensitivity Functions.\n",
    "    \n",
    "    Args:\n",
    "        :rfl:\n",
    "            | ndarray with spectral reflectance functions (1st row is wavelengths if wl is None).\n",
    "        :spd:\n",
    "            | None, optional\n",
    "            | ndarray with illumination spectrum\n",
    "        :CSF:\n",
    "            | None, optional\n",
    "            | ndarray with camera sensitivity functions \n",
    "            | If None: use Nikon D700\n",
    "        :normalize_to_white:\n",
    "            | True, optional\n",
    "            | If True: white-balance output rgb to a perfect white diffuser.\n",
    "    \n",
    "    Returns:\n",
    "        :rgb:\n",
    "            | ndarray with rgb values for each spectral reflectance functions\n",
    "    \"\"\"\n",
    "    rfl_cp = rfl.copy()\n",
    "    if (wl is None): \n",
    "        wl = rfl_cp[0] \n",
    "        rfl_cp = rfl_cp[1:]\n",
    "    wlr = getwlr(wl)\n",
    "    if spd is not None:\n",
    "        spd = cie_interp(spd,wlr,kind='linear')[1:]\n",
    "    else:\n",
    "        spd = np.ones_like(wlr)\n",
    "    if CSF is None: CSF = _CSF_NIKON_D700\n",
    "    CSF = cie_interp(CSF,wlr,kind='linear')\n",
    "    CSF[1:] = CSF[1:]*spd\n",
    "    rgb = rfl_cp @ CSF[1:].T \n",
    "    if normalize_to_white:\n",
    "        white = np.ones_like(spd)\n",
    "        white = white/white.sum()*spd.sum()\n",
    "        rgbw = white @ CSF[1:].T  \n",
    "        rgb = rgb/rgbw.max(axis = 0,keepdims=True) \n",
    "    \n",
    "    return rgb\n",
    "\n",
    "    \n",
    "    \n",
    "def hsi_to_rgb(hsi, spd = None, cieobs = _CIEOBS, srgb = False, \n",
    "               linear_rgb = False, CSF = None, normalize_to_white = True, \n",
    "               wl = [380,780,1]):\n",
    "    \"\"\" \n",
    "    Convert HyperSpectral Image to rgb.\n",
    "    \n",
    "    Args:\n",
    "        :hsi:\n",
    "            | ndarray with hyperspectral image [M,N,L]\n",
    "        :spd:\n",
    "            | None, optional\n",
    "            | ndarray with illumination spectrum\n",
    "        :cieobs:\n",
    "            | _CIEOBS, optional\n",
    "            | CMF set to convert spectral data to xyz tristimulus values.\n",
    "        :srgb:\n",
    "            | False, optional\n",
    "            | If False: Use xyz_to_srgb(spd_to_xyz(...)) to convert to srgb values\n",
    "            | If True: use camera sensitivity functions.\n",
    "        :linear_rgb:\n",
    "            | False, optional\n",
    "            | If False: use gamma = 2.4 in xyz_to_srgb, if False: use gamma = 1 and set :use_linear_part: to False.\n",
    "        :CSF:\n",
    "            | None, optional\n",
    "            | ndarray with camera sensitivity functions \n",
    "            | If None: use Nikon D700\n",
    "        :normalize_to_white:\n",
    "            | True, optional\n",
    "            | If True & CSF is not None: white-balance output rgb to a perfect white diffuser.\n",
    "        :wl:\n",
    "            | [380,780,1], optional\n",
    "            | Wavelength range and spacing or ndarray with wavelengths of HSI image.\n",
    "    \n",
    "    Returns:\n",
    "        :rgb:\n",
    "            | ndarray with rgb image [M,N,3]\n",
    "    \"\"\"\n",
    "    if spd is None:\n",
    "        spd = _CIE_E.copy()\n",
    "    wlr = getwlr(wl)\n",
    "    spd = cie_interp(spd,wl,kind='linear')\n",
    "    \n",
    "    hsi_2d = np.reshape(hsi,(hsi.shape[0]*hsi.shape[1],hsi.shape[2]))\n",
    "    if srgb:\n",
    "        xyz = spd_to_xyz(spd, cieobs = cieobs, relative = True, rfl = np.vstack((wlr,hsi_2d)))\n",
    "        gamma = 1 if linear_rgb else 2.4\n",
    "        rgb = xyz_to_srgb(xyz, gamma = gamma, use_linear_part = not linear_rgb)/255\n",
    "    else:\n",
    "        if CSF is None: CSF = _CSF_NIKON_D700\n",
    "        rgb = rfl_to_rgb(hsi_2d, spd = spd, CSF = CSF, wl = wl, normalize_to_white = normalize_to_white)        \n",
    "    return np.reshape(rgb,(hsi.shape[0],hsi.shape[1],3))\n",
    "\n",
    "       \n",
    "def get_superresolution_hsi(lrhsi, hrci, CSF, wl = [380,780,1], csf_based_rgb_rounding = _ROUNDING,\n",
    "                            interp_type = 'nd', k_neighbours = 4, verbosity = 0):\n",
    "    \"\"\" \n",
    "    Get a HighResolution HyperSpectral Image (super-resolution HSI) based on a LowResolution HSI and a HighResolution Color Image.\n",
    "    \n",
    "    Args:\n",
    "        :lrhsi:\n",
    "            | ndarray with float (max = 1) LowResolution HSI [m,m,L].\n",
    "        :hrci:\n",
    "            | ndarray with float (max = 1) HighResolution HSI [M,N,3].\n",
    "        :CSF:\n",
    "            | None, optional\n",
    "            | ndarray with camera sensitivity functions \n",
    "            | If None: use Nikon D700\n",
    "        :wl:\n",
    "            | [380,780,1], optional\n",
    "            | Wavelength range and spacing or ndarray with wavelengths of HSI image.\n",
    "        :interp_type:\n",
    "            | 'nd', optional\n",
    "            | Options:\n",
    "            | - 'nd': perform n-dimensional linear interpolation using Delaunay triangulation.\n",
    "            | - 'nearest': perform nearest neighbour interpolation. \n",
    "        :k_neighbours:\n",
    "            | 4 or int, optional\n",
    "            | Number of nearest neighbours for reflectance spectrum interpolation.\n",
    "            | Neighbours are found using scipy.spatial.cKDTree\n",
    "        :verbosity:\n",
    "            | 0, optional\n",
    "            | Verbosity level for sub-call to render_image().\n",
    "            | If > 0: make a plot of the color coordinates of original and \n",
    "            | rendered image pixels.\n",
    "        :csf_based_rgb_rounding:\n",
    "            | _ROUNDING, optional\n",
    "            | Int representing the number of decimals to round the RGB values (obtained from not-None CSF input) to before applying the search algorithm.\n",
    "            | Smaller values increase the search speed, but could cause fatal error that causes python kernel to die. If this happens increase the rounding int value.\n",
    "\n",
    "    Returns:\n",
    "        :hrhsi:\n",
    "            | ndarray with HighResolution HSI [M,N,L].\n",
    "        \n",
    "    Procedure:\n",
    "        | Call render_image(hrci, rfl = lrhsi_2, CSF = ...) to estimate a hyperspectral image\n",
    "        | from the high-resolution color image hrci with the reflectance spectra \n",
    "        | in the low-resolution hyper-spectral image as database for the estimation.\n",
    "        | Estimation is done in raw RGB space with the lrhsi converted using the\n",
    "        | camera sensitivity functions in CSF.\n",
    "    \"\"\"\n",
    "    wlr = getwlr(wl)\n",
    "    eew = np.vstack((wlr,np.ones_like(wlr)))\n",
    "    lrhsi_2d = np.vstack((wlr,np.reshape(lrhsi,(lrhsi.shape[0]*lrhsi.shape[1],lrhsi.shape[2])))) # create 2D rfl database\n",
    "    if CSF is None: CSF = _CSF_NIKON_D700\n",
    "    hrhsi = render_image(hrci, spd = eew,\n",
    "                         refspd = eew, rfl = lrhsi_2d, D = None,\n",
    "                         interp_type = interp_type, k_neighbours = k_neighbours,\n",
    "                         verbosity = verbosity, show = bool(verbosity),\n",
    "                         CSF = CSF, csf_based_rgb_rounding = csf_based_rgb_rounding) # render HR-hsi from HR-ci using LR-HSI rfls as database        \n",
    "    return hrhsi\n",
    "\n",
    "\n",
    "def illuminanceEstimation(input_img):\n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    "    # Example / test code for HSI simulation and rendering:\n",
    "    #--------------------------------------------------------------------------\n",
    "    # plt.close('all')\n",
    "    # from luxpy.toolboxes import spdbuild as spb\n",
    "    # S = spb.spd_builder(peakwl = [460,525,590],fwhm=[20,40,20],target=4000, tar_type = 'cct') \n",
    "    # img = _HYPSPCIM_DEFAULT_IMAGE\n",
    "    # img_hyp,img_ren = render_image(img = img, \n",
    "    #                                 cspace = 'Yuv',interp_type='nd',\n",
    "    #                                 spd = S, D=1, \n",
    "    #                                 show_ref_img = True,\n",
    "    #                                 stack_test_ref = 21,\n",
    "    #                                 out='img_hyp,img_ren',\n",
    "    #                                 write_to_file = 'test.jpg') \n",
    "    # raise Exception('')\n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    "    # Example / test code for super resolution:\n",
    "    #--------------------------------------------------------------------------\n",
    "    import time\n",
    "    import luxpy as lx\n",
    "    import matplotlib.pyplot as plt\n",
    "    from skimage import transform\n",
    "    import imageio\n",
    "    from skimage.transform import rescale,resize\n",
    "    \n",
    "    np.random.seed(1)    \n",
    "    \n",
    "    # Set some default parameters:\n",
    "    #----------------------------\n",
    "    load_hsi = False # If True: load hrci and hrhsi from npy-file.\n",
    "    file = input_img\n",
    "\n",
    "    cieobs = '1931_2' # CIE CMF set\n",
    "    linear_rgb = 1 # only used when srgb in hsi_to_rgb == True !!!\n",
    "    verbosity = 0\n",
    "    \n",
    "    # Create HR-rgb image and HR-HSI for code testing: \n",
    "    #---------------------------------------------------\n",
    "    # get an image:\n",
    "    im = imageio.v2.imread(file)/255\n",
    "    \n",
    "    # rescale to n x dimensions of typical hyperspectral camera:\n",
    "    n = 2 # downscale factor\n",
    "    w, h = 1280, 960\n",
    "    cr,cc = np.array(im.shape[:2])//2\n",
    "    crop = lambda im,cr,cc,h,w:im[(cr-h//2):(cr+h//2),(cc-w//2):(cc+w//2),:].copy()\n",
    "    im = crop(im,cr,cc,h*n,w*n)\n",
    "#     print('New image shape:',im.shape)\n",
    "    \n",
    "    # simulate HR hyperspectral image:\n",
    "    hrhsi = render_image(im,show=False)\n",
    "    wlr = getwlr([380,780,1]) #  = wavelength range of default TM30 rfl set\n",
    "    wlr = wlr[20:-80:10] # wavelength range from 400nm-700nm every 10 nm\n",
    "    hrhsi = hrhsi[...,20:-80:10] # wavelength range from 400nm-700nm every 10 nm\n",
    "#     print('Simulated HR-HSI shape:',hrhsi.shape)\n",
    "    # np.save(file[:-4]+'.npy',{'hrhsi':hrhsi,'im':im, 'wlr':wlr})\n",
    "    \n",
    "    # Illumination spectrum of HSI:    \n",
    "    eew = np.vstack((wlr,np.ones_like(wlr)))\n",
    "        \n",
    "    # Create fig and axes for plots:\n",
    "    if verbosity > 0: fig, axs = plt.subplots(1,3)\n",
    "    \n",
    "    # convert HR hsi to HR rgb image:\n",
    "    hrci = hsi_to_rgb(hrhsi, spd = eew, cieobs = cieobs, wl = wlr, linear_rgb = linear_rgb)\n",
    "    if verbosity > 0:  axs[0].imshow(hrci)\n",
    "    \n",
    "    # create LR hsi image for testing:\n",
    "    dl = n \n",
    "    lrhsi = hrhsi[::dl,::dl,:]\n",
    "#     print('Simulated LR-HSI shape:',lrhsi.shape)\n",
    "    \n",
    "    # convert LR hsi to LR rgb image:\n",
    "    lrci = hsi_to_rgb(lrhsi, spd = eew, cieobs = cieobs, wl = wlr,linear_rgb = linear_rgb)\n",
    "    if verbosity > 0:  axs[1].imshow(lrci)\n",
    "    \n",
    "    # # Perform rgb guided super-resolution:\n",
    "    #hrci = lrci # for testing of estimation code\n",
    "    tic = time.time()\n",
    "    hrhsi_est = get_superresolution_hsi(lrhsi, hrci, CSF = _CSF_NIKON_D700, wl = wlr)\n",
    "#     print('Elapsed time (s): {:1.4f}'.format(time.time() - tic))\n",
    "    hrci_est = hsi_to_rgb(hrhsi_est, spd = eew, cieobs = cieobs, wl = wlr, linear_rgb = linear_rgb)\n",
    "\n",
    "    if verbosity > 0:  axs[2].imshow(hrci_est)\n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    "    # Plot some rfl to visually evaluate estimation accuracy:\n",
    "    \n",
    "    hsi_rmse = np.linalg.norm(hrhsi-hrhsi_est)/np.array(hrhsi.shape[:2]).prod()**0.5\n",
    "#     print('RMSE(ground-truth,estimate): {:1.4f}'.format(hsi_rmse))\n",
    "    \n",
    "    global illum_out\n",
    "    return illum_out\n",
    "    \n",
    "#     fig, axs = plt.subplots(1,4, figsize=(22,5))\n",
    "    \n",
    "#     axs[0].imshow(transform.rescale(lrci,dl,order=0,multichannel=True),aspect='auto')\n",
    "#     axs[0].set_title('Color image of LR-HSI\\n(HR-to-LR scale factor = {:1.2f})'.format(1/dl))\n",
    "#     axs[0].axis('off')\n",
    "#     axs[1].imshow(hrci_est,aspect='auto')\n",
    "#     axs[1].set_title('Color image of estimated HR-HSI')\n",
    "#     axs[1].axis('off')\n",
    "    \n",
    "#     px_rmse = ((hrhsi_est-hrhsi)**2).sum(axis=-1)**0.5 # rmse per pixel\n",
    "#     axs[2].set_title('RMSE(ground-truth, estimated) HR-HSI\\nRMSE = {:1.4f}, max = {:1.4f}'.format((px_rmse**2).mean()**0.5,px_rmse.max()))\n",
    "#     im = axs[2].imshow(px_rmse, cmap = 'jet',aspect='auto') # rmse per pixel\n",
    "#     cbar = axs[2].figure.colorbar(im, ax=axs[2])\n",
    "#     cbar.ax.set_ylabel('RMSE', rotation=-90, va=\"bottom\")\n",
    "    \n",
    "    \n",
    "#     psorted = np.unravel_index(np.argsort(px_rmse, axis=None), px_rmse.shape) # index of pixels sorted by px_rmse\n",
    "#     np.random.seed(1)\n",
    "#     pxs = np.random.permutation(min(hrhsi.shape[:2]))[:12].reshape(2,3,2)\n",
    "#     iis = np.hstack((pxs[...,0].ravel(),psorted[0][-3:]))\n",
    "#     jjs = np.hstack((pxs[...,1].ravel(),psorted[1][-3:]))\n",
    "#     colors = np.array(['m','b','c','g','y','r','k','lightgrey','grey'])\n",
    "#     for t in range(len(iis)):\n",
    "#         ii,jj = iis[t],jjs[t]\n",
    "#         axs[1].plot(jj,ii,color = 'none', marker = 'o', mec = colors[t])\n",
    "#         axs[2].plot(jj,ii,color = 'none', marker = 'o', mec = colors[t])\n",
    "#         axs[3].plot(wlr,hrhsi[ii,jj,:],color = colors[t], linestyle ='-',label='ground-truth (r{:1.0f},c{:1.0f})'.format(ii,jj))\n",
    "#         axs[3].plot(wlr,hrhsi_est[ii,jj,:],color = colors[t], linestyle = '--',label='estimate (r{:1.0f},c{:1.0f})'.format(ii,jj))\n",
    "#     axs[3].legend(bbox_to_anchor=(1.05, 1))   \n",
    "#     axs[3].set_xlabel('Wavelengths (nm)')\n",
    "#     axs[3].set_ylabel('Spectral Reflectance')\n",
    "#     plt.subplots_adjust(right=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb1ae358",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T08:04:25.883538Z",
     "start_time": "2023-10-24T08:04:25.878891Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def illumEstOneFolder_prep():\n",
    "    images = Path(r\"data\\Training\\001_nonsleepyCombination\") #imgs\n",
    "#     landmarks = []\n",
    "#     fileNames = []\n",
    "#     illums = []\n",
    "    \n",
    "#     for img_file in images.glob(\"*.jpg\"):\n",
    "\n",
    "#         print(os.path.basename(img_file)[:-4])\n",
    "#         fileNames.append(os.path.basename(img_file)[:-4])\n",
    "#         landmarks.append(generateLandmarks(os.path.dirname(img_file) + \"\\\\\" + os.path.basename(img_file)))\n",
    "#         illums.append(illuminanceEstimation(img_file))\n",
    "#         retinexImplement(img_file)\n",
    "    \n",
    "#     ilu_dis = pd.DataFrame({'File Names': fileNames, 'Landmark Confidence': landmarks, 'Illuminance': illums})\n",
    "    \n",
    "#     print(ilu_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b6f26ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T08:04:25.890600Z",
     "start_time": "2023-10-24T08:04:25.885391Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def generateLandmarks(img):\n",
    "    img = cv2.imread(img)\n",
    "    detector = MTCNN()\n",
    "    output = detector.detect_faces(img)\n",
    "#     print(output[0]['confidence'])\n",
    "    \n",
    "    return output[0]['keypoints']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6a15479",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T08:04:25.897440Z",
     "start_time": "2023-10-24T08:04:25.893062Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# notes:\n",
    "\n",
    "# average 2.67s per image for generating landmarks and illuminance estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f9a8d5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T08:04:29.726740Z",
     "start_time": "2023-10-24T08:04:25.899637Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import URetinexNet\n",
    "from URetinex_Net import *\n",
    "\n",
    "# Import Arch_network\n",
    "import sys\n",
    "sys.path.append(r\"URetinex_Net\")\n",
    "\n",
    "# testing of URetinex.Net\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from network.Math_Module import P, Q\n",
    "from network.decom import Decom\n",
    "import os\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import time\n",
    "from utils import *\n",
    "\n",
    "def one2three(x):\n",
    "    return torch.cat([x, x, x], dim=1).to(x)\n",
    "\n",
    "class Inference(nn.Module):\n",
    "    #Class Inference Methods\n",
    "    def __init__(self, opts):\n",
    "        super().__init__()\n",
    "        self.opts = opts\n",
    "        # loading decomposition model \n",
    "        self.model_Decom_low = Decom()\n",
    "        self.model_Decom_low = load_initialize(self.model_Decom_low, self.opts.Decom_model_low_path)\n",
    "        # loading R; old_model_opts; and L model\n",
    "        self.unfolding_opts, self.model_R, self.model_L= load_unfolding(self.opts.unfolding_model_path)\n",
    "        # loading adjustment model\n",
    "        self.adjust_model = load_adjustment(self.opts.adjust_model_path)\n",
    "        self.P = P()\n",
    "        self.Q = Q()\n",
    "\n",
    "        transform = [\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "        self.transform = transforms.Compose(transform)\n",
    "        print(self.model_Decom_low)\n",
    "        print(self.model_R)\n",
    "        print(self.model_L)\n",
    "        print(self.adjust_model)\n",
    "        #time.sleep(8)\n",
    "\n",
    "    def unfolding(self, input_low_img):\n",
    "        for t in range(self.unfolding_opts.round):      \n",
    "            if t == 0: # initialize R0, L0\n",
    "                P, Q = self.model_Decom_low(input_low_img)\n",
    "            else: # update P and Q\n",
    "                w_p = (self.unfolding_opts.gamma + self.unfolding_opts.Roffset * t)\n",
    "                w_q = (self.unfolding_opts.lamda + self.unfolding_opts.Loffset * t)\n",
    "                P = self.P(I=input_low_img, Q=Q, R=R, gamma=w_p)\n",
    "                Q = self.Q(I=input_low_img, P=P, L=L, lamda=w_q) \n",
    "            R = self.model_R(r=P, l=Q)\n",
    "            L = self.model_L(l=Q)\n",
    "        return R, L\n",
    "    \n",
    "    def lllumination_adjust(self, L, ratio):\n",
    "        ratio = torch.ones(L.shape) * self.opts.ratio\n",
    "        return self.adjust_model(l=L, alpha=ratio)\n",
    "    \n",
    "    def forward(self, input_low_img):\n",
    "        if torch.cuda.is_available():\n",
    "            input_low_img = input_low_img\n",
    "        with torch.no_grad():\n",
    "            start = time.time()  \n",
    "            R, L = self.unfolding(input_low_img)\n",
    "            High_L = self.lllumination_adjust(L, self.opts.ratio)\n",
    "            I_enhance = High_L * R\n",
    "            p_time = (time.time() - start)\n",
    "        return I_enhance, p_time\n",
    "\n",
    "    def run(self, low_img_path):\n",
    "        file_name = os.path.basename(self.opts.img_path)\n",
    "        name = file_name.split('.')[0]\n",
    "        low_img = self.transform(Image.open(low_img_path)).unsqueeze(0)\n",
    "        enhance, p_time = self.forward(input_low_img=low_img)\n",
    "        if not os.path.exists(self.opts.output):\n",
    "            os.makedirs(self.opts.output)\n",
    "        save_path = os.path.join(self.opts.output, file_name.replace(name, \"%s_URetinexNet\"%(name)))\n",
    "        np_save_TensorImg(enhance, save_path)  \n",
    "        print(\"================================= time for %s: %f============================\"%(file_name, p_time))\n",
    "\n",
    "#         add to own function for input of img path\n",
    "def retinexImplement(img, outPath):\n",
    "    parser = argparse.ArgumentParser(description='Configure')\n",
    "    \n",
    "    # specify your data path here!\n",
    "    parser.add_argument('--img_path', type=str, default=img)\n",
    "    parser.add_argument('--output', type=str, default=outPath)\n",
    "    # ratio are recommended to be 3-5, bigger ratio will lead to over-exposure \n",
    "    parser.add_argument('--ratio', type=int, default=5)\n",
    "    # model path\n",
    "    parser.add_argument('--Decom_model_low_path', type=str, default=\"./URetinex_Net/ckpt/init_low.pth\")\n",
    "    parser.add_argument('--unfolding_model_path', type=str, default=\"./URetinex_Net/ckpt/unfolding.pth\")\n",
    "    parser.add_argument('--adjust_model_path', type=str, default=\"./URetinex_Net/ckpt/L_adjust.pth\")\n",
    "    parser.add_argument('--gpu_id', type=int, default=0)\n",
    "    \n",
    "#     opts = parser.parse_args() change parse_args() to parse_known_args\n",
    "    opts, _ = parser.parse_known_args()\n",
    "    for k, v in vars(opts).items():\n",
    "        print(k, v)\n",
    "    \n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(opts.gpu_id)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = Inference(opts)\n",
    "        print(\"CUDA (GPU) is available\")\n",
    "    else:\n",
    "        print(\"CUDA (GPU) is not available. Loading the model on CPU...\")\n",
    "        model = Inference(opts).to(torch.device('cpu'))\n",
    "    \n",
    "#    \n",
    "    model.run(opts.img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "679d8739",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T08:04:29.732996Z",
     "start_time": "2023-10-24T08:04:29.728254Z"
    }
   },
   "outputs": [],
   "source": [
    "# evalData_prep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e4da41d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T08:09:16.315427Z",
     "start_time": "2023-10-24T08:07:00.736533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_0_0.jpg\n",
      "1/1 [==============================] - 0s 212ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michael\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Illuminance: 0.29567792373263885\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_0_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 5\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "================================= time for 001_nonsleepyCombination_0_0.jpg: 4.041516============================\n",
      "Illuminance: 0.516026363376736\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000231BADB94E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_15_0.jpg\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000231BB03CE00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "Illuminance: 0.2929762817534723\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_15_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 5\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================= time for 001_nonsleepyCombination_15_0.jpg: 3.087507============================\n",
      "Illuminance: 0.513045514422743\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_30_0.jpg\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "Illuminance: 0.29606498448350704\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_30_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 5\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "================================= time for 001_nonsleepyCombination_30_0.jpg: 3.043949============================\n",
      "Illuminance: 0.5178306256293403\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_45_0.jpg\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "Illuminance: 0.2991066410894097\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_45_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 5\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================= time for 001_nonsleepyCombination_45_0.jpg: 3.086156============================\n",
      "Illuminance: 0.5205153679947918\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_60_0.jpg\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "Illuminance: 0.29778815756076393\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_60_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 5\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "================================= time for 001_nonsleepyCombination_60_0.jpg: 3.084773============================\n",
      "Illuminance: 0.5188656798350694\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_75_0.jpg\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "Illuminance: 0.29540649650173617\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_75_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 5\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================= time for 001_nonsleepyCombination_75_0.jpg: 3.115362============================\n",
      "Illuminance: 0.5162518392838541\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_90_0.jpg\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "Illuminance: 0.2979124167057292\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_90_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 5\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "================================= time for 001_nonsleepyCombination_90_0.jpg: 2.989439============================\n",
      "Illuminance: 0.5190531985807292\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_105_0.jpg\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "Illuminance: 0.29994796240885413\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_105_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 5\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================= time for 001_nonsleepyCombination_105_0.jpg: 3.006984============================\n",
      "Illuminance: 0.5218593519053819\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_120_0.jpg\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "Illuminance: 0.29637356141927085\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_120_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 5\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "================================= time for 001_nonsleepyCombination_120_0.jpg: 3.226498============================\n",
      "Illuminance: 0.5163899465885416\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_135_0.jpg\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "Illuminance: 0.2969686096050347\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_135_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 5\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================= time for 001_nonsleepyCombination_135_0.jpg: 2.969605============================\n",
      "Illuminance: 0.5168914892708333\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_150_0.jpg\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "Illuminance: 0.2955887041015627\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_150_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 5\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "================================= time for 001_nonsleepyCombination_150_0.jpg: 2.949124============================\n",
      "Illuminance: 0.5162052219791669\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_165_0.jpg\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "Illuminance: 0.2953554853255209\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_165_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 5\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================= time for 001_nonsleepyCombination_165_0.jpg: 3.292972============================\n",
      "Illuminance: 0.5153277215190973\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_180_0.jpg\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "Illuminance: 0.29572583037326383\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_180_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 5\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "================================= time for 001_nonsleepyCombination_180_0.jpg: 2.922006============================\n",
      "Illuminance: 0.517773762782118\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_195_0.jpg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainingData_prep()\n",
      "Cell \u001b[1;32mIn[6], line 57\u001b[0m, in \u001b[0;36mtrainingData_prep\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(videos_file)\n\u001b[0;32m     56\u001b[0m training_video_paths\u001b[38;5;241m.\u001b[39mappend(video_path)\n\u001b[1;32m---> 57\u001b[0m frame_capture(video_path)\n",
      "Cell \u001b[1;32mIn[5], line 89\u001b[0m, in \u001b[0;36mframe_capture\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_path \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/Training/\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     88\u001b[0m     trainFileNames\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(img_file)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m])\n\u001b[1;32m---> 89\u001b[0m     trainLandmarks\u001b[38;5;241m.\u001b[39mappend(generateLandmarks(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(img_file) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(img_file)))\n\u001b[0;32m     90\u001b[0m     trainIllumsRaw\u001b[38;5;241m.\u001b[39mappend(illuminanceEstimation(img_file))\n\u001b[0;32m     92\u001b[0m     retSave_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/RetTraining/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m parentDir[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(file)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m, in \u001b[0;36mgenerateLandmarks\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerateLandmarks\u001b[39m(img):\n\u001b[0;32m      2\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(img)\n\u001b[1;32m----> 3\u001b[0m     detector \u001b[38;5;241m=\u001b[39m MTCNN()\n\u001b[0;32m      4\u001b[0m     output \u001b[38;5;241m=\u001b[39m detector\u001b[38;5;241m.\u001b[39mdetect_faces(img)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#     print(output[0]['confidence'])\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\mtcnn\\mtcnn.py:87\u001b[0m, in \u001b[0;36mMTCNN.__init__\u001b[1;34m(self, weights_file, min_face_size, steps_threshold, scale_factor)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_threshold \u001b[38;5;241m=\u001b[39m steps_threshold\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale_factor \u001b[38;5;241m=\u001b[39m scale_factor\n\u001b[1;32m---> 87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pnet, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rnet, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_onet \u001b[38;5;241m=\u001b[39m NetworkFactory()\u001b[38;5;241m.\u001b[39mbuild_P_R_O_nets_from_file(weights_file)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\mtcnn\\network\\factory.py:124\u001b[0m, in \u001b[0;36mNetworkFactory.build_P_R_O_nets_from_file\u001b[1;34m(self, weights_file)\u001b[0m\n\u001b[0;32m    121\u001b[0m weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(weights_file, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    123\u001b[0m p_net \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_pnet()\n\u001b[1;32m--> 124\u001b[0m r_net \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_rnet()\n\u001b[0;32m    125\u001b[0m o_net \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_onet()\n\u001b[0;32m    127\u001b[0m p_net\u001b[38;5;241m.\u001b[39mset_weights(weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpnet\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\mtcnn\\network\\factory.py:77\u001b[0m, in \u001b[0;36mNetworkFactory.build_rnet\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m     75\u001b[0m r_layer \u001b[38;5;241m=\u001b[39m Flatten()(r_layer)\n\u001b[0;32m     76\u001b[0m r_layer \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m128\u001b[39m)(r_layer)\n\u001b[1;32m---> 77\u001b[0m r_layer \u001b[38;5;241m=\u001b[39m PReLU()(r_layer)\n\u001b[0;32m     79\u001b[0m r_layer_out1 \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m2\u001b[39m)(r_layer)\n\u001b[0;32m     80\u001b[0m r_layer_out1 \u001b[38;5;241m=\u001b[39m Softmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)(r_layer_out1)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py:1063\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;66;03m# Functional Model construction mode is invoked when `Layer`s are called\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;66;03m# on symbolic `KerasTensor`s, i.e.:\u001b[39;00m\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;66;03m# >> inputs = tf.keras.Input(10)\u001b[39;00m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;66;03m# >> outputs = MyLayer()(inputs)  # Functional construction mode.\u001b[39;00m\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;66;03m# >> model = tf.keras.Model(inputs, outputs)\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _in_functional_construction_mode(\n\u001b[0;32m   1061\u001b[0m     \u001b[38;5;28mself\u001b[39m, inputs, args, kwargs, input_list\n\u001b[0;32m   1062\u001b[0m ):\n\u001b[1;32m-> 1063\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_functional_construction_call(\n\u001b[0;32m   1064\u001b[0m         inputs, args, kwargs, input_list\n\u001b[0;32m   1065\u001b[0m     )\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;66;03m# Maintains info about the `Layer.call` stack.\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m call_context \u001b[38;5;241m=\u001b[39m base_layer_utils\u001b[38;5;241m.\u001b[39mcall_context()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py:2603\u001b[0m, in \u001b[0;36mLayer._functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   2597\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m call_context\u001b[38;5;241m.\u001b[39menter(\n\u001b[0;32m   2598\u001b[0m     layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, inputs\u001b[38;5;241m=\u001b[39minputs, build_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39mtraining_value\n\u001b[0;32m   2599\u001b[0m ):\n\u001b[0;32m   2600\u001b[0m     \u001b[38;5;66;03m# Check input assumptions set after layer building, e.g. input\u001b[39;00m\n\u001b[0;32m   2601\u001b[0m     \u001b[38;5;66;03m# shape.\u001b[39;00m\n\u001b[0;32m   2602\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2603\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keras_tensor_symbolic_call(\n\u001b[0;32m   2604\u001b[0m             inputs, input_masks, args, kwargs\n\u001b[0;32m   2605\u001b[0m         )\n\u001b[0;32m   2606\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2607\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDictWrapper\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py:2449\u001b[0m, in \u001b[0;36mLayer._keras_tensor_symbolic_call\u001b[1;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[0;32m   2445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[0;32m   2446\u001b[0m         keras_tensor\u001b[38;5;241m.\u001b[39mKerasTensor, output_signature\n\u001b[0;32m   2447\u001b[0m     )\n\u001b[0;32m   2448\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_output_signature(\n\u001b[0;32m   2450\u001b[0m         inputs, args, kwargs, input_masks\n\u001b[0;32m   2451\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py:2508\u001b[0m, in \u001b[0;36mLayer._infer_output_signature\u001b[1;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[0;32m   2506\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_build(inputs)\n\u001b[0;32m   2507\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs)\n\u001b[1;32m-> 2508\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m call_fn(inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2510\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n\u001b[0;32m   2511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_mask_metadata(\n\u001b[0;32m   2512\u001b[0m     inputs, outputs, input_masks, build_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   2513\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:690\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m conversion_ctx:\n\u001b[1;32m--> 690\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    692\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:331\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conversion\u001b[38;5;241m.\u001b[39mis_in_allowlist_cache(f, options):\n\u001b[0;32m    330\u001b[0m   logging\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllowlisted \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: from cache\u001b[39m\u001b[38;5;124m'\u001b[39m, f)\n\u001b[1;32m--> 331\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx()\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m ag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mDISABLED:\n\u001b[0;32m    334\u001b[0m   logging\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllowlisted: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: AutoGraph is disabled in context\u001b[39m\u001b[38;5;124m'\u001b[39m, f)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    456\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m\u001b[38;5;241m.\u001b[39mcall(args, kwargs)\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 459\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\activation\\prelu.py:108\u001b[0m, in \u001b[0;36mPReLU.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m--> 108\u001b[0m     pos \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mrelu(inputs)\n\u001b[0;32m    109\u001b[0m     neg \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m backend\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;241m-\u001b[39minputs)\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pos \u001b[38;5;241m+\u001b[39m neg\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:5397\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(x, alpha, max_value, threshold)\u001b[0m\n\u001b[0;32m   5395\u001b[0m     clip_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   5396\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 5397\u001b[0m     x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m   5399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clip_max:\n\u001b[0;32m   5400\u001b[0m     max_value \u001b[38;5;241m=\u001b[39m _constant_to_tensor(max_value, x\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py:11809\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(features, name)\u001b[0m\n\u001b[0;32m  11807\u001b[0m \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[0;32m  11808\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m> 11809\u001b[0m   _, _, _op, _outputs \u001b[38;5;241m=\u001b[39m _op_def_library\u001b[38;5;241m.\u001b[39m_apply_op_helper(\n\u001b[0;32m  11810\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRelu\u001b[39m\u001b[38;5;124m\"\u001b[39m, features\u001b[38;5;241m=\u001b[39mfeatures, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m  11811\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m  11812\u001b[0m   _result \u001b[38;5;241m=\u001b[39m _dispatch\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[0;32m  11813\u001b[0m         relu, (), \u001b[38;5;28mdict\u001b[39m(features\u001b[38;5;241m=\u001b[39mfeatures, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m  11814\u001b[0m       )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:795\u001b[0m, in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    790\u001b[0m must_colocate_inputs \u001b[38;5;241m=\u001b[39m [val \u001b[38;5;28;01mfor\u001b[39;00m arg, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(op_def\u001b[38;5;241m.\u001b[39minput_arg, inputs)\n\u001b[0;32m    791\u001b[0m                         \u001b[38;5;28;01mif\u001b[39;00m arg\u001b[38;5;241m.\u001b[39mis_ref]\n\u001b[0;32m    792\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _MaybeColocateWith(must_colocate_inputs):\n\u001b[0;32m    793\u001b[0m   \u001b[38;5;66;03m# Add Op to graph\u001b[39;00m\n\u001b[0;32m    794\u001b[0m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 795\u001b[0m   op \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39m_create_op_internal(op_type_name, inputs, dtypes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    796\u001b[0m                              name\u001b[38;5;241m=\u001b[39mscope, input_types\u001b[38;5;241m=\u001b[39minput_types,\n\u001b[0;32m    797\u001b[0m                              attrs\u001b[38;5;241m=\u001b[39mattr_protos, op_def\u001b[38;5;241m=\u001b[39mop_def)\n\u001b[0;32m    799\u001b[0m \u001b[38;5;66;03m# `outputs` is returned as a separate return value so that the output\u001b[39;00m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;66;03m# tensors can the `op` per se can be decoupled so that the\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# `op_callbacks` can function properly. See framework/op_callbacks.py\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# for more details.\u001b[39;00m\n\u001b[0;32m    803\u001b[0m outputs \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39moutputs\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:670\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    668\u001b[0m   inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcapture(inp)\n\u001b[0;32m    669\u001b[0m   captured_inputs\u001b[38;5;241m.\u001b[39mappend(inp)\n\u001b[1;32m--> 670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_create_op_internal(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    671\u001b[0m     op_type, captured_inputs, dtypes, input_types, name, attrs, op_def,\n\u001b[0;32m    672\u001b[0m     compute_device)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3381\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3378\u001b[0m \u001b[38;5;66;03m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[0;32m   3379\u001b[0m \u001b[38;5;66;03m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[0;32m   3380\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mutation_lock():\n\u001b[1;32m-> 3381\u001b[0m   ret \u001b[38;5;241m=\u001b[39m Operation\u001b[38;5;241m.\u001b[39mfrom_node_def(\n\u001b[0;32m   3382\u001b[0m       node_def,\n\u001b[0;32m   3383\u001b[0m       \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3384\u001b[0m       inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m   3385\u001b[0m       output_types\u001b[38;5;241m=\u001b[39mdtypes,\n\u001b[0;32m   3386\u001b[0m       control_inputs\u001b[38;5;241m=\u001b[39mcontrol_inputs,\n\u001b[0;32m   3387\u001b[0m       input_types\u001b[38;5;241m=\u001b[39minput_types,\n\u001b[0;32m   3388\u001b[0m       original_op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_original_op,\n\u001b[0;32m   3389\u001b[0m       op_def\u001b[38;5;241m=\u001b[39mop_def,\n\u001b[0;32m   3390\u001b[0m   )\n\u001b[0;32m   3391\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_op_helper(ret, compute_device\u001b[38;5;241m=\u001b[39mcompute_device)\n\u001b[0;32m   3392\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1889\u001b[0m, in \u001b[0;36mOperation.from_node_def\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1886\u001b[0m     control_input_ops\u001b[38;5;241m.\u001b[39mappend(control_op)\n\u001b[0;32m   1888\u001b[0m \u001b[38;5;66;03m# Initialize c_op from node_def and other inputs\u001b[39;00m\n\u001b[1;32m-> 1889\u001b[0m c_op \u001b[38;5;241m=\u001b[39m _create_c_op(g, node_def, inputs, control_input_ops, op_def\u001b[38;5;241m=\u001b[39mop_def)\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Operation(c_op, GraphTensor)\n\u001b[0;32m   1891\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init(g)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1748\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[0;32m   1744\u001b[0m   pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_SetAttrValueProto(op_desc, compat\u001b[38;5;241m.\u001b[39mas_str(name),\n\u001b[0;32m   1745\u001b[0m                                          serialized)\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1748\u001b[0m   c_op \u001b[38;5;241m=\u001b[39m pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_FinishOperation(op_desc)\n\u001b[0;32m   1749\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInvalidArgumentError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1750\u001b[0m   \u001b[38;5;66;03m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n\u001b[0;32m   1751\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainingData_prep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8df106c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T17:01:12.383036Z",
     "start_time": "2023-10-23T17:01:09.784468Z"
    }
   },
   "outputs": [],
   "source": [
    "# requires images converted to feature array\n",
    "\n",
    "# base_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "#               loss=tf.keras.losses.MeanSquaredError(),\n",
    "#               metrics=[tf.keras.metrics.Accuracy(),\n",
    "#                        tf.keras.metrics.Precision(),\n",
    "#                        tf.keras.metrics.Recall(),\n",
    "#                        tf.keras.metrics.F1Score()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8e4c5e20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T09:08:44.414345Z",
     "start_time": "2023-10-24T09:08:44.039386Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_83\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_85 (InputLayer)       [(None, 224, 224, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)         1792      ['input_85[0][0]']            \n",
      "                                                                                                  \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)         36928     ['block1_conv1[0][0]']        \n",
      "                                                                                                  \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)         0         ['block1_conv2[0][0]']        \n",
      "                                                                                                  \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)        73856     ['block1_pool[0][0]']         \n",
      "                                                                                                  \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)        147584    ['block2_conv1[0][0]']        \n",
      "                                                                                                  \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)          0         ['block2_conv2[0][0]']        \n",
      "                                                                                                  \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)          295168    ['block2_pool[0][0]']         \n",
      "                                                                                                  \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)          590080    ['block3_conv1[0][0]']        \n",
      "                                                                                                  \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)          590080    ['block3_conv2[0][0]']        \n",
      "                                                                                                  \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)          0         ['block3_conv3[0][0]']        \n",
      "                                                                                                  \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)          1180160   ['block3_pool[0][0]']         \n",
      "                                                                                                  \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)          2359808   ['block4_conv1[0][0]']        \n",
      "                                                                                                  \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)          2359808   ['block4_conv2[0][0]']        \n",
      "                                                                                                  \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)          0         ['block4_conv3[0][0]']        \n",
      "                                                                                                  \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)          2359808   ['block4_pool[0][0]']         \n",
      "                                                                                                  \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)          2359808   ['block5_conv1[0][0]']        \n",
      "                                                                                                  \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)          2359808   ['block5_conv2[0][0]']        \n",
      "                                                                                                  \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)            0         ['block5_conv3[0][0]']        \n",
      "                                                                                                  \n",
      " additional_dense (Dense)    (None, 7, 7, 64)             32832     ['block5_pool[0][0]']         \n",
      "                                                                                                  \n",
      " illuminance_retinex_output  (None, 7, 7, 1)              65        ['additional_dense[0][0]']    \n",
      "  (Dense)                                                                                         \n",
      "                                                                                                  \n",
      " landmark_output (Dense)     (None, 7, 7, 2)              130       ['additional_dense[0][0]']    \n",
      "                                                                                                  \n",
      " previous_illuminance_outpu  (None, 7, 7, 1)              65        ['additional_dense[0][0]']    \n",
      " t (Dense)                                                                                        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14747780 (56.26 MB)\n",
      "Trainable params: 14747780 (56.26 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = tf.keras.applications.VGG16(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "\n",
    "additional_dense_layer = tf.keras.layers.Dense(64, activation='relu', name='additional_dense')(base_model.output)\n",
    "\n",
    "task_outputs = {\n",
    "    'landmark_detection': tf.keras.layers.Dense(2, activation='relu', name='landmark_output')(additional_dense_layer),\n",
    "    'previous_illuminance': tf.keras.layers.Dense(1, activation='relu', name='previous_illuminance_output')(additional_dense_layer),\n",
    "    'illuminance_retinex': tf.keras.layers.Dense(1, activation='relu', name='illuminance_retinex_output')(additional_dense_layer)\n",
    "}\n",
    "\n",
    "inputData = [trainLandmarks, trainIllumsRaw, trainIllumsRet]\n",
    "\n",
    "multi_task_model = tf.keras.Model(inputs=base_model.input, outputs=task_outputs)\n",
    "\n",
    "# Compile the model with specific loss functions and metrics for each task\n",
    "multi_task_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={\n",
    "        'landmark_output': 'mean_squared_error',\n",
    "        'previous_illuminance_output': 'mean_squared_error',\n",
    "        'illuminance_retinex_output': 'mean_squared_error'\n",
    "    },\n",
    "    metrics={\n",
    "        'landmark_output': 'mse', \"accuracy\"\n",
    "        'previous_illuminance_output': 'mse', \"accuracy\" \n",
    "        'illuminance_retinex_output': 'mse' \"accuracy\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Summary of the multi-task model\n",
    "multi_task_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f6d29847",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T09:08:51.355068Z",
     "start_time": "2023-10-24T09:08:51.347711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'left_eye': (213, 232), 'right_eye': (306, 222), 'nose': (239, 283), 'mouth_left': (227, 342), 'mouth_right': (298, 332)}, {'left_eye': (215, 230), 'right_eye': (307, 222), 'nose': (238, 285), 'mouth_left': (228, 339), 'mouth_right': (299, 333)}, {'left_eye': (207, 242), 'right_eye': (299, 228), 'nose': (236, 291), 'mouth_left': (226, 352), 'mouth_right': (296, 340)}, {'left_eye': (218, 240), 'right_eye': (313, 232), 'nose': (245, 291), 'mouth_left': (231, 352), 'mouth_right': (304, 345)}, {'left_eye': (223, 246), 'right_eye': (304, 238), 'nose': (242, 295), 'mouth_left': (234, 350), 'mouth_right': (302, 344)}, {'left_eye': (214, 237), 'right_eye': (306, 233), 'nose': (241, 295), 'mouth_left': (227, 352), 'mouth_right': (298, 348)}, {'left_eye': (216, 241), 'right_eye': (309, 230), 'nose': (245, 288), 'mouth_left': (231, 351), 'mouth_right': (303, 342)}, {'left_eye': (225, 235), 'right_eye': (317, 232), 'nose': (249, 291), 'mouth_left': (235, 346), 'mouth_right': (305, 343)}, {'left_eye': (214, 240), 'right_eye': (306, 231), 'nose': (243, 293), 'mouth_left': (229, 350), 'mouth_right': (300, 342)}, {'left_eye': (214, 239), 'right_eye': (305, 232), 'nose': (240, 294), 'mouth_left': (227, 352), 'mouth_right': (299, 347)}, {'left_eye': (207, 241), 'right_eye': (301, 230), 'nose': (236, 291), 'mouth_left': (224, 354), 'mouth_right': (296, 345)}, {'left_eye': (206, 241), 'right_eye': (298, 229), 'nose': (232, 291), 'mouth_left': (223, 354), 'mouth_right': (293, 344)}, {'left_eye': (205, 243), 'right_eye': (299, 232), 'nose': (234, 293), 'mouth_left': (222, 355), 'mouth_right': (293, 346)}]\n",
      "[0.29567792373263885, 0.2929762817534723, 0.29606498448350704, 0.2991066410894097, 0.29778815756076393, 0.29540649650173617, 0.2979124167057292, 0.29994796240885413, 0.29637356141927085, 0.2969686096050347, 0.2955887041015627, 0.2953554853255209, 0.29572583037326383]\n",
      "[0.516026363376736, 0.513045514422743, 0.5178306256293403, 0.5205153679947918, 0.5188656798350694, 0.5162518392838541, 0.5190531985807292, 0.5218593519053819, 0.5163899465885416, 0.5168914892708333, 0.5162052219791669, 0.5153277215190973, 0.517773762782118]\n"
     ]
    }
   ],
   "source": [
    "# multi_task_model.fit()\n",
    "# trainLandmarks = trainLandmarks[:-1]\n",
    "# trainIllumsRaw = trainIllumsRaw[:-1]\n",
    "print(trainLandmarks)\n",
    "print(trainIllumsRaw)\n",
    "print(trainIllumsRet)\n",
    "\n",
    "trainIllumsRawArray = np.array(trainIllumsRaw)\n",
    "trainIllumsRetArray = np.array(trainIllumsRet)\n",
    "\n",
    "# print(trainIllumsRawArray)\n",
    "# print(trainIllumsRetArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d888c6eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T09:08:53.459416Z",
     "start_time": "2023-10-24T09:08:53.451547Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'left_eye': (213, 232), 'right_eye': (306, 222), 'nose': (239, 283), 'mouth_left': (227, 342), 'mouth_right': (298, 332)}\n",
      "  0.29567792373263885 0.516026363376736]\n",
      " [{'left_eye': (215, 230), 'right_eye': (307, 222), 'nose': (238, 285), 'mouth_left': (228, 339), 'mouth_right': (299, 333)}\n",
      "  0.2929762817534723 0.513045514422743]\n",
      " [{'left_eye': (207, 242), 'right_eye': (299, 228), 'nose': (236, 291), 'mouth_left': (226, 352), 'mouth_right': (296, 340)}\n",
      "  0.29606498448350704 0.5178306256293403]\n",
      " [{'left_eye': (218, 240), 'right_eye': (313, 232), 'nose': (245, 291), 'mouth_left': (231, 352), 'mouth_right': (304, 345)}\n",
      "  0.2991066410894097 0.5205153679947918]\n",
      " [{'left_eye': (223, 246), 'right_eye': (304, 238), 'nose': (242, 295), 'mouth_left': (234, 350), 'mouth_right': (302, 344)}\n",
      "  0.29778815756076393 0.5188656798350694]\n",
      " [{'left_eye': (214, 237), 'right_eye': (306, 233), 'nose': (241, 295), 'mouth_left': (227, 352), 'mouth_right': (298, 348)}\n",
      "  0.29540649650173617 0.5162518392838541]\n",
      " [{'left_eye': (216, 241), 'right_eye': (309, 230), 'nose': (245, 288), 'mouth_left': (231, 351), 'mouth_right': (303, 342)}\n",
      "  0.2979124167057292 0.5190531985807292]\n",
      " [{'left_eye': (225, 235), 'right_eye': (317, 232), 'nose': (249, 291), 'mouth_left': (235, 346), 'mouth_right': (305, 343)}\n",
      "  0.29994796240885413 0.5218593519053819]\n",
      " [{'left_eye': (214, 240), 'right_eye': (306, 231), 'nose': (243, 293), 'mouth_left': (229, 350), 'mouth_right': (300, 342)}\n",
      "  0.29637356141927085 0.5163899465885416]\n",
      " [{'left_eye': (214, 239), 'right_eye': (305, 232), 'nose': (240, 294), 'mouth_left': (227, 352), 'mouth_right': (299, 347)}\n",
      "  0.2969686096050347 0.5168914892708333]\n",
      " [{'left_eye': (207, 241), 'right_eye': (301, 230), 'nose': (236, 291), 'mouth_left': (224, 354), 'mouth_right': (296, 345)}\n",
      "  0.2955887041015627 0.5162052219791669]\n",
      " [{'left_eye': (206, 241), 'right_eye': (298, 229), 'nose': (232, 291), 'mouth_left': (223, 354), 'mouth_right': (293, 344)}\n",
      "  0.2953554853255209 0.5153277215190973]\n",
      " [{'left_eye': (205, 243), 'right_eye': (299, 232), 'nose': (234, 293), 'mouth_left': (222, 355), 'mouth_right': (293, 346)}\n",
      "  0.29572583037326383 0.517773762782118]]\n"
     ]
    }
   ],
   "source": [
    "Y_train = np.vstack((trainLandmarks, trainIllumsRaw, trainIllumsRet)).T\n",
    "print(Y_train)\n",
    "\n",
    "# Sample 2D NumPy array\n",
    "data = Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6eff25d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T09:08:55.876589Z",
     "start_time": "2023-10-24T09:08:55.867347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[213 232 306 222 239 283 227 342 298 332]\n",
      " [215 230 307 222 238 285 228 339 299 333]\n",
      " [207 242 299 228 236 291 226 352 296 340]\n",
      " [218 240 313 232 245 291 231 352 304 345]\n",
      " [223 246 304 238 242 295 234 350 302 344]\n",
      " [214 237 306 233 241 295 227 352 298 348]\n",
      " [216 241 309 230 245 288 231 351 303 342]\n",
      " [225 235 317 232 249 291 235 346 305 343]\n",
      " [214 240 306 231 243 293 229 350 300 342]\n",
      " [214 239 305 232 240 294 227 352 299 347]\n",
      " [207 241 301 230 236 291 224 354 296 345]\n",
      " [206 241 298 229 232 291 223 354 293 344]\n",
      " [205 243 299 232 234 293 222 355 293 346]]\n"
     ]
    }
   ],
   "source": [
    "# Sample 2D NumPy array\n",
    "data = Y_train\n",
    "\n",
    "# Extract numerical values from the dictionaries\n",
    "numerical_values = []\n",
    "\n",
    "for row in data:\n",
    "    row_values = []\n",
    "    for element in row:\n",
    "        if isinstance(element, dict):\n",
    "            # Extract numerical values from the dictionary\n",
    "            dict_values = [val for key, val in element.items() if isinstance(val, tuple)]\n",
    "            for tpl in dict_values:\n",
    "                for value in tpl:\n",
    "                    if isinstance(value, (int, float)):\n",
    "                        row_values.append(value)\n",
    "    numerical_values.append(row_values)\n",
    "\n",
    "# Print the numerical values\n",
    "# for row in numerical_values:\n",
    "#     print(row)\n",
    "\n",
    "numerical_values = np.array(numerical_values)\n",
    "print(numerical_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8a40ff52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T09:08:58.518327Z",
     "start_time": "2023-10-24T09:08:58.503870Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255)  # Rescale pixel values to [0, 1]\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(r'data\\Training', target_size=(224, 224), batch_size=32, class_mode='input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b74a8175",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T09:09:02.098393Z",
     "start_time": "2023-10-24T09:09:02.046811Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "for batch in train_generator:\n",
    "    images, labels = batch\n",
    "    break\n",
    "\n",
    "print(len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7de54326",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T09:09:03.638681Z",
     "start_time": "2023-10-24T09:09:03.631320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(images[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "719fab0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T09:09:05.023252Z",
     "start_time": "2023-10-24T09:09:05.010835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[9.4118e-02 9.4118e-02 9.4118e-02]\n",
      "   [9.0196e-02 9.0196e-02 9.0196e-02]\n",
      "   [9.4118e-02 9.4118e-02 9.4118e-02]\n",
      "   ...\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]]\n",
      "\n",
      "  [[1.0196e-01 1.0196e-01 1.0196e-01]\n",
      "   [1.0588e-01 1.0588e-01 1.0588e-01]\n",
      "   [1.0980e-01 1.0980e-01 1.0980e-01]\n",
      "   ...\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]]\n",
      "\n",
      "  [[1.5294e-01 1.5294e-01 1.5294e-01]\n",
      "   [1.5686e-01 1.5686e-01 1.5686e-01]\n",
      "   [1.6471e-01 1.6471e-01 1.6471e-01]\n",
      "   ...\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.9608e-02 1.9608e-02 1.9608e-02]\n",
      "   [3.1373e-02 3.1373e-02 3.1373e-02]\n",
      "   [3.1373e-02 3.1373e-02 3.1373e-02]\n",
      "   ...\n",
      "   [4.9412e-01 4.9412e-01 4.9412e-01]\n",
      "   [4.9412e-01 4.9412e-01 4.9412e-01]\n",
      "   [4.9412e-01 4.9412e-01 4.9412e-01]]\n",
      "\n",
      "  [[1.9608e-02 1.9608e-02 1.9608e-02]\n",
      "   [2.3529e-02 2.3529e-02 2.3529e-02]\n",
      "   [2.3529e-02 2.3529e-02 2.3529e-02]\n",
      "   ...\n",
      "   [5.0588e-01 5.0588e-01 5.0588e-01]\n",
      "   [4.9804e-01 4.9804e-01 4.9804e-01]\n",
      "   [4.9020e-01 4.9020e-01 4.9020e-01]]\n",
      "\n",
      "  [[1.9608e-02 1.9608e-02 1.9608e-02]\n",
      "   [1.9608e-02 1.9608e-02 1.9608e-02]\n",
      "   [1.9608e-02 1.9608e-02 1.9608e-02]\n",
      "   ...\n",
      "   [4.9804e-01 4.9804e-01 4.9804e-01]\n",
      "   [4.9020e-01 4.9020e-01 4.9020e-01]\n",
      "   [4.9412e-01 4.9412e-01 4.9412e-01]]]\n",
      "\n",
      "\n",
      " [[[1.0588e-01 1.0588e-01 1.0588e-01]\n",
      "   [1.0196e-01 1.0196e-01 1.0196e-01]\n",
      "   [1.0196e-01 1.0196e-01 1.0196e-01]\n",
      "   ...\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]\n",
      "   [6.0000e-01 6.0000e-01 6.0000e-01]]\n",
      "\n",
      "  [[1.0196e-01 1.0196e-01 1.0196e-01]\n",
      "   [1.0980e-01 1.0980e-01 1.0980e-01]\n",
      "   [1.1765e-01 1.1765e-01 1.1765e-01]\n",
      "   ...\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]\n",
      "   [6.0000e-01 6.0000e-01 6.0000e-01]]\n",
      "\n",
      "  [[1.4510e-01 1.4510e-01 1.4510e-01]\n",
      "   [1.5294e-01 1.5294e-01 1.5294e-01]\n",
      "   [1.6078e-01 1.6078e-01 1.6078e-01]\n",
      "   ...\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.9608e-02 1.9608e-02 1.9608e-02]\n",
      "   [1.9608e-02 1.9608e-02 1.9608e-02]\n",
      "   [2.3529e-02 2.3529e-02 2.3529e-02]\n",
      "   ...\n",
      "   [4.9020e-01 4.9020e-01 4.9020e-01]\n",
      "   [4.9804e-01 4.9804e-01 4.9804e-01]\n",
      "   [4.9412e-01 4.9412e-01 4.9412e-01]]\n",
      "\n",
      "  [[1.9608e-02 1.9608e-02 1.9608e-02]\n",
      "   [1.9608e-02 1.9608e-02 1.9608e-02]\n",
      "   [2.3529e-02 2.3529e-02 2.3529e-02]\n",
      "   ...\n",
      "   [4.9020e-01 4.9020e-01 4.9020e-01]\n",
      "   [4.9804e-01 4.9804e-01 4.9804e-01]\n",
      "   [4.9412e-01 4.9412e-01 4.9412e-01]]\n",
      "\n",
      "  [[1.9608e-02 1.9608e-02 1.9608e-02]\n",
      "   [1.9608e-02 1.9608e-02 1.9608e-02]\n",
      "   [2.3529e-02 2.3529e-02 2.3529e-02]\n",
      "   ...\n",
      "   [4.9412e-01 4.9412e-01 4.9412e-01]\n",
      "   [4.9412e-01 4.9412e-01 4.9412e-01]\n",
      "   [4.9412e-01 4.9412e-01 4.9412e-01]]]\n",
      "\n",
      "\n",
      " [[[9.4118e-02 9.4118e-02 9.4118e-02]\n",
      "   [9.8039e-02 9.8039e-02 9.8039e-02]\n",
      "   [1.0196e-01 1.0196e-01 1.0196e-01]\n",
      "   ...\n",
      "   [6.1176e-01 6.1176e-01 6.1176e-01]\n",
      "   [6.1176e-01 6.1176e-01 6.1176e-01]\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]]\n",
      "\n",
      "  [[1.0980e-01 1.0980e-01 1.0980e-01]\n",
      "   [1.0980e-01 1.0980e-01 1.0980e-01]\n",
      "   [1.2549e-01 1.2549e-01 1.2549e-01]\n",
      "   ...\n",
      "   [6.1176e-01 6.1176e-01 6.1176e-01]\n",
      "   [6.0784e-01 6.0784e-01 6.0784e-01]\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]]\n",
      "\n",
      "  [[1.5294e-01 1.5294e-01 1.5294e-01]\n",
      "   [1.5686e-01 1.5686e-01 1.5686e-01]\n",
      "   [1.6863e-01 1.6863e-01 1.6863e-01]\n",
      "   ...\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.9608e-02 1.9608e-02 1.9608e-02]\n",
      "   [2.3529e-02 2.3529e-02 2.3529e-02]\n",
      "   [2.3529e-02 2.3529e-02 2.3529e-02]\n",
      "   ...\n",
      "   [5.0196e-01 5.0196e-01 5.0196e-01]\n",
      "   [5.0588e-01 5.0588e-01 5.0588e-01]\n",
      "   [5.0196e-01 5.0196e-01 5.0196e-01]]\n",
      "\n",
      "  [[1.9608e-02 1.9608e-02 1.9608e-02]\n",
      "   [2.3529e-02 2.3529e-02 2.3529e-02]\n",
      "   [2.3529e-02 2.3529e-02 2.3529e-02]\n",
      "   ...\n",
      "   [5.0588e-01 5.0588e-01 5.0588e-01]\n",
      "   [5.0588e-01 5.0588e-01 5.0588e-01]\n",
      "   [4.9804e-01 4.9804e-01 4.9804e-01]]\n",
      "\n",
      "  [[1.9608e-02 1.9608e-02 1.9608e-02]\n",
      "   [1.9608e-02 1.9608e-02 1.9608e-02]\n",
      "   [1.9608e-02 1.9608e-02 1.9608e-02]\n",
      "   ...\n",
      "   [5.0196e-01 5.0196e-01 5.0196e-01]\n",
      "   [5.0196e-01 5.0196e-01 5.0196e-01]\n",
      "   [4.9804e-01 4.9804e-01 4.9804e-01]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[1.0588e-01 1.0588e-01 1.0588e-01]\n",
      "   [1.0588e-01 1.0588e-01 1.0588e-01]\n",
      "   [9.8039e-02 9.8039e-02 9.8039e-02]\n",
      "   ...\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]\n",
      "   [6.0000e-01 6.0000e-01 6.0000e-01]]\n",
      "\n",
      "  [[1.0196e-01 1.0196e-01 1.0196e-01]\n",
      "   [1.0588e-01 1.0588e-01 1.0588e-01]\n",
      "   [1.0588e-01 1.0588e-01 1.0588e-01]\n",
      "   ...\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]\n",
      "   [6.0000e-01 6.0000e-01 6.0000e-01]]\n",
      "\n",
      "  [[1.4118e-01 1.4118e-01 1.4118e-01]\n",
      "   [1.4902e-01 1.4902e-01 1.4902e-01]\n",
      "   [1.6078e-01 1.6078e-01 1.6078e-01]\n",
      "   ...\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[2.3529e-02 2.3529e-02 2.3529e-02]\n",
      "   [2.3529e-02 2.3529e-02 2.3529e-02]\n",
      "   [2.3529e-02 2.3529e-02 2.3529e-02]\n",
      "   ...\n",
      "   [4.9020e-01 4.9020e-01 4.9020e-01]\n",
      "   [4.9804e-01 4.9804e-01 4.9804e-01]\n",
      "   [4.9804e-01 4.9804e-01 4.9804e-01]]\n",
      "\n",
      "  [[2.3529e-02 2.3529e-02 2.3529e-02]\n",
      "   [2.3529e-02 2.3529e-02 2.3529e-02]\n",
      "   [2.3529e-02 2.3529e-02 2.3529e-02]\n",
      "   ...\n",
      "   [4.9020e-01 4.9020e-01 4.9020e-01]\n",
      "   [4.9412e-01 4.9412e-01 4.9412e-01]\n",
      "   [4.9412e-01 4.9412e-01 4.9412e-01]]\n",
      "\n",
      "  [[2.3529e-02 2.3529e-02 2.3529e-02]\n",
      "   [2.3529e-02 2.3529e-02 2.3529e-02]\n",
      "   [2.3529e-02 2.3529e-02 2.3529e-02]\n",
      "   ...\n",
      "   [4.9412e-01 4.9412e-01 4.9412e-01]\n",
      "   [4.9412e-01 4.9412e-01 4.9412e-01]\n",
      "   [4.9020e-01 4.9020e-01 4.9020e-01]]]\n",
      "\n",
      "\n",
      " [[[9.4118e-02 9.4118e-02 9.4118e-02]\n",
      "   [9.4118e-02 9.4118e-02 9.4118e-02]\n",
      "   [9.4118e-02 9.4118e-02 9.4118e-02]\n",
      "   ...\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]]\n",
      "\n",
      "  [[1.0588e-01 1.0588e-01 1.0588e-01]\n",
      "   [1.1765e-01 1.1765e-01 1.1765e-01]\n",
      "   [1.2157e-01 1.2157e-01 1.2157e-01]\n",
      "   ...\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]]\n",
      "\n",
      "  [[1.4510e-01 1.4510e-01 1.4510e-01]\n",
      "   [1.5686e-01 1.5686e-01 1.5686e-01]\n",
      "   [1.6471e-01 1.6471e-01 1.6471e-01]\n",
      "   ...\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]\n",
      "   [6.0000e-01 6.0000e-01 6.0000e-01]\n",
      "   [6.0000e-01 6.0000e-01 6.0000e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[2.3529e-02 2.3529e-02 2.3529e-02]\n",
      "   [1.9608e-02 1.9608e-02 1.9608e-02]\n",
      "   [2.3529e-02 2.3529e-02 2.3529e-02]\n",
      "   ...\n",
      "   [4.9412e-01 4.9412e-01 4.9412e-01]\n",
      "   [5.0196e-01 5.0196e-01 5.0196e-01]\n",
      "   [5.0196e-01 5.0196e-01 5.0196e-01]]\n",
      "\n",
      "  [[1.9608e-02 1.9608e-02 1.9608e-02]\n",
      "   [2.3529e-02 2.3529e-02 2.3529e-02]\n",
      "   [2.3529e-02 2.3529e-02 2.3529e-02]\n",
      "   ...\n",
      "   [4.9020e-01 4.9020e-01 4.9020e-01]\n",
      "   [4.9412e-01 4.9412e-01 4.9412e-01]\n",
      "   [4.9020e-01 4.9020e-01 4.9020e-01]]\n",
      "\n",
      "  [[1.9608e-02 1.9608e-02 1.9608e-02]\n",
      "   [2.3529e-02 2.3529e-02 2.3529e-02]\n",
      "   [1.9608e-02 1.9608e-02 1.9608e-02]\n",
      "   ...\n",
      "   [4.9412e-01 4.9412e-01 4.9412e-01]\n",
      "   [4.9412e-01 4.9412e-01 4.9412e-01]\n",
      "   [4.9412e-01 4.9412e-01 4.9412e-01]]]\n",
      "\n",
      "\n",
      " [[[1.0588e-01 1.0588e-01 1.0588e-01]\n",
      "   [1.0196e-01 1.0196e-01 1.0196e-01]\n",
      "   [9.4118e-02 9.4118e-02 9.4118e-02]\n",
      "   ...\n",
      "   [6.1176e-01 6.1176e-01 6.1176e-01]\n",
      "   [6.1176e-01 6.1176e-01 6.1176e-01]\n",
      "   [6.1176e-01 6.1176e-01 6.1176e-01]]\n",
      "\n",
      "  [[9.8039e-02 9.8039e-02 9.8039e-02]\n",
      "   [1.0588e-01 1.0588e-01 1.0588e-01]\n",
      "   [1.0980e-01 1.0980e-01 1.0980e-01]\n",
      "   ...\n",
      "   [6.1176e-01 6.1176e-01 6.1176e-01]\n",
      "   [6.1176e-01 6.1176e-01 6.1176e-01]\n",
      "   [6.1176e-01 6.1176e-01 6.1176e-01]]\n",
      "\n",
      "  [[1.4902e-01 1.4902e-01 1.4902e-01]\n",
      "   [1.5686e-01 1.5686e-01 1.5686e-01]\n",
      "   [1.5686e-01 1.5686e-01 1.5686e-01]\n",
      "   ...\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]\n",
      "   [6.0392e-01 6.0392e-01 6.0392e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.9608e-02 1.9608e-02 1.9608e-02]\n",
      "   [2.3529e-02 2.3529e-02 2.3529e-02]\n",
      "   [3.1373e-02 3.1373e-02 3.1373e-02]\n",
      "   ...\n",
      "   [4.9020e-01 4.9020e-01 4.9020e-01]\n",
      "   [4.9804e-01 4.9804e-01 4.9804e-01]\n",
      "   [4.9020e-01 4.9020e-01 4.9020e-01]]\n",
      "\n",
      "  [[1.9608e-02 1.9608e-02 1.9608e-02]\n",
      "   [1.9608e-02 1.9608e-02 1.9608e-02]\n",
      "   [2.7451e-02 2.7451e-02 2.7451e-02]\n",
      "   ...\n",
      "   [5.0196e-01 5.0196e-01 5.0196e-01]\n",
      "   [4.9412e-01 4.9412e-01 4.9412e-01]\n",
      "   [4.9020e-01 4.9020e-01 4.9020e-01]]\n",
      "\n",
      "  [[1.9608e-02 1.9608e-02 1.9608e-02]\n",
      "   [1.9608e-02 1.9608e-02 1.9608e-02]\n",
      "   [1.9608e-02 1.9608e-02 1.9608e-02]\n",
      "   ...\n",
      "   [4.9804e-01 4.9804e-01 4.9804e-01]\n",
      "   [4.9020e-01 4.9020e-01 4.9020e-01]\n",
      "   [4.9020e-01 4.9020e-01 4.9020e-01]]]], shape=(13, 224, 224, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "imageTensor = tf.convert_to_tensor(images)\n",
    "landmarkTensor = tf.convert_to_tensor(numerical_values)\n",
    "illuminanceTensor = tf.convert_to_tensor(trainIllumsRawArray)\n",
    "illumsRetTensor = tf.convert_to_tensor(trainIllumsRetArray)\n",
    "\n",
    "print(imageTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "655648b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T09:09:15.140426Z",
     "start_time": "2023-10-24T09:09:14.334806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['illuminance_retinex_output/kernel:0', 'illuminance_retinex_output/bias:0', 'previous_illuminance_output/kernel:0', 'previous_illuminance_output/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['illuminance_retinex_output/kernel:0', 'illuminance_retinex_output/bias:0', 'previous_illuminance_output/kernel:0', 'previous_illuminance_output/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\michael\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\michael\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\michael\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\michael\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1085, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\michael\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1179, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"C:\\Users\\michael\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 577, in update_state\n        self.build(y_pred, y_true)\n    File \"C:\\Users\\michael\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 483, in build\n        self._metrics = tf.__internal__.nest.map_structure_up_to(\n\n    ValueError: The two structures don't have the same sequence length. Input structure has length 6, while shallow structure has length 3.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[139], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m multi_task_model\u001b[38;5;241m.\u001b[39mfit({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_85\u001b[39m\u001b[38;5;124m'\u001b[39m: imageTensor},\n\u001b[0;32m      2\u001b[0m                                {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlandmark_output\u001b[39m\u001b[38;5;124m'\u001b[39m: landmarkTensor, \n\u001b[0;32m      3\u001b[0m                                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprevious_illuminance_output\u001b[39m\u001b[38;5;124m'\u001b[39m: illuminanceTensor, \n\u001b[0;32m      4\u001b[0m                                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124milluminance_retinex_output\u001b[39m\u001b[38;5;124m'\u001b[39m: illumsRetTensor},\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#                                },\u001b[39;00m\n\u001b[0;32m      6\u001b[0m                                epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m      7\u001b[0m                                batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filez07ecxyf.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\michael\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\michael\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\michael\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\michael\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1085, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\michael\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1179, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"C:\\Users\\michael\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 577, in update_state\n        self.build(y_pred, y_true)\n    File \"C:\\Users\\michael\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 483, in build\n        self._metrics = tf.__internal__.nest.map_structure_up_to(\n\n    ValueError: The two structures don't have the same sequence length. Input structure has length 6, while shallow structure has length 3.\n"
     ]
    }
   ],
   "source": [
    "history = multi_task_model.fit({'input_85': imageTensor},\n",
    "                               {'landmark_output': landmarkTensor, \n",
    "                                'previous_illuminance_output': illuminanceTensor, \n",
    "                                'illuminance_retinex_output': illumsRetTensor},\n",
    "#                                },\n",
    "                               epochs=10,\n",
    "                               batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fab1e751",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T08:49:37.253726Z",
     "start_time": "2023-10-24T08:49:37.249206Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.1603e-01, 5.1305e-01, 5.1783e-01, 5.2052e-01, 5.1887e-01,\n",
       "       5.1625e-01, 5.1905e-01, 5.2186e-01, 5.1639e-01, 5.1689e-01,\n",
       "       5.1621e-01, 5.1533e-01, 5.1777e-01])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_values\n",
    "trainIllumsRawArray\n",
    "trainIllumsRetArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6f354310",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T08:49:40.213522Z",
     "start_time": "2023-10-24T08:49:40.205309Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.9568e-01, 2.9298e-01, 2.9606e-01, 2.9911e-01, 2.9779e-01,\n",
       "       2.9541e-01, 2.9791e-01, 2.9995e-01, 2.9637e-01, 2.9697e-01,\n",
       "       2.9559e-01, 2.9536e-01, 2.9573e-01])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainIllumsRawArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9e7e661e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T08:49:41.960426Z",
     "start_time": "2023-10-24T08:49:41.953075Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.1603e-01, 5.1305e-01, 5.1783e-01, 5.2052e-01, 5.1887e-01,\n",
       "       5.1625e-01, 5.1905e-01, 5.2186e-01, 5.1639e-01, 5.1689e-01,\n",
       "       5.1621e-01, 5.1533e-01, 5.1777e-01])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainIllumsRetArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8295d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
