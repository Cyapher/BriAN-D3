{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f69663c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T08:35:49.415956Z",
     "start_time": "2023-10-31T08:35:37.017689Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from luxpy import (cat, colortf, _CIEOBS, _CIE_ILLUMINANTS, _CRI_RFL, _CIE_D65,_CIE_E,\n",
    "                   spd_to_xyz, plot_color_data, math, cie_interp, getwlr, xyz_to_srgb)\n",
    "from luxpy.utils import np, plt, sp, _PKG_PATH, _SEP, _EPS \n",
    "import warnings\n",
    "from imageio import imsave\n",
    "\n",
    "import tensorflow as tf\n",
    "from mtcnn import MTCNN\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import glob\n",
    "import cv2\n",
    "import csv\n",
    "import os\n",
    "import pydot\n",
    "import pydotplus\n",
    "import graphviz\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bf89806",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T08:36:05.109264Z",
     "start_time": "2023-10-31T08:36:05.104787Z"
    }
   },
   "outputs": [],
   "source": [
    "trainLandmarks = []\n",
    "trainLandmarksRet = []\n",
    "trainFileNames = []\n",
    "trainIllumsRaw = []\n",
    "trainIllumsRet = []\n",
    "evalLandmarks = []\n",
    "evalLandmarksRet = []\n",
    "evalFileNames = []\n",
    "evalIllumsRaw = []\n",
    "evalIllumsRet = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "096c8392",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T08:36:06.050631Z",
     "start_time": "2023-10-31T08:36:06.025302Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def frame_capture(file):\n",
    "    \n",
    "    cap = cv2.VideoCapture(file)\n",
    "    \n",
    "    try:\n",
    "        if not os.path.exists('data'):\n",
    "            os.makedirs('data')\n",
    "    except OSError:\n",
    "        print('Error: Creating directory of data')\n",
    "            \n",
    "    current_frame = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "#         print(file)\n",
    "        if not ret:\n",
    "            current_frame = 0\n",
    "            break \n",
    "\n",
    "        if current_frame % 15 == 0:\n",
    "            \n",
    "            datasetDir = os.path.dirname(os.path.dirname(os.path.dirname(file)))\n",
    "            indexDataset = datasetDir.rfind('\\\\')\n",
    "            datasetDir = datasetDir[indexDataset:]\n",
    "            \n",
    "            parentDir = os.path.dirname(os.path.dirname(file)) # refers to <driver_num> e.g. \"001, 002\"\n",
    "            currDir = parentDir\n",
    "            \n",
    "            childDir = os.path.dirname(file) # refers to where the current video file is <scenario> e.g. noglasses, glasses, etc.\n",
    "            indexParent = parentDir.rfind('\\\\')\n",
    "            parentDir = parentDir[indexParent:] # refers to <scenario> only\n",
    "            \n",
    "            indexChild = childDir.rfind('\\\\')\n",
    "            childDir = childDir[indexChild:]\n",
    "            \n",
    "#             filename of txt document containing labels for current video\n",
    "            if datasetDir[1:] == \"Training Dataset\":\n",
    "                labelFile = currDir + \"\\\\\" + childDir[1:] + \"\\\\\" + parentDir[1:] + \"_\" + os.path.basename(file)[:-4] + \"_drowsiness\" + '.txt'\n",
    "            elif parentDir[1:] == \"Evaluation Dataset\":\n",
    "                labelFile = currDir + \"\\\\\" + childDir[1:] + \"\\\\\" + os.path.basename(file)[:-4] + \"_drowsiness\" + '.txt'\n",
    "                labelFile = labelFile.replace(\"mix\", \"mixing\")\n",
    "                labelFile = labelFile.replace(\"nightno\", \"night_no\")\n",
    "            else:\n",
    "                scenario = \"\"\n",
    "                if \"sunglasses\" in os.path.basename(file)[:-4]:\n",
    "                    scenario = \"sunglasses\"\n",
    "                elif \"night_noglasses\" in os.path.basename(file)[:-4]:\n",
    "                    scenario = \"night_noglasses\"    \n",
    "                elif \"noglasses\" in os.path.basename(file)[:-4]:\n",
    "                    scenario = \"noglasses\"\n",
    "                elif \"night_glasses\" in os.path.basename(file)[:-4]:\n",
    "                    scenario = \"sunglasses\"\n",
    "                else:\n",
    "                    scenario = \"glasses\"\n",
    "                \n",
    "                labelFile = currDir + \"\\\\\" + childDir[1:] + \"\\\\\" + \"test_label_txt\\\\\" + scenario + \"\\\\\" + os.path.basename(file)[:-4] + \"_drowsiness\" + '.txt'\n",
    "                \n",
    "                if not os.path.exists(labelFile):                    \n",
    "                    labelFile = currDir + \"\\\\\" + childDir[1:] + \"\\\\\" + \"test_label_txt\\\\\" + \"wh\" + \"\\\\\" + os.path.basename(file)[:-4] + \"_drowsiness\" + '.txt'\n",
    "                \n",
    "                labelFile = labelFile.replace(\"mix\", \"mixing\")\n",
    "                \n",
    "            with open(labelFile) as f:\n",
    "                labels = f.readline()\n",
    "            try:\n",
    "                \n",
    "                if datasetDir[1:] == \"Training Dataset\":\n",
    "                    save_path = \"./data/Training/\" # specify where (in relation to root path to place new folder)\n",
    "                    file_name = save_path + parentDir[1:] + \"_\" + os.path.basename(file)[:-4] + \"/\" + parentDir[1:] + \"_\" + os.path.basename(file)[:-4] + \"_\" + str(current_frame) + \"_\" + labels[current_frame] + '.jpg'\n",
    "                elif parentDir[1:] == \"Evaluation Dataset\":\n",
    "                    save_path = \"./data/Evaluation/\" # specify where (in relation to root path to place new folder)\n",
    "                    file_name = save_path + os.path.basename(file)[:-4] + \"/\" + os.path.basename(file)[:-4] + \"_\" + str(current_frame) + \"_\" + labels[current_frame] + '.jpg'\n",
    "                else:\n",
    "                    save_path = \"./data/Testing/\" # specify where (in relation to root path to place new folder)\n",
    "                    file_name = save_path + os.path.basename(file)[:-4] + \"/\" + os.path.basename(file)[:-4] + \"_\" + str(current_frame) + \"_\" + labels[current_frame] + '.jpg'\n",
    "                \n",
    "#                 Creating..../data/Testing/037_sunglasses_mix/037_sunglasses_mix_7110_1.jpg\n",
    "                \n",
    "            except IndexError:\n",
    "                continue\n",
    "            \n",
    "            print('Creating...' + file_name)\n",
    "            cv2.imwrite(file_name, frame)\n",
    "            \n",
    "            img_file = Path(file_name) # for Retinex images, use replace() to get path of same named images in order to get post-retinex illum\n",
    "            \n",
    "            if save_path == \"./data/Training/\":\n",
    "                trainFileNames.append(os.path.basename(img_file)[:-4])\n",
    "                trainLandmarks.append(generateLandmarks(os.path.dirname(img_file) + \"\\\\\" + os.path.basename(img_file)))\n",
    "                trainIllumsRaw.append(illuminanceEstimation(img_file))\n",
    "                \n",
    "                retSave_path = \"./data/RetTraining/\" + parentDir[1:] + \"_\" + os.path.basename(file)[:-4] + \"/\"\n",
    "                retinexImplement(img_file, retSave_path)\n",
    "                \n",
    "#               post-retinex functions\n",
    "                \n",
    "                retSave_path = retSave_path + os.path.basename(img_file)[:-4] + \"_URetinexNet.jpg\"\n",
    "                trainIllumsRet.append(illuminanceEstimation(retSave_path))\n",
    "            \n",
    "                trainLandmarksRet.append(generateLandmarks(retSave_path))\n",
    "                \n",
    "                \n",
    "\n",
    "            elif save_path == \"./data/Evaluation/\":\n",
    "                evalFileNames.append(os.path.basename(img_file)[:-4])\n",
    "                evalLandmarks.append(generateLandmarks(os.path.dirname(img_file) + \"\\\\\" + os.path.basename(img_file)))\n",
    "                evalIllumsRaw.append(illuminanceEstimation(img_file))\n",
    "                \n",
    "                retSave_path = \"./data/RetEvaluation/\" + parentDir[1:] + \"_\" + os.path.basename(file)[:-4] + \"/\"\n",
    "                retinexImplement(img_file, retSave_path)\n",
    "                \n",
    "#               post-retinex functions\n",
    "                \n",
    "                retSave_path = retSave_path + os.path.basename(img_file)[:-4] + \"_URetinexNet.jpg\"\n",
    "                evalIllumsRet.append(illuminanceEstimation(retSave_path))\n",
    "            \n",
    "                evalLandmarksRet.append(generateLandmarks(retSave_path))\n",
    "            \n",
    "            \n",
    "        current_frame += 1\n",
    "    \n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcf88578",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T08:36:06.937244Z",
     "start_time": "2023-10-31T08:36:06.924802Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Training Videos Path\n",
    "def trainingData_prep():\n",
    "    training_videos = Path(r\"NTHU Dataset\\Training_Evaluation_Dataset\\Training Dataset\") #AVIs\n",
    "    training_video_paths = []\n",
    "\n",
    "    # create data folder\n",
    "    try:\n",
    "        if not os.path.exists('data'):\n",
    "                os.makedirs('data')\n",
    "                os.mkdir(os.path.join('./data/', 'Training'))\n",
    "                os.mkdir(os.path.join('./data/', 'Evaluation'))\n",
    "                os.mkdir(os.path.join('./data/', 'Testing'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetTraining'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetEvaluation'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetTesting'))\n",
    "    except OSError:\n",
    "        print('Error: Creating directory of data')\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for driver in training_videos.glob(\"*\"):\n",
    "        for scenario in driver.glob(\"*\"):\n",
    "            for videos_file in scenario.glob(\"*.avi\"):\n",
    "\n",
    "    #         note: videos_file refers to direct path of current video file\n",
    "\n",
    "                datasetDir = os.path.dirname(os.path.dirname(os.path.dirname(videos_file)))\n",
    "                indexDataset = datasetDir.rfind('\\\\')\n",
    "                datasetDir = datasetDir[indexDataset:]\n",
    "\n",
    "                parentDir = os.path.dirname(os.path.dirname(videos_file)) # refers to <driver_num> e.g. \"001, 002\"\n",
    "                currDir = parentDir\n",
    "\n",
    "                childDir = os.path.dirname(videos_file) # refers to where the current video file is <scenario> e.g. noglasses, glasses, etc.\n",
    "                indexParent = parentDir.rfind('\\\\')\n",
    "                parentDir = parentDir[indexParent:] # refers to <scenario> only\n",
    "\n",
    "                indexChild = childDir.rfind('\\\\')\n",
    "                childDir = childDir[indexChild:]\n",
    "\n",
    "                if datasetDir[1:] == \"Training Dataset\":\n",
    "                    save_path = \"./data/Training/\" # specify where (in relation to root path to place new folder)\n",
    "                    folder_name = parentDir[1:] + \"_\" + os.path.basename(videos_file)[:-4] # specify name of new folder\n",
    "                elif parentDir[1:] == \"Evaluation Dataset\":\n",
    "                    save_path = \"./data/Evaluation/\" # specify where (in relation to root path to place new folder)\n",
    "                    folder_name = os.path.basename(videos_file)[:-4] # specify name of new folder\n",
    "\n",
    "                data_path = os.path.join(save_path, folder_name) # set path and folder name as input for os.path.join()\n",
    "\n",
    "                inputPath = save_path + folder_name\n",
    "\n",
    "                if not os.path.exists(inputPath):\n",
    "                    os.mkdir(data_path) # create new folder based on data_path\n",
    "\n",
    "                video_path = str(videos_file)\n",
    "                training_video_paths.append(video_path)\n",
    "                frame_capture(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "732f5fa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T02:54:41.267995Z",
     "start_time": "2023-10-26T02:54:41.257009Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluation Videos Path\n",
    "def evalData_prep():\n",
    "    evaluation_videos = Path(r\"NTHU Dataset\\Training_Evaluation_Dataset\\Evaluation Dataset\") #AVIs\n",
    "    evaluation_video_paths = []\n",
    "\n",
    "    # create data folder\n",
    "    try:\n",
    "        if not os.path.exists('data'):\n",
    "                os.makedirs('data')\n",
    "                os.mkdir(os.path.join('./data/', 'Training'))\n",
    "                os.mkdir(os.path.join('./data/', 'Evaluation'))\n",
    "                os.mkdir(os.path.join('./data/', 'Testing'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetTraining'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetEvaluation'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetTesting'))\n",
    "    except OSError:\n",
    "        print('Error: Creating directory of data')\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for driver in evaluation_videos.glob(\"*\"):\n",
    "        for videos_file in driver.glob(\"*.mp4\"):\n",
    "\n",
    "    #         note: videos_file refers to direct path of current video file\n",
    "\n",
    "            datasetDir = os.path.dirname(os.path.dirname(os.path.dirname(videos_file)))\n",
    "            indexDataset = datasetDir.rfind('\\\\')\n",
    "            datasetDir = datasetDir[indexDataset:]\n",
    "\n",
    "            parentDir = os.path.dirname(os.path.dirname(videos_file)) # refers to <driver_num> e.g. \"001, 002\"\n",
    "            currDir = parentDir\n",
    "\n",
    "            childDir = os.path.dirname(videos_file) # refers to where the current video file is <scenario> e.g. noglasses, glasses, etc.\n",
    "            indexParent = parentDir.rfind('\\\\')\n",
    "            parentDir = parentDir[indexParent:] # refers to <scenario> only\n",
    "\n",
    "            indexChild = childDir.rfind('\\\\')\n",
    "            childDir = childDir[indexChild:]\n",
    "\n",
    "            if datasetDir[1:] == \"Training Dataset\":\n",
    "                save_path = \"./data/Training/\" # specify where (in relation to root path to place new folder)\n",
    "                folder_name = parentDir[1:] + \"_\" + os.path.basename(videos_file)[:-4] # specify name of new folder\n",
    "            elif parentDir[1:] == \"Evaluation Dataset\":\n",
    "                save_path = \"./data/Evaluation/\" # specify where (in relation to root path to place new folder)\n",
    "                folder_name = os.path.basename(videos_file)[:-4] # specify name of new folder\n",
    "\n",
    "            data_path = os.path.join(save_path, folder_name) # set path and folder name as input for os.path.join()\n",
    "\n",
    "            inputPath = save_path + folder_name\n",
    "\n",
    "            if not os.path.exists(inputPath):\n",
    "                os.mkdir(data_path) # create new folder based on data_path\n",
    "\n",
    "            video_path = str(videos_file)\n",
    "            evaluation_video_paths.append(video_path)\n",
    "            frame_capture(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2243a300",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T02:54:41.502677Z",
     "start_time": "2023-10-26T02:54:41.496660Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Testing Videos Path\n",
    "def testData_prep():\n",
    "    testing_videos = Path(r\"NTHU Dataset\\Testing_Dataset\") #MP4s\n",
    "    testing_video_paths = []\n",
    "\n",
    "    # create data folder\n",
    "    try:\n",
    "        if not os.path.exists('data'):\n",
    "                os.makedirs('data')\n",
    "                os.mkdir(os.path.join('./data/', 'Training'))\n",
    "                os.mkdir(os.path.join('./data/', 'Evaluation'))\n",
    "                os.mkdir(os.path.join('./data/', 'Testing'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetTraining'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetEvaluation'))\n",
    "                os.mkdir(os.path.join('./data/', 'RetTesting'))\n",
    "    except OSError:\n",
    "        print('Error: Creating directory of data')\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for videos_file in testing_videos.glob(\"*.mp4\"):\n",
    "        print(videos_file)\n",
    "\n",
    "#         note: videos_file refers to direct path of current video file\n",
    "\n",
    "        print(os.path.dirname(videos_file))\n",
    "\n",
    "        datasetDir = os.path.dirname(videos_file)\n",
    "        indexDataset = datasetDir.rfind('\\\\')\n",
    "        datasetDir = datasetDir[indexDataset:]\n",
    "\n",
    "        save_path = \"./data/Testing/\" # specify where (in relation to root path to place new folder)\n",
    "        folder_name = os.path.basename(videos_file)[:-4] # specify name of new folder\n",
    "\n",
    "        print(save_path + folder_name)\n",
    "\n",
    "        data_path = os.path.join(save_path, folder_name) # set path and folder name as input for os.path.join()\n",
    "\n",
    "        inputPath = save_path + folder_name\n",
    "\n",
    "        if not os.path.exists(inputPath):\n",
    "            os.mkdir(data_path) # create new folder based on data_path\n",
    "\n",
    "        video_path = str(videos_file)\n",
    "        testing_video_paths.append(video_path)\n",
    "        frame_capture(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0936f309",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T08:36:09.050716Z",
     "start_time": "2023-10-31T08:36:08.966675Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*- ILLUM EST FUNCTIONS\n",
    "\"\"\"\n",
    "Module for hyper spectral image simulation\n",
    "==========================================\n",
    "\n",
    " :_HYPSPCIM_PATH: path to module\n",
    "\n",
    " :_HYPSPCIM_DEFAULT_IMAGE: path + filename to default image\n",
    " \n",
    " :_CSF_NIKON_D700: Nikon D700 camera sensitivity functions\n",
    " \n",
    " :_ROUNDING: rounding of input to xyz_to_rfl() search algorithm for improved speed\n",
    "\n",
    " :xyz_to_rfl(): approximate spectral reflectance of xyz based on k nearest \n",
    "                neighbour interpolation of samples from a standard reflectance \n",
    "                set.\n",
    "\n",
    " :render_image(): Render image under specified light source spd.\n",
    "\n",
    " :get_superresolution_hsi(): Get a HighResolution HyperSpectral Image (super-resolution HSI) based on a LowResolution HSI and a HighResolution Color Image.\n",
    "\n",
    " :hsi_to_rgb(): Convert HyperSpectral Image to rgb\n",
    " \n",
    " :rfl_to_rgb(): Convert spectral reflectance functions (illuminated by spd) to Camera Sensitivity Functions.\n",
    "     \n",
    ".. codeauthor:: Kevin A.G. Smet (ksmet1977 at gmail.com)\n",
    "\"\"\"\n",
    "\n",
    "from luxpy import (cat, colortf, _CIEOBS, _CIE_ILLUMINANTS, _CRI_RFL, _CIE_D65,_CIE_E,\n",
    "                   spd_to_xyz, plot_color_data, math, cie_interp, getwlr, xyz_to_srgb)\n",
    "from luxpy.utils import np, plt, sp, _PKG_PATH, _SEP, _EPS \n",
    "\n",
    "import warnings\n",
    "from imageio import imsave\n",
    "\n",
    "illum_out = 0\n",
    "\n",
    "__all__ =['_HYPSPCIM_PATH','_HYPSPCIM_DEFAULT_IMAGE','render_image','xyz_to_rfl',\n",
    "          'get_superresolution_hsi','hsi_to_rgb','rfl_to_rgb','_CSF_NIKON_D700']             \n",
    "\n",
    "_HYPSPCIM_PATH = _PKG_PATH + _SEP + 'hypspcim' + _SEP\n",
    "_HYPSPCIM_DEFAULT_IMAGE = _PKG_PATH + _SEP + 'toolboxes' + _SEP + 'hypspcim' +  _SEP + 'data' + _SEP + 'testimage1.jpg'\n",
    "\n",
    "\n",
    "_ROUNDING = 6 # to speed up xyz_to_rfl search algorithm, increase if kernel dies!!!\n",
    "\n",
    "# Nikon D700 camera sensitivity functions:\n",
    "_CSF_NIKON_D700 = np.vstack((np.arange(400,710,10),\n",
    "                             np.array([[0.005, 0.007, 0.012, 0.015, 0.023, 0.025, 0.030, 0.026, 0.024, 0.019, 0.010, 0.004, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  0.000,  0.000,  0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000], \n",
    "                                       [0.000, 0.000, 0.000, 0.000, 0.000, 0.001, 0.002, 0.003, 0.005, 0.007, 0.012, 0.013, 0.015, 0.016, 0.017, 0.020, 0.013, 0.011, 0.009, 0.005,  0.001,  0.001,  0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.002, 0.002, 0.003],\n",
    "                                       [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.001, 0.003, 0.010, 0.012,  0.013,  0.022,  0.020, 0.020, 0.018, 0.017, 0.016, 0.016, 0.014, 0.014, 0.013]])[::-1]))\n",
    "\n",
    "\n",
    "def xyz_to_rfl(xyz, CSF = None, rfl = None, out = 'rfl_est', \\\n",
    "                 refspd = None, D = None, cieobs = _CIEOBS, \\\n",
    "                 cspace = 'xyz', cspace_tf = {},\\\n",
    "                 interp_type = 'nd', k_neighbours = 4, verbosity = 0,\n",
    "                 csf_based_rgb_rounding = _ROUNDING):\n",
    "    \"\"\"\n",
    "    Approximate spectral reflectance of xyz values based on nd-dimensional linear interpolation \n",
    "    or k nearest neighbour interpolation of samples from a standard reflectance set.\n",
    "    \n",
    "    Args:\n",
    "        :xyz: \n",
    "            | ndarray with xyz values of target points.\n",
    "        :CSF:\n",
    "            | None, optional\n",
    "            | RGB camera response functions.\n",
    "            | If None: input :xyz: contains raw rgb (float) values. Override :cspace:\n",
    "            | argument and perform estimation directly in raw rgb space!!!\n",
    "        :rfl: \n",
    "            | ndarray, optional\n",
    "            | Reflectance set for color coordinate to rfl mapping.\n",
    "        :out: \n",
    "            | 'rfl_est' or str, optional\n",
    "        :refspd: \n",
    "            | None, optional\n",
    "            | Refer ence spectrum for color coordinate to rfl mapping.\n",
    "            | None defaults to D65.\n",
    "        :cieobs:\n",
    "            | _CIEOBS, optional\n",
    "            | CMF set used for calculation of xyz from spectral data.\n",
    "        :cspace:\n",
    "            | 'xyz',  optional\n",
    "            | Color space for color coordinate to rfl mapping.\n",
    "            | Tip: Use linear space (e.g. 'xyz', 'Yuv',...) for (interp_type == 'nd'),\n",
    "            |      and perceptually uniform space (e.g. 'ipt') for (interp_type == 'nearest')\n",
    "        :cspace_tf:\n",
    "            | {}, optional\n",
    "            | Dict with parameters for xyz_to_cspace and cspace_to_xyz transform.\n",
    "        :interp_type:\n",
    "            | 'nd', optional\n",
    "            | Options:\n",
    "            | - 'nd': perform n-dimensional linear interpolation using Delaunay triangulation.\n",
    "            | - 'nearest': perform nearest neighbour interpolation. \n",
    "        :k_neighbours:\n",
    "            | 4 or int, optional\n",
    "            | Number of nearest neighbours for reflectance spectrum interpolation.\n",
    "            | Neighbours are found using scipy.spatial.cKDTree\n",
    "        :verbosity:\n",
    "            | 0, optional\n",
    "            | If > 0: make a plot of the color coordinates of original and \n",
    "            | rendered image pixels.\n",
    "        :csf_based_rgb_rounding:\n",
    "            | _ROUNDING, optional\n",
    "            | Int representing the number of decimals to round the RGB values (obtained from not-None CSF input) to before applying the search algorithm.\n",
    "            | Smaller values increase the search speed, but could cause fatal error that causes python kernel to die. If this happens increase the rounding int value.\n",
    "\n",
    "    Returns:\n",
    "        :returns: \n",
    "            | :rfl_est:\n",
    "            | ndarrays with estimated reflectance spectra.\n",
    "    \"\"\"\n",
    "\n",
    "    # get rfl set:\n",
    "    if rfl is None: # use IESTM30['4880'] set \n",
    "        rfl = _CRI_RFL['ies-tm30']['4880']['5nm']\n",
    "    \n",
    "    wlr = rfl[0]\n",
    "    \n",
    "    # get Ref spd:\n",
    "    if refspd is None:\n",
    "        refspd = _CIE_ILLUMINANTS['D65'].copy()\n",
    "    refspd = cie_interp(refspd, wlr, kind = 'linear') # force spd to same wavelength range as rfl\n",
    "        \n",
    "    # Calculate rgb values of standard rfl set under refspd:\n",
    "    if CSF is None:\n",
    "        # Calculate lab coordinates:\n",
    "        xyz_rr, xyz_wr = spd_to_xyz(refspd, relative = True, rfl = rfl, cieobs = cieobs, out = 2)\n",
    "        cspace_tf_copy = cspace_tf.copy()\n",
    "        cspace_tf_copy['xyzw'] = xyz_wr # put correct white point in param. dict\n",
    "        lab_rr = colortf(xyz_rr, tf = cspace, fwtf = cspace_tf_copy, bwtf = cspace_tf_copy)[:,0,:]\n",
    "    else:\n",
    "        # Calculate rgb coordinates from camera sensitivity functions\n",
    "        rgb_rr = rfl_to_rgb(rfl, spd = refspd, CSF = CSF, wl = None)   \n",
    "        lab_rr = rgb_rr\n",
    "        xyz = xyz\n",
    "        lab_rr = np.round(lab_rr,csf_based_rgb_rounding) # speed up search\n",
    "        \n",
    "        global illum_out\n",
    "        illum_out = np.mean(lab_rr)\n",
    "        print(\"Illuminance: \" + str(np.mean(lab_rr)))\n",
    "        \n",
    "    # Convert xyz to lab-type values under refspd:\n",
    "    if CSF is None:\n",
    "        lab = colortf(xyz, tf = cspace, fwtf = cspace_tf_copy, bwtf = cspace_tf_copy)\n",
    "    else:\n",
    "        lab = xyz # xyz contained rgb values !!!\n",
    "        rgb = xyz\n",
    "        lab = np.round(lab,csf_based_rgb_rounding) # speed up search\n",
    "    \n",
    "    if interp_type == 'nearest':\n",
    "        # Find rfl (cfr. lab_rr) from rfl set that results in 'near' metameric \n",
    "        # color coordinates for each value in lab_ur (i.e. smallest DE):\n",
    "        # Construct cKDTree:\n",
    "        tree = sp.spatial.cKDTree(lab_rr, copy_data = True)\n",
    "        \n",
    "        # Interpolate rfls using k nearest neightbours and inverse distance weigthing:\n",
    "        d, inds = tree.query(lab, k = k_neighbours )\n",
    "        if k_neighbours  > 1:\n",
    "            d += _EPS\n",
    "            w = (1.0 / d**2)[:,:,None] # inverse distance weigthing\n",
    "            rfl_est = np.sum(w * rfl[inds+1,:], axis=1) / np.sum(w, axis=1)\n",
    "        else:\n",
    "            rfl_est = rfl[inds+1,:].copy()\n",
    "    elif interp_type == 'nd':\n",
    "\n",
    "        rfl_est = math.ndinterp1_scipy(lab_rr, rfl[1:], lab)\n",
    "            \n",
    "        _isnan = np.isnan(rfl_est[:,0]) \n",
    "\n",
    "        if (_isnan.any()): #do nearest neigbour method for those that fail using Delaunay (i.e. ndinterp1_scipy)\n",
    "\n",
    "            # Find rfl (cfr. lab_rr) from rfl set that results in 'near' metameric \n",
    "            # color coordinates for each value in lab_ur (i.e. smallest DE):\n",
    "            # Construct cKDTree:\n",
    "            tree = sp.spatial.cKDTree(lab_rr, copy_data = True)\n",
    "\n",
    "            # Interpolate rfls using k nearest neightbours and inverse distance weigthing:\n",
    "            d, inds = tree.query(lab[_isnan,...], k = k_neighbours )\n",
    "\n",
    "            if k_neighbours  > 1:\n",
    "                d += _EPS\n",
    "                w = (1.0 / d**2)[:,:,None] # inverse distance weigthing\n",
    "                rfl_est_isnan = np.sum(w * rfl[inds+1,:], axis=1) / np.sum(w, axis=1)\n",
    "            else:\n",
    "                rfl_est_isnan = rfl[inds+1,:].copy()\n",
    "            rfl_est[_isnan, :] = rfl_est_isnan\n",
    "\n",
    "    else:\n",
    "        raise Exception('xyz_to_rfl(): unsupported interp_type!')\n",
    "    \n",
    "    rfl_est[rfl_est<0] = 0 #can occur for points outside convexhull of standard rfl set.\n",
    "\n",
    "    rfl_est = np.vstack((rfl[0],rfl_est))\n",
    "        \n",
    "    if ((verbosity > 0) | ('xyz_est' in out.split(',')) | ('lab_est' in out.split(',')) | ('DEi_ab' in out.split(',')) | ('DEa_ab' in out.split(','))) & (CSF is None):\n",
    "        xyz_est, _ = spd_to_xyz(refspd, rfl = rfl_est, relative = True, cieobs = cieobs, out = 2)\n",
    "        cspace_tf_copy = cspace_tf.copy()\n",
    "        cspace_tf_copy['xyzw'] = xyz_wr # put correct white point in param. dict\n",
    "        lab_est = colortf(xyz_est, tf = cspace, fwtf = cspace_tf_copy)[:,0,:]\n",
    "        DEi_ab = np.sqrt(((lab_est[:,1:3]-lab[:,1:3])**2).sum(axis=1))\n",
    "        DEa_ab = DEi_ab.mean()\n",
    "    elif ((verbosity > 0) | ('xyz_est' in out.split(',')) | ('rgb_est' in out.split(',')) | ('DEi_rgb' in out.split(',')) | ('DEa_rgb' in out.split(','))) & (CSF is not None):\n",
    "        rgb_est = rfl_to_rgb(rfl_est[1:], spd = refspd, CSF = CSF, wl = wlr) \n",
    "        xyz_est = rgb_est\n",
    "        DEi_rgb = np.sqrt(((rgb_est - rgb)**2).sum(axis=1))\n",
    "        DEa_rgb = DEi_rgb.mean()\n",
    "\n",
    "        \n",
    "    if verbosity > 0:\n",
    "        if CSF is None:\n",
    "            ax = plot_color_data(lab[...,1], lab[...,2], z = lab[...,0], \\\n",
    "                            show = False, cieobs = cieobs, cspace = cspace, \\\n",
    "                            formatstr = 'ro', label = 'Original')\n",
    "            plot_color_data(lab_est[...,1], lab_est[...,2], z = lab_est[...,0], \\\n",
    "                            show = True, axh = ax, cieobs = cieobs, cspace = cspace, \\\n",
    "                            formatstr = 'bd', label = 'Rendered')\n",
    "        else:\n",
    "            n = 100 #min(rfl.shape[0]-1,rfl_est.shape[0]-1)\n",
    "            s = np.random.permutation(rfl.shape[0]-1)[:min(n,rfl.shape[0]-1)]\n",
    "            st = np.random.permutation(rfl_est.shape[0]-1)[:min(n,rfl_est.shape[0]-1)]\n",
    "            fig = plt.figure()\n",
    "            ax = np.zeros((3,),dtype=np.object)\n",
    "            ax[0] = fig.add_subplot(131)\n",
    "            ax[1] = fig.add_subplot(132)\n",
    "            ax[2] = fig.add_subplot(133,projection='3d')\n",
    "            ax[0].plot(rfl[0],rfl[1:][s].T, linestyle = '-')\n",
    "            ax[0].set_title('Original RFL set (random selection of all)')\n",
    "            ax[0].set_ylim([0,1])\n",
    "            ax[1].plot(rfl_est[0],rfl_est[1:][st].T, linestyle = '--')\n",
    "            ax[0].set_title('Estimated RFL set (random selection of targets)')\n",
    "            ax[1].set_ylim([0,1])\n",
    "            ax[2].plot(rgb[st,0],rgb[st,1],rgb[st,2],'ro', label = 'Original')\n",
    "            ax[2].plot(rgb_est[st,0],rgb_est[st,1],rgb_est[st,2],'bd', label = 'Rendered')\n",
    "            ax[2].legend()\n",
    "    if out == 'rfl_est':\n",
    "        return rfl_est\n",
    "    elif out == 'rfl_est,xyz_est':\n",
    "        return rfl_est, xyz_est\n",
    "    else:\n",
    "        return eval(out)\n",
    "\n",
    "\n",
    "\n",
    "def render_image(img = None, spd = None, rfl = None, out = 'img_hyp', \\\n",
    "                 refspd = None, D = None, cieobs = _CIEOBS, \\\n",
    "                 cspace = 'xyz', cspace_tf = {}, CSF = None,\\\n",
    "                 interp_type = 'nd', k_neighbours = 4, show = True,\n",
    "                 verbosity = 0, show_ref_img = True,\\\n",
    "                 stack_test_ref = 12,\\\n",
    "                 write_to_file = None,\\\n",
    "                 csf_based_rgb_rounding = _ROUNDING):\n",
    "    \"\"\"\n",
    "    Render image under specified light source spd.\n",
    "    \n",
    "    Args:\n",
    "        :img: \n",
    "            | None or str or ndarray with float (max = 1) rgb image.\n",
    "            | None load a default image.\n",
    "        :spd: \n",
    "            | ndarray, optional\n",
    "            | Light source spectrum for rendering\n",
    "            | If None: use CIE illuminant F4\n",
    "        :rfl: \n",
    "            | ndarray, optional\n",
    "            | Reflectance set for color coordinate to rfl mapping.\n",
    "        :out: \n",
    "            | 'img_hyp' or str, optional\n",
    "            |  (other option: 'img_ren': rendered image under :spd:)\n",
    "        :refspd:\n",
    "            | None, optional\n",
    "            | Reference spectrum for color coordinate to rfl mapping.\n",
    "            | None defaults to D65 (srgb has a D65 white point)\n",
    "        :D: \n",
    "            | None, optional\n",
    "            | Degree of (von Kries) adaptation from spd to refspd. \n",
    "        :cieobs:\n",
    "            | _CIEOBS, optional\n",
    "            | CMF set for calculation of xyz from spectral data.\n",
    "        :cspace:\n",
    "            | 'xyz',  optional\n",
    "            | Color space for color coordinate to rfl mapping.\n",
    "            | Tip: Use linear space (e.g. 'xyz', 'Yuv',...) for (interp_type == 'nd'),\n",
    "            |      and perceptually uniform space (e.g. 'ipt') for (interp_type == 'nearest')\n",
    "        :cspace_tf:\n",
    "            | {}, optional\n",
    "            | Dict with parameters for xyz_to_cspace and cspace_to_xyz transform.\n",
    "        :CSF:\n",
    "            | None, optional\n",
    "            | RGB camera response functions.\n",
    "            | If None: input :xyz: contains raw rgb values. Override :cspace:\n",
    "            | argument and perform estimation directly in raw rgb space!!!\n",
    "        :interp_type:\n",
    "            | 'nd', optional\n",
    "            | Options:\n",
    "            | - 'nd': perform n-dimensional linear interpolation using Delaunay triangulation.\n",
    "            | - 'nearest': perform nearest neighbour interpolation. \n",
    "        :k_neighbours:\n",
    "            | 4 or int, optional\n",
    "            | Number of nearest neighbours for reflectance spectrum interpolation.\n",
    "            | Neighbours are found using scipy.spatial.cKDTree\n",
    "        :show: \n",
    "            | True, optional\n",
    "            |  Show images.\n",
    "        :verbosity:\n",
    "            | 0, optional\n",
    "            | If > 0: make a plot of the color coordinates of original and \n",
    "              rendered image pixels.\n",
    "        :show_ref_img:\n",
    "            | True, optional\n",
    "            | True: shows rendered image under reference spd. False: shows\n",
    "            |  original image.\n",
    "        :write_to_file:\n",
    "            | None, optional\n",
    "            | None: do nothing, else: write to filename(+path) in :write_to_file:\n",
    "        :stack_test_ref: \n",
    "            | 12, optional\n",
    "            |   - 12: left (test), right (ref) format for show and imwrite\n",
    "            |   - 21: top (test), bottom (ref)\n",
    "            |   - 1: only show/write test\n",
    "            |   - 2: only show/write ref\n",
    "            |   - 0: show both, write test\n",
    "        :csf_based_rgb_rounding:\n",
    "            | _ROUNDING, optional\n",
    "            | Int representing the number of decimals to round the RGB values (obtained from not-None CSF input) to before applying the search algorithm.\n",
    "            | Smaller values increase the search speed, but could cause fatal error that causes python kernel to die. If this happens increase the rounding int value.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        :returns: \n",
    "            | img_hyp, img_ren, \n",
    "            | ndarrays with float hyperspectral image and rendered images \n",
    "    \"\"\"\n",
    "    \n",
    "    # Get image:\n",
    "    #imread = lambda x: plt.imread(x) #matplotlib.pyplot\n",
    "   \n",
    "    if img is not None:\n",
    "        if isinstance(img,str):\n",
    "            img = plt.imread(img).copy() # use matplotlib.pyplot's imread\n",
    "    else:\n",
    "        img = plt.imread(_HYPSPCIM_DEFAULT_IMAGE).copy()\n",
    "    \n",
    "    if img.dtype == np.uint8: \n",
    "        img = img/255\n",
    "    elif img.dtype == np.uint16:\n",
    "        img = img/(2**16-1)\n",
    "    elif (img.dtype == np.float64) | (img.dtype == np.float32):\n",
    "        pass\n",
    "    else:\n",
    "        raise Exception('img input must be None, string or ndarray of (max = 1) float32 or float64 !')\n",
    "    if img.max() > 1.0: raise Exception('img input must be None, string or ndarray of (max = 1) float32 or float64 !')\n",
    "    \n",
    "    \n",
    "    # Convert to 2D format:\n",
    "    rgb = img.reshape(img.shape[0]*img.shape[1],3) # *1.0: make float\n",
    "    rgb[rgb==0] = _EPS # avoid division by zero for pure blacks.\n",
    "\n",
    "    \n",
    "    # Get unique rgb values and positions:\n",
    "    rgb_u, rgb_indices = np.unique(rgb, return_inverse=True, axis = 0)\n",
    "\n",
    "    \n",
    "    # get rfl set:\n",
    "    if rfl is None: # use IESTM30['4880'] set \n",
    "        rfl = _CRI_RFL['ies-tm30']['4880']['5nm']\n",
    "    wlr = rfl[0] # spectral reflectance set determines wavelength range for estimation (xyz_to_rfl())\n",
    "        \n",
    "    # get Ref spd:\n",
    "    if refspd is None:\n",
    "        refspd = _CIE_ILLUMINANTS['D65'].copy()\n",
    "    refspd = cie_interp(refspd, wlr, kind = 'linear') # force spd to same wavelength range as rfl\n",
    "\n",
    "\n",
    "    # Convert rgb_u to xyz and lab-type values under assumed refspd:\n",
    "    if CSF is None:\n",
    "        xyz_wr = spd_to_xyz(refspd, cieobs = cieobs, relative = True)\n",
    "        xyz_ur = colortf(rgb_u*255, tf = 'srgb>xyz')\n",
    "    else:\n",
    "        xyz_ur = rgb_u # for input in xyz_to_rfl (when CSF is not None: this functions assumes input is indeed rgb !!!)\n",
    "    \n",
    "    # Estimate rfl's for xyz_ur:\n",
    "    rfl_est, xyzri = xyz_to_rfl(xyz_ur, rfl = rfl, out = 'rfl_est,xyz_est', \\\n",
    "                 refspd = refspd, D = D, cieobs = cieobs, \\\n",
    "                 cspace = cspace, cspace_tf = cspace_tf, CSF = CSF,\\\n",
    "                 interp_type = interp_type, k_neighbours = k_neighbours, \n",
    "                 verbosity = verbosity,\n",
    "                 csf_based_rgb_rounding = csf_based_rgb_rounding)\n",
    "\n",
    "    # Get default test spd if none supplied:\n",
    "    if spd is None:\n",
    "        spd = _CIE_ILLUMINANTS['F4']\n",
    "        \n",
    "    if CSF is None:\n",
    "        # calculate xyz values under test spd:\n",
    "        xyzti, xyztw = spd_to_xyz(spd, rfl = rfl_est, cieobs = cieobs, out = 2)\n",
    "    \n",
    "        # Chromatic adaptation from test spd to refspd:\n",
    "        if D is not None:\n",
    "            xyzti = cat.apply(xyzti, xyzw1 = xyztw, xyzw2 = xyz_wr, D = D)\n",
    "    \n",
    "        # Convert xyzti under test spd to srgb:\n",
    "        rgbti = colortf(xyzti, tf = 'srgb')/255\n",
    "    else:\n",
    "        # Calculate rgb coordinates from camera sensitivity functions under spd:\n",
    "        rgbti = rfl_to_rgb(rfl_est, spd = spd, CSF = CSF, wl = None) \n",
    "        \n",
    "         # Chromatic adaptation from test spd to refspd:\n",
    "        if D is not None:\n",
    "            white = np.ones_like(spd)\n",
    "            white[0] = spd[0]\n",
    "            rgbwr = rfl_to_rgb(white, spd = refspd, CSF = CSF, wl = None)\n",
    "            rgbwt = rfl_to_rgb(white, spd = spd, CSF = CSF, wl = None)\n",
    "            rgbti = cat.apply_vonkries2(rgbti,rgbwt,rgbwr,xyzw0=np.array([[1.0,1.0,1.0]]), in_type='rgb',out_type= 'rgb',D=1)\n",
    "        \n",
    "    \n",
    "    # Reconstruct original locations for rendered image rgbs:\n",
    "    img_ren = rgbti[rgb_indices]\n",
    "    img_ren.shape = img.shape # reshape back to 3D size of original\n",
    "    img_ren = img_ren\n",
    "    \n",
    "    # For output:\n",
    "    if show_ref_img == True:\n",
    "        rgb_ref = colortf(xyzri, tf = 'srgb')/255 if (CSF is None) else xyzri # if CSF not None: xyzri contains rgbri !!!\n",
    "        img_ref = rgb_ref[rgb_indices]\n",
    "        img_ref.shape = img.shape # reshape back to 3D size of original\n",
    "        img_str = 'Rendered (under ref. spd)'\n",
    "        img = img_ref\n",
    "    else:\n",
    "        img_str = 'Original'\n",
    "        img = img\n",
    "       \n",
    "    \n",
    "    if (stack_test_ref > 0) | show == True:\n",
    "        if stack_test_ref == 21:\n",
    "            img_original_rendered = np.vstack((img_ren,np.ones((4,img.shape[1],3)),img))\n",
    "            img_original_rendered_str = 'Rendered (under test spd)\\n ' + img_str \n",
    "        elif stack_test_ref == 12:\n",
    "            img_original_rendered = np.hstack((img_ren,np.ones((img.shape[0],4,3)),img))\n",
    "            img_original_rendered_str = 'Rendered (under test spd) | ' + img_str \n",
    "        elif stack_test_ref == 1:\n",
    "            img_original_rendered = img_ren\n",
    "            img_original_rendered_str = 'Rendered (under test spd)' \n",
    "        elif stack_test_ref == 2:\n",
    "            img_original_rendered = img\n",
    "            img_original_rendered_str = img_str\n",
    "        elif stack_test_ref == 0:\n",
    "            img_original_rendered = img_ren\n",
    "            img_original_rendered_str =  'Rendered (under test spd)' \n",
    "            \n",
    "    if write_to_file is not None:\n",
    "        # Convert from RGB to BGR formatand write:\n",
    "        #print('Writing rendering results to image file: {}'.format(write_to_file))\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            imsave(write_to_file, img_original_rendered)\n",
    "            \n",
    "    if show == True:\n",
    "        # show images using pyplot.show():\n",
    "        plt.figure()\n",
    "        \n",
    "        plt.imshow(img_original_rendered)\n",
    "        plt.title(img_original_rendered_str)\n",
    "        plt.gca().get_xaxis().set_ticklabels([])\n",
    "        plt.gca().get_yaxis().set_ticklabels([])\n",
    "        \n",
    "        if stack_test_ref == 0:\n",
    "            plt.figure()\n",
    "            plt.imshow(img)\n",
    "            plt.title(img_str)\n",
    "            plt.axis('off')\n",
    "      \n",
    "    if 'img_hyp' in out.split(','):\n",
    "        # Create hyper_spectral image:\n",
    "        rfl_image_2D = rfl_est[rgb_indices+1,:] # create array with all rfls required for each pixel\n",
    "        img_hyp = rfl_image_2D.reshape(img.shape[0],img.shape[1],rfl_image_2D.shape[1])\n",
    "\n",
    "\n",
    "    # Setup output:\n",
    "    if out == 'img_hyp':\n",
    "        return img_hyp\n",
    "    elif out == 'img_ren':\n",
    "        return img_ren\n",
    "    else:\n",
    "        return eval(out)\n",
    "\n",
    "\n",
    "def rfl_to_rgb(rfl, spd = None, CSF = None, wl = None, normalize_to_white = True):\n",
    "    \"\"\" \n",
    "    Convert spectral reflectance functions (illuminated by spd) to Camera Sensitivity Functions.\n",
    "    \n",
    "    Args:\n",
    "        :rfl:\n",
    "            | ndarray with spectral reflectance functions (1st row is wavelengths if wl is None).\n",
    "        :spd:\n",
    "            | None, optional\n",
    "            | ndarray with illumination spectrum\n",
    "        :CSF:\n",
    "            | None, optional\n",
    "            | ndarray with camera sensitivity functions \n",
    "            | If None: use Nikon D700\n",
    "        :normalize_to_white:\n",
    "            | True, optional\n",
    "            | If True: white-balance output rgb to a perfect white diffuser.\n",
    "    \n",
    "    Returns:\n",
    "        :rgb:\n",
    "            | ndarray with rgb values for each spectral reflectance functions\n",
    "    \"\"\"\n",
    "    rfl_cp = rfl.copy()\n",
    "    if (wl is None): \n",
    "        wl = rfl_cp[0] \n",
    "        rfl_cp = rfl_cp[1:]\n",
    "    wlr = getwlr(wl)\n",
    "    if spd is not None:\n",
    "        spd = cie_interp(spd,wlr,kind='linear')[1:]\n",
    "    else:\n",
    "        spd = np.ones_like(wlr)\n",
    "    if CSF is None: CSF = _CSF_NIKON_D700\n",
    "    CSF = cie_interp(CSF,wlr,kind='linear')\n",
    "    CSF[1:] = CSF[1:]*spd\n",
    "    rgb = rfl_cp @ CSF[1:].T \n",
    "    if normalize_to_white:\n",
    "        white = np.ones_like(spd)\n",
    "        white = white/white.sum()*spd.sum()\n",
    "        rgbw = white @ CSF[1:].T  \n",
    "        rgb = rgb/rgbw.max(axis = 0,keepdims=True) \n",
    "    \n",
    "    return rgb\n",
    "\n",
    "    \n",
    "    \n",
    "def hsi_to_rgb(hsi, spd = None, cieobs = _CIEOBS, srgb = False, \n",
    "               linear_rgb = False, CSF = None, normalize_to_white = True, \n",
    "               wl = [380,780,1]):\n",
    "    \"\"\" \n",
    "    Convert HyperSpectral Image to rgb.\n",
    "    \n",
    "    Args:\n",
    "        :hsi:\n",
    "            | ndarray with hyperspectral image [M,N,L]\n",
    "        :spd:\n",
    "            | None, optional\n",
    "            | ndarray with illumination spectrum\n",
    "        :cieobs:\n",
    "            | _CIEOBS, optional\n",
    "            | CMF set to convert spectral data to xyz tristimulus values.\n",
    "        :srgb:\n",
    "            | False, optional\n",
    "            | If False: Use xyz_to_srgb(spd_to_xyz(...)) to convert to srgb values\n",
    "            | If True: use camera sensitivity functions.\n",
    "        :linear_rgb:\n",
    "            | False, optional\n",
    "            | If False: use gamma = 2.4 in xyz_to_srgb, if False: use gamma = 1 and set :use_linear_part: to False.\n",
    "        :CSF:\n",
    "            | None, optional\n",
    "            | ndarray with camera sensitivity functions \n",
    "            | If None: use Nikon D700\n",
    "        :normalize_to_white:\n",
    "            | True, optional\n",
    "            | If True & CSF is not None: white-balance output rgb to a perfect white diffuser.\n",
    "        :wl:\n",
    "            | [380,780,1], optional\n",
    "            | Wavelength range and spacing or ndarray with wavelengths of HSI image.\n",
    "    \n",
    "    Returns:\n",
    "        :rgb:\n",
    "            | ndarray with rgb image [M,N,3]\n",
    "    \"\"\"\n",
    "    if spd is None:\n",
    "        spd = _CIE_E.copy()\n",
    "    wlr = getwlr(wl)\n",
    "    spd = cie_interp(spd,wl,kind='linear')\n",
    "    \n",
    "    hsi_2d = np.reshape(hsi,(hsi.shape[0]*hsi.shape[1],hsi.shape[2]))\n",
    "    if srgb:\n",
    "        xyz = spd_to_xyz(spd, cieobs = cieobs, relative = True, rfl = np.vstack((wlr,hsi_2d)))\n",
    "        gamma = 1 if linear_rgb else 2.4\n",
    "        rgb = xyz_to_srgb(xyz, gamma = gamma, use_linear_part = not linear_rgb)/255\n",
    "    else:\n",
    "        if CSF is None: CSF = _CSF_NIKON_D700\n",
    "        rgb = rfl_to_rgb(hsi_2d, spd = spd, CSF = CSF, wl = wl, normalize_to_white = normalize_to_white)        \n",
    "    return np.reshape(rgb,(hsi.shape[0],hsi.shape[1],3))\n",
    "\n",
    "       \n",
    "def get_superresolution_hsi(lrhsi, hrci, CSF, wl = [380,780,1], csf_based_rgb_rounding = _ROUNDING,\n",
    "                            interp_type = 'nd', k_neighbours = 4, verbosity = 0):\n",
    "    \"\"\" \n",
    "    Get a HighResolution HyperSpectral Image (super-resolution HSI) based on a LowResolution HSI and a HighResolution Color Image.\n",
    "    \n",
    "    Args:\n",
    "        :lrhsi:\n",
    "            | ndarray with float (max = 1) LowResolution HSI [m,m,L].\n",
    "        :hrci:\n",
    "            | ndarray with float (max = 1) HighResolution HSI [M,N,3].\n",
    "        :CSF:\n",
    "            | None, optional\n",
    "            | ndarray with camera sensitivity functions \n",
    "            | If None: use Nikon D700\n",
    "        :wl:\n",
    "            | [380,780,1], optional\n",
    "            | Wavelength range and spacing or ndarray with wavelengths of HSI image.\n",
    "        :interp_type:\n",
    "            | 'nd', optional\n",
    "            | Options:\n",
    "            | - 'nd': perform n-dimensional linear interpolation using Delaunay triangulation.\n",
    "            | - 'nearest': perform nearest neighbour interpolation. \n",
    "        :k_neighbours:\n",
    "            | 4 or int, optional\n",
    "            | Number of nearest neighbours for reflectance spectrum interpolation.\n",
    "            | Neighbours are found using scipy.spatial.cKDTree\n",
    "        :verbosity:\n",
    "            | 0, optional\n",
    "            | Verbosity level for sub-call to render_image().\n",
    "            | If > 0: make a plot of the color coordinates of original and \n",
    "            | rendered image pixels.\n",
    "        :csf_based_rgb_rounding:\n",
    "            | _ROUNDING, optional\n",
    "            | Int representing the number of decimals to round the RGB values (obtained from not-None CSF input) to before applying the search algorithm.\n",
    "            | Smaller values increase the search speed, but could cause fatal error that causes python kernel to die. If this happens increase the rounding int value.\n",
    "\n",
    "    Returns:\n",
    "        :hrhsi:\n",
    "            | ndarray with HighResolution HSI [M,N,L].\n",
    "        \n",
    "    Procedure:\n",
    "        | Call render_image(hrci, rfl = lrhsi_2, CSF = ...) to estimate a hyperspectral image\n",
    "        | from the high-resolution color image hrci with the reflectance spectra \n",
    "        | in the low-resolution hyper-spectral image as database for the estimation.\n",
    "        | Estimation is done in raw RGB space with the lrhsi converted using the\n",
    "        | camera sensitivity functions in CSF.\n",
    "    \"\"\"\n",
    "    wlr = getwlr(wl)\n",
    "    eew = np.vstack((wlr,np.ones_like(wlr)))\n",
    "    lrhsi_2d = np.vstack((wlr,np.reshape(lrhsi,(lrhsi.shape[0]*lrhsi.shape[1],lrhsi.shape[2])))) # create 2D rfl database\n",
    "    if CSF is None: CSF = _CSF_NIKON_D700\n",
    "    hrhsi = render_image(hrci, spd = eew,\n",
    "                         refspd = eew, rfl = lrhsi_2d, D = None,\n",
    "                         interp_type = interp_type, k_neighbours = k_neighbours,\n",
    "                         verbosity = verbosity, show = bool(verbosity),\n",
    "                         CSF = CSF, csf_based_rgb_rounding = csf_based_rgb_rounding) # render HR-hsi from HR-ci using LR-HSI rfls as database        \n",
    "    return hrhsi\n",
    "\n",
    "\n",
    "def illuminanceEstimation(input_img):\n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    "    # Example / test code for HSI simulation and rendering:\n",
    "    #--------------------------------------------------------------------------\n",
    "    # plt.close('all')\n",
    "    # from luxpy.toolboxes import spdbuild as spb\n",
    "    # S = spb.spd_builder(peakwl = [460,525,590],fwhm=[20,40,20],target=4000, tar_type = 'cct') \n",
    "    # img = _HYPSPCIM_DEFAULT_IMAGE\n",
    "    # img_hyp,img_ren = render_image(img = img, \n",
    "    #                                 cspace = 'Yuv',interp_type='nd',\n",
    "    #                                 spd = S, D=1, \n",
    "    #                                 show_ref_img = True,\n",
    "    #                                 stack_test_ref = 21,\n",
    "    #                                 out='img_hyp,img_ren',\n",
    "    #                                 write_to_file = 'test.jpg') \n",
    "    # raise Exception('')\n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    "    # Example / test code for super resolution:\n",
    "    #--------------------------------------------------------------------------\n",
    "    import time\n",
    "    import luxpy as lx\n",
    "    import matplotlib.pyplot as plt\n",
    "    from skimage import transform\n",
    "    import imageio\n",
    "    from skimage.transform import rescale,resize\n",
    "    \n",
    "    np.random.seed(1)    \n",
    "    \n",
    "    # Set some default parameters:\n",
    "    #----------------------------\n",
    "    load_hsi = False # If True: load hrci and hrhsi from npy-file.\n",
    "    file = input_img\n",
    "\n",
    "    cieobs = '1931_2' # CIE CMF set\n",
    "    linear_rgb = 1 # only used when srgb in hsi_to_rgb == True !!!\n",
    "    verbosity = 0\n",
    "    \n",
    "    # Create HR-rgb image and HR-HSI for code testing: \n",
    "    #---------------------------------------------------\n",
    "    # get an image:\n",
    "    im = imageio.v2.imread(file)/255\n",
    "    \n",
    "    # rescale to n x dimensions of typical hyperspectral camera:\n",
    "    n = 2 # downscale factor\n",
    "    w, h = 1280, 960\n",
    "    cr,cc = np.array(im.shape[:2])//2\n",
    "    crop = lambda im,cr,cc,h,w:im[(cr-h//2):(cr+h//2),(cc-w//2):(cc+w//2),:].copy()\n",
    "    im = crop(im,cr,cc,h*n,w*n)\n",
    "#     print('New image shape:',im.shape)\n",
    "    \n",
    "    # simulate HR hyperspectral image:\n",
    "    hrhsi = render_image(im,show=False)\n",
    "    wlr = getwlr([380,780,1]) #  = wavelength range of default TM30 rfl set\n",
    "    wlr = wlr[20:-80:10] # wavelength range from 400nm-700nm every 10 nm\n",
    "    hrhsi = hrhsi[...,20:-80:10] # wavelength range from 400nm-700nm every 10 nm\n",
    "#     print('Simulated HR-HSI shape:',hrhsi.shape)\n",
    "    # np.save(file[:-4]+'.npy',{'hrhsi':hrhsi,'im':im, 'wlr':wlr})\n",
    "    \n",
    "    # Illumination spectrum of HSI:    \n",
    "    eew = np.vstack((wlr,np.ones_like(wlr)))\n",
    "        \n",
    "    # Create fig and axes for plots:\n",
    "    if verbosity > 0: fig, axs = plt.subplots(1,3)\n",
    "    \n",
    "    # convert HR hsi to HR rgb image:\n",
    "    hrci = hsi_to_rgb(hrhsi, spd = eew, cieobs = cieobs, wl = wlr, linear_rgb = linear_rgb)\n",
    "    if verbosity > 0:  axs[0].imshow(hrci)\n",
    "    \n",
    "    # create LR hsi image for testing:\n",
    "    dl = n \n",
    "    lrhsi = hrhsi[::dl,::dl,:]\n",
    "#     print('Simulated LR-HSI shape:',lrhsi.shape)\n",
    "    \n",
    "    # convert LR hsi to LR rgb image:\n",
    "    lrci = hsi_to_rgb(lrhsi, spd = eew, cieobs = cieobs, wl = wlr,linear_rgb = linear_rgb)\n",
    "    if verbosity > 0:  axs[1].imshow(lrci)\n",
    "    \n",
    "    # # Perform rgb guided super-resolution:\n",
    "    #hrci = lrci # for testing of estimation code\n",
    "    tic = time.time()\n",
    "    hrhsi_est = get_superresolution_hsi(lrhsi, hrci, CSF = _CSF_NIKON_D700, wl = wlr)\n",
    "#     print('Elapsed time (s): {:1.4f}'.format(time.time() - tic))\n",
    "    hrci_est = hsi_to_rgb(hrhsi_est, spd = eew, cieobs = cieobs, wl = wlr, linear_rgb = linear_rgb)\n",
    "\n",
    "    if verbosity > 0:  axs[2].imshow(hrci_est)\n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    "    # Plot some rfl to visually evaluate estimation accuracy:\n",
    "    \n",
    "    hsi_rmse = np.linalg.norm(hrhsi-hrhsi_est)/np.array(hrhsi.shape[:2]).prod()**0.5\n",
    "#     print('RMSE(ground-truth,estimate): {:1.4f}'.format(hsi_rmse))\n",
    "    \n",
    "    global illum_out\n",
    "    return illum_out\n",
    "    \n",
    "#     fig, axs = plt.subplots(1,4, figsize=(22,5))\n",
    "    \n",
    "#     axs[0].imshow(transform.rescale(lrci,dl,order=0,multichannel=True),aspect='auto')\n",
    "#     axs[0].set_title('Color image of LR-HSI\\n(HR-to-LR scale factor = {:1.2f})'.format(1/dl))\n",
    "#     axs[0].axis('off')\n",
    "#     axs[1].imshow(hrci_est,aspect='auto')\n",
    "#     axs[1].set_title('Color image of estimated HR-HSI')\n",
    "#     axs[1].axis('off')\n",
    "    \n",
    "#     px_rmse = ((hrhsi_est-hrhsi)**2).sum(axis=-1)**0.5 # rmse per pixel\n",
    "#     axs[2].set_title('RMSE(ground-truth, estimated) HR-HSI\\nRMSE = {:1.4f}, max = {:1.4f}'.format((px_rmse**2).mean()**0.5,px_rmse.max()))\n",
    "#     im = axs[2].imshow(px_rmse, cmap = 'jet',aspect='auto') # rmse per pixel\n",
    "#     cbar = axs[2].figure.colorbar(im, ax=axs[2])\n",
    "#     cbar.ax.set_ylabel('RMSE', rotation=-90, va=\"bottom\")\n",
    "    \n",
    "    \n",
    "#     psorted = np.unravel_index(np.argsort(px_rmse, axis=None), px_rmse.shape) # index of pixels sorted by px_rmse\n",
    "#     np.random.seed(1)\n",
    "#     pxs = np.random.permutation(min(hrhsi.shape[:2]))[:12].reshape(2,3,2)\n",
    "#     iis = np.hstack((pxs[...,0].ravel(),psorted[0][-3:]))\n",
    "#     jjs = np.hstack((pxs[...,1].ravel(),psorted[1][-3:]))\n",
    "#     colors = np.array(['m','b','c','g','y','r','k','lightgrey','grey'])\n",
    "#     for t in range(len(iis)):\n",
    "#         ii,jj = iis[t],jjs[t]\n",
    "#         axs[1].plot(jj,ii,color = 'none', marker = 'o', mec = colors[t])\n",
    "#         axs[2].plot(jj,ii,color = 'none', marker = 'o', mec = colors[t])\n",
    "#         axs[3].plot(wlr,hrhsi[ii,jj,:],color = colors[t], linestyle ='-',label='ground-truth (r{:1.0f},c{:1.0f})'.format(ii,jj))\n",
    "#         axs[3].plot(wlr,hrhsi_est[ii,jj,:],color = colors[t], linestyle = '--',label='estimate (r{:1.0f},c{:1.0f})'.format(ii,jj))\n",
    "#     axs[3].legend(bbox_to_anchor=(1.05, 1))   \n",
    "#     axs[3].set_xlabel('Wavelengths (nm)')\n",
    "#     axs[3].set_ylabel('Spectral Reflectance')\n",
    "#     plt.subplots_adjust(right=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb1ae358",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T08:36:10.152132Z",
     "start_time": "2023-10-31T08:36:10.145670Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def illumEstOneFolder_prep():\n",
    "    images = Path(r\"data\\Training\\001_nonsleepyCombination\") #imgs\n",
    "    landmarks = []\n",
    "    fileNames = []\n",
    "    illums = []\n",
    "    \n",
    "    for img_file in images.glob(\"*.jpg\"):\n",
    "\n",
    "        print(os.path.basename(img_file)[:-4])\n",
    "        fileNames.append(os.path.basename(img_file)[:-4])\n",
    "        landmarks.append(generateLandmarks(os.path.dirname(img_file) + \"\\\\\" + os.path.basename(img_file)))\n",
    "        illums.append(illuminanceEstimation(img_file))\n",
    "        retinexImplement(img_file)\n",
    "    \n",
    "    ilu_dis = pd.DataFrame({'File Names': fileNames, 'Landmark Confidence': landmarks, 'Illuminance': illums})\n",
    "    \n",
    "    print(ilu_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b6f26ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T08:36:10.353144Z",
     "start_time": "2023-10-31T08:36:10.348611Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def generateLandmarks(img):\n",
    "    img = cv2.imread(img)\n",
    "    detector = MTCNN()\n",
    "    output = detector.detect_faces(img)\n",
    "#     print(output[0]['confidence'])\n",
    "    \n",
    "    return output[0]['keypoints']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6a15479",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T02:54:44.182865Z",
     "start_time": "2023-10-26T02:54:44.180882Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# notes:\n",
    "\n",
    "# average 2.67s per image for generating landmarks and illuminance estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f9a8d5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T08:36:16.608395Z",
     "start_time": "2023-10-31T08:36:12.805772Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import URetinexNet\n",
    "from URetinex_Net import *\n",
    "\n",
    "# Import Arch_network\n",
    "import sys\n",
    "sys.path.append(r\"URetinex_Net\")\n",
    "\n",
    "# testing of URetinex.Net\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from network.Math_Module import P, Q\n",
    "from network.decom import Decom\n",
    "import os\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import time\n",
    "from utils import *\n",
    "\n",
    "def one2three(x):\n",
    "    return torch.cat([x, x, x], dim=1).to(x)\n",
    "\n",
    "class Inference(nn.Module):\n",
    "    #Class Inference Methods\n",
    "    def __init__(self, opts):\n",
    "        super().__init__()\n",
    "        self.opts = opts\n",
    "        # loading decomposition model \n",
    "        self.model_Decom_low = Decom()\n",
    "        self.model_Decom_low = load_initialize(self.model_Decom_low, self.opts.Decom_model_low_path)\n",
    "        # loading R; old_model_opts; and L model\n",
    "        self.unfolding_opts, self.model_R, self.model_L= load_unfolding(self.opts.unfolding_model_path)\n",
    "        # loading adjustment model\n",
    "        self.adjust_model = load_adjustment(self.opts.adjust_model_path)\n",
    "        self.P = P()\n",
    "        self.Q = Q()\n",
    "\n",
    "        transform = [\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "        self.transform = transforms.Compose(transform)\n",
    "        print(self.model_Decom_low)\n",
    "        print(self.model_R)\n",
    "        print(self.model_L)\n",
    "        print(self.adjust_model)\n",
    "        #time.sleep(8)\n",
    "\n",
    "    def unfolding(self, input_low_img):\n",
    "        for t in range(self.unfolding_opts.round):      \n",
    "            if t == 0: # initialize R0, L0\n",
    "                P, Q = self.model_Decom_low(input_low_img)\n",
    "            else: # update P and Q\n",
    "                w_p = (self.unfolding_opts.gamma + self.unfolding_opts.Roffset * t)\n",
    "                w_q = (self.unfolding_opts.lamda + self.unfolding_opts.Loffset * t)\n",
    "                P = self.P(I=input_low_img, Q=Q, R=R, gamma=w_p)\n",
    "                Q = self.Q(I=input_low_img, P=P, L=L, lamda=w_q) \n",
    "            R = self.model_R(r=P, l=Q)\n",
    "            L = self.model_L(l=Q)\n",
    "        return R, L\n",
    "    \n",
    "    def lllumination_adjust(self, L, ratio):\n",
    "        ratio = torch.ones(L.shape) * self.opts.ratio\n",
    "        return self.adjust_model(l=L, alpha=ratio)\n",
    "    \n",
    "    def forward(self, input_low_img):\n",
    "        if torch.cuda.is_available():\n",
    "            input_low_img = input_low_img\n",
    "        with torch.no_grad():\n",
    "            start = time.time()  \n",
    "            R, L = self.unfolding(input_low_img)\n",
    "            High_L = self.lllumination_adjust(L, self.opts.ratio)\n",
    "            I_enhance = High_L * R\n",
    "            p_time = (time.time() - start)\n",
    "        return I_enhance, p_time\n",
    "\n",
    "    def run(self, low_img_path):\n",
    "        file_name = os.path.basename(self.opts.img_path)\n",
    "        name = file_name.split('.')[0]\n",
    "        low_img = self.transform(Image.open(low_img_path)).unsqueeze(0)\n",
    "        enhance, p_time = self.forward(input_low_img=low_img)\n",
    "        if not os.path.exists(self.opts.output):\n",
    "            os.makedirs(self.opts.output)\n",
    "        save_path = os.path.join(self.opts.output, file_name.replace(name, \"%s_URetinexNet\"%(name)))\n",
    "        np_save_TensorImg(enhance, save_path)  \n",
    "        print(\"================================= time for %s: %f============================\"%(file_name, p_time))\n",
    "\n",
    "#         add to own function for input of img path\n",
    "def retinexImplement(img, outPath):\n",
    "    parser = argparse.ArgumentParser(description='Configure')\n",
    "    \n",
    "    # specify your data path here!\n",
    "    parser.add_argument('--img_path', type=str, default=img)\n",
    "    parser.add_argument('--output', type=str, default=outPath)\n",
    "    # ratio are recommended to be 3-5, bigger ratio will lead to over-exposure \n",
    "    parser.add_argument('--ratio', type=int, default=2)\n",
    "    # model path\n",
    "    parser.add_argument('--Decom_model_low_path', type=str, default=\"./URetinex_Net/ckpt/init_low.pth\")\n",
    "    parser.add_argument('--unfolding_model_path', type=str, default=\"./URetinex_Net/ckpt/unfolding.pth\")\n",
    "    parser.add_argument('--adjust_model_path', type=str, default=\"./URetinex_Net/ckpt/L_adjust.pth\")\n",
    "    parser.add_argument('--gpu_id', type=int, default=0)\n",
    "    \n",
    "#     opts = parser.parse_args() change parse_args() to parse_known_args\n",
    "    opts, _ = parser.parse_known_args()\n",
    "    for k, v in vars(opts).items():\n",
    "        print(k, v)\n",
    "    \n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(opts.gpu_id)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = Inference(opts)\n",
    "        print(\"CUDA (GPU) is available\")\n",
    "    else:\n",
    "        print(\"CUDA (GPU) is not available. Loading the model on CPU...\")\n",
    "        model = Inference(opts).to(torch.device('cpu'))\n",
    "    \n",
    "#    \n",
    "    model.run(opts.img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "679d8739",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T02:54:46.868723Z",
     "start_time": "2023-10-26T02:54:46.866229Z"
    }
   },
   "outputs": [],
   "source": [
    "# evalData_prep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e4da41d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T08:40:10.299901Z",
     "start_time": "2023-10-31T08:36:17.091086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_0_0.jpg\n",
      "1/1 [==============================] - 0s 293ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\michael\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Illuminance: 0.29567792373263885\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_0_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 2\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "================================= time for 001_nonsleepyCombination_0_0.jpg: 4.698539============================\n",
      "Illuminance: 0.45679098484374997\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001DA8A4E2D40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_15_0.jpg\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001DA8CCA20C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 187ms/step\n",
      "Illuminance: 0.2929762817534723\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_15_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 2\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================= time for 001_nonsleepyCombination_15_0.jpg: 4.383471============================\n",
      "Illuminance: 0.45493657617621525\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_30_0.jpg\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "Illuminance: 0.29606498448350704\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_30_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 2\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "================================= time for 001_nonsleepyCombination_30_0.jpg: 4.468406============================\n",
      "Illuminance: 0.45702348750868055\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 210ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_45_0.jpg\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "Illuminance: 0.2991066410894097\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_45_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 2\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================= time for 001_nonsleepyCombination_45_0.jpg: 4.504473============================\n",
      "Illuminance: 0.45941350959201405\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_60_0.jpg\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "Illuminance: 0.29778815756076393\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_60_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 2\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "================================= time for 001_nonsleepyCombination_60_0.jpg: 4.628499============================\n",
      "Illuminance: 0.45841267974826383\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_75_0.jpg\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "Illuminance: 0.29540649650173617\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_75_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 2\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================= time for 001_nonsleepyCombination_75_0.jpg: 4.258220============================\n",
      "Illuminance: 0.45675732172309014\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_90_0.jpg\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "Illuminance: 0.2979124167057292\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_90_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 2\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "================================= time for 001_nonsleepyCombination_90_0.jpg: 4.354694============================\n",
      "Illuminance: 0.45879753255208333\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_105_0.jpg\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "Illuminance: 0.29994796240885413\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_105_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 2\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================= time for 001_nonsleepyCombination_105_0.jpg: 4.235244============================\n",
      "Illuminance: 0.46005176036458334\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_120_0.jpg\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "Illuminance: 0.29637356141927085\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_120_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 2\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "================================= time for 001_nonsleepyCombination_120_0.jpg: 5.397648============================\n",
      "Illuminance: 0.45746501846354154\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 245ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_135_0.jpg\n",
      "1/1 [==============================] - 0s 223ms/step\n",
      "1/1 [==============================] - 0s 263ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "1/1 [==============================] - 0s 238ms/step\n",
      "Illuminance: 0.2969686096050347\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_135_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 2\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================= time for 001_nonsleepyCombination_135_0.jpg: 4.409904============================\n",
      "Illuminance: 0.45746883875434036\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_150_0.jpg\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "Illuminance: 0.2955887041015627\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_150_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 2\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "================================= time for 001_nonsleepyCombination_150_0.jpg: 4.256275============================\n",
      "Illuminance: 0.45666625572048614\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_165_0.jpg\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "Illuminance: 0.2953554853255209\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_165_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 2\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================= time for 001_nonsleepyCombination_165_0.jpg: 3.911584============================\n",
      "Illuminance: 0.4563671923090278\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_180_0.jpg\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "Illuminance: 0.29572583037326383\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_180_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 2\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "================================= time for 001_nonsleepyCombination_180_0.jpg: 3.949409============================\n",
      "Illuminance: 0.45686267787760415\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "Creating..../data/Training/001_nonsleepyCombination/001_nonsleepyCombination_195_0.jpg\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "Illuminance: 0.2982099434548611\n",
      "img_path data\\Training\\001_nonsleepyCombination\\001_nonsleepyCombination_195_0.jpg\n",
      "output ./data/RetTraining/001_nonsleepyCombination/\n",
      "ratio 2\n",
      "Decom_model_low_path ./URetinex_Net/ckpt/init_low.pth\n",
      "unfolding_model_path ./URetinex_Net/ckpt/unfolding.pth\n",
      "adjust_model_path ./URetinex_Net/ckpt/L_adjust.pth\n",
      "gpu_id 0\n",
      "CUDA (GPU) is not available. Loading the model on CPU...\n",
      " ===========>  loading pretrained Illumination Adjustment Model from: ./URetinex_Net/ckpt/L_adjust.pth \n",
      "Decom(\n",
      "  (decom): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "  )\n",
      ")\n",
      "HalfDnCNNSE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (se_layer): SELayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (conv6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Illumination_Alone(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu_1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (leaky_relu_4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Adjust_naive(\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================= time for 001_nonsleepyCombination_195_0.jpg: 4.262244============================\n",
      "Illuminance: 0.4581530092925347\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainingData_prep()\n",
      "Cell \u001b[1;32mIn[5], line 57\u001b[0m, in \u001b[0;36mtrainingData_prep\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(videos_file)\n\u001b[0;32m     56\u001b[0m training_video_paths\u001b[38;5;241m.\u001b[39mappend(video_path)\n\u001b[1;32m---> 57\u001b[0m frame_capture(video_path)\n",
      "Cell \u001b[1;32mIn[4], line 100\u001b[0m, in \u001b[0;36mframe_capture\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m     97\u001b[0m     retSave_path \u001b[38;5;241m=\u001b[39m retSave_path \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(img_file)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_URetinexNet.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m     trainIllumsRet\u001b[38;5;241m.\u001b[39mappend(illuminanceEstimation(retSave_path))\n\u001b[1;32m--> 100\u001b[0m     trainLandmarksRet\u001b[38;5;241m.\u001b[39mappend(generateLandmarks(retSave_path))\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m save_path \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/Evaluation/\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    105\u001b[0m     evalFileNames\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(img_file)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m])\n",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m, in \u001b[0;36mgenerateLandmarks\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m      2\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(img)\n\u001b[0;32m      3\u001b[0m     detector \u001b[38;5;241m=\u001b[39m MTCNN()\n\u001b[1;32m----> 4\u001b[0m     output \u001b[38;5;241m=\u001b[39m detector\u001b[38;5;241m.\u001b[39mdetect_faces(img)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#     print(output[0]['confidence'])\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\mtcnn\\mtcnn.py:300\u001b[0m, in \u001b[0;36mMTCNN.detect_faces\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;66;03m# We pipe here each of the stages\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stage \u001b[38;5;129;01min\u001b[39;00m stages:\n\u001b[1;32m--> 300\u001b[0m     result \u001b[38;5;241m=\u001b[39m stage(img, result[\u001b[38;5;241m0\u001b[39m], result[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    302\u001b[0m [total_boxes, points] \u001b[38;5;241m=\u001b[39m result\n\u001b[0;32m    304\u001b[0m bounding_boxes \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\mtcnn\\mtcnn.py:342\u001b[0m, in \u001b[0;36mMTCNN.__stage1\u001b[1;34m(self, image, scales, stage_status)\u001b[0m\n\u001b[0;32m    339\u001b[0m img_x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(scaled_image, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    340\u001b[0m img_y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(img_x, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m--> 342\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pnet\u001b[38;5;241m.\u001b[39mpredict(img_y)\n\u001b[0;32m    344\u001b[0m out0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(out[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m    345\u001b[0m out1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(out[\u001b[38;5;241m1\u001b[39m], (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:2521\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2512\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m   2513\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2514\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing Model.predict with MultiWorkerMirroredStrategy \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2515\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor TPUStrategy and AutoShardPolicy.FILE might lead to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2518\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   2519\u001b[0m         )\n\u001b[1;32m-> 2521\u001b[0m data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[0;32m   2522\u001b[0m     x\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m   2523\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2524\u001b[0m     steps_per_epoch\u001b[38;5;241m=\u001b[39msteps,\n\u001b[0;32m   2525\u001b[0m     initial_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2526\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   2527\u001b[0m     max_queue_size\u001b[38;5;241m=\u001b[39mmax_queue_size,\n\u001b[0;32m   2528\u001b[0m     workers\u001b[38;5;241m=\u001b[39mworkers,\n\u001b[0;32m   2529\u001b[0m     use_multiprocessing\u001b[38;5;241m=\u001b[39muse_multiprocessing,\n\u001b[0;32m   2530\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2531\u001b[0m     steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution,\n\u001b[0;32m   2532\u001b[0m )\n\u001b[0;32m   2534\u001b[0m \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   2535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1678\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1676\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorExactEvalDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1677\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1285\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[0;32m   1282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution \u001b[38;5;241m=\u001b[39m steps_per_execution\n\u001b[0;32m   1284\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1285\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m adapter_cls(\n\u001b[0;32m   1286\u001b[0m     x,\n\u001b[0;32m   1287\u001b[0m     y,\n\u001b[0;32m   1288\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1289\u001b[0m     steps\u001b[38;5;241m=\u001b[39msteps_per_epoch,\n\u001b[0;32m   1290\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepochs \u001b[38;5;241m-\u001b[39m initial_epoch,\n\u001b[0;32m   1291\u001b[0m     sample_weights\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1292\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39mshuffle,\n\u001b[0;32m   1293\u001b[0m     max_queue_size\u001b[38;5;241m=\u001b[39mmax_queue_size,\n\u001b[0;32m   1294\u001b[0m     workers\u001b[38;5;241m=\u001b[39mworkers,\n\u001b[0;32m   1295\u001b[0m     use_multiprocessing\u001b[38;5;241m=\u001b[39muse_multiprocessing,\n\u001b[0;32m   1296\u001b[0m     distribution_strategy\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy(),\n\u001b[0;32m   1297\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   1298\u001b[0m     pss_evaluation_shards\u001b[38;5;241m=\u001b[39mpss_evaluation_shards,\n\u001b[0;32m   1299\u001b[0m )\n\u001b[0;32m   1301\u001b[0m strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[0;32m   1303\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:355\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m flat_dataset\n\u001b[0;32m    353\u001b[0m indices_dataset \u001b[38;5;241m=\u001b[39m indices_dataset\u001b[38;5;241m.\u001b[39mflat_map(slice_batch_indices)\n\u001b[1;32m--> 355\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslice_inputs(indices_dataset, inputs)\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshuffle_batch\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:396\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.slice_inputs\u001b[1;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrab_batch\u001b[39m(i, data):\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m d: tf\u001b[38;5;241m.\u001b[39mgather(d, i, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), data\n\u001b[0;32m    394\u001b[0m     )\n\u001b[1;32m--> 396\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(grab_batch, num_parallel_calls\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n\u001b[0;32m    398\u001b[0m \u001b[38;5;66;03m# Default optimizations are disabled to avoid the overhead of\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;66;03m# (unnecessary) input pipeline graph serialization and deserialization\u001b[39;00m\n\u001b[0;32m    400\u001b[0m options \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mOptions()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2278\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[1;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[0;32m   2274\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops -> map_op ->\u001b[39;00m\n\u001b[0;32m   2275\u001b[0m \u001b[38;5;66;03m# dataset_ops).\u001b[39;00m\n\u001b[0;32m   2276\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[0;32m   2277\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m map_op\n\u001b[1;32m-> 2278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m map_op\u001b[38;5;241m.\u001b[39m_map_v2(\n\u001b[0;32m   2279\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2280\u001b[0m     map_func,\n\u001b[0;32m   2281\u001b[0m     num_parallel_calls\u001b[38;5;241m=\u001b[39mnum_parallel_calls,\n\u001b[0;32m   2282\u001b[0m     deterministic\u001b[38;5;241m=\u001b[39mdeterministic,\n\u001b[0;32m   2283\u001b[0m     name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\map_op.py:40\u001b[0m, in \u001b[0;36m_map_v2\u001b[1;34m(input_dataset, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[0;32m     37\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _MapDataset(\n\u001b[0;32m     38\u001b[0m       input_dataset, map_func, preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 40\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _ParallelMapDataset(\n\u001b[0;32m     41\u001b[0m       input_dataset,\n\u001b[0;32m     42\u001b[0m       map_func,\n\u001b[0;32m     43\u001b[0m       num_parallel_calls\u001b[38;5;241m=\u001b[39mnum_parallel_calls,\n\u001b[0;32m     44\u001b[0m       deterministic\u001b[38;5;241m=\u001b[39mdeterministic,\n\u001b[0;32m     45\u001b[0m       preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     46\u001b[0m       name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\map_op.py:163\u001b[0m, in \u001b[0;36m_ParallelMapDataset.__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_parallel_calls \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\n\u001b[0;32m    161\u001b[0m     num_parallel_calls, dtype\u001b[38;5;241m=\u001b[39mdtypes\u001b[38;5;241m.\u001b[39mint64, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_parallel_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m--> 163\u001b[0m variant_tensor \u001b[38;5;241m=\u001b[39m gen_dataset_ops\u001b[38;5;241m.\u001b[39mparallel_map_dataset_v2(\n\u001b[0;32m    164\u001b[0m     input_dataset\u001b[38;5;241m.\u001b[39m_variant_tensor,  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs,\n\u001b[0;32m    166\u001b[0m     f\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[0;32m    167\u001b[0m     num_parallel_calls\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_parallel_calls,\n\u001b[0;32m    168\u001b[0m     deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deterministic,\n\u001b[0;32m    169\u001b[0m     use_inter_op_parallelism\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_inter_op_parallelism,\n\u001b[0;32m    170\u001b[0m     preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preserve_cardinality,\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_common_args)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(input_dataset, variant_tensor)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:5957\u001b[0m, in \u001b[0;36mparallel_map_dataset_v2\u001b[1;34m(input_dataset, other_arguments, num_parallel_calls, f, output_types, output_shapes, use_inter_op_parallelism, deterministic, preserve_cardinality, metadata, name)\u001b[0m\n\u001b[0;32m   5955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   5956\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 5957\u001b[0m     _result \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_FastPathExecute(\n\u001b[0;32m   5958\u001b[0m       _ctx, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParallelMapDatasetV2\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, input_dataset, other_arguments,\n\u001b[0;32m   5959\u001b[0m       num_parallel_calls, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m, f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_types\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_types,\n\u001b[0;32m   5960\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_shapes\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_shapes, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_inter_op_parallelism\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5961\u001b[0m       use_inter_op_parallelism, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeterministic\u001b[39m\u001b[38;5;124m\"\u001b[39m, deterministic,\n\u001b[0;32m   5962\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreserve_cardinality\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_cardinality, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, metadata)\n\u001b[0;32m   5963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   5964\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainingData_prep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8df106c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T16:17:10.623362Z",
     "start_time": "2023-10-30T16:17:10.620355Z"
    }
   },
   "outputs": [],
   "source": [
    "# requires images converted to feature array\n",
    "\n",
    "# base_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "#               loss=tf.keras.losses.MeanSquaredError(),\n",
    "#               metrics=[tf.keras.metrics.Accuracy(),\n",
    "#                        tf.keras.metrics.Precision(),\n",
    "#                        tf.keras.metrics.Recall(),\n",
    "#                        tf.keras.metrics.F1Score()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8e4c5e20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T11:06:26.951033Z",
     "start_time": "2023-10-31T11:06:26.245923Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_121\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_125 (InputLayer)      [(None, 224, 224, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)         1792      ['input_125[0][0]']           \n",
      "                                                                                                  \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)         36928     ['block1_conv1[0][0]']        \n",
      "                                                                                                  \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)         0         ['block1_conv2[0][0]']        \n",
      "                                                                                                  \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)        73856     ['block1_pool[0][0]']         \n",
      "                                                                                                  \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)        147584    ['block2_conv1[0][0]']        \n",
      "                                                                                                  \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)          0         ['block2_conv2[0][0]']        \n",
      "                                                                                                  \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)          295168    ['block2_pool[0][0]']         \n",
      "                                                                                                  \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)          590080    ['block3_conv1[0][0]']        \n",
      "                                                                                                  \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)          590080    ['block3_conv2[0][0]']        \n",
      "                                                                                                  \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)          0         ['block3_conv3[0][0]']        \n",
      "                                                                                                  \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)          1180160   ['block3_pool[0][0]']         \n",
      "                                                                                                  \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)          2359808   ['block4_conv1[0][0]']        \n",
      "                                                                                                  \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)          2359808   ['block4_conv2[0][0]']        \n",
      "                                                                                                  \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)          0         ['block4_conv3[0][0]']        \n",
      "                                                                                                  \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)          2359808   ['block4_pool[0][0]']         \n",
      "                                                                                                  \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)          2359808   ['block5_conv1[0][0]']        \n",
      "                                                                                                  \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)          2359808   ['block5_conv2[0][0]']        \n",
      "                                                                                                  \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)            0         ['block5_conv3[0][0]']        \n",
      "                                                                                                  \n",
      " flattened_features (Flatte  (None, 25088)                0         ['block5_pool[0][0]']         \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " reshape_22 (Reshape)        (None, 7, 7, 512)            0         ['flattened_features[0][0]']  \n",
      "                                                                                                  \n",
      " additional_dense1 (Dense)   (None, 64)                   1605696   ['flattened_features[0][0]']  \n",
      "                                                                                                  \n",
      " up_sampling2d_62 (UpSampli  (None, 56, 56, 512)          0         ['reshape_22[0][0]']          \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " additional_dense2 (Dense)   (None, 64)                   4160      ['additional_dense1[0][0]']   \n",
      "                                                                                                  \n",
      " up_sampling2d_63 (UpSampli  (None, 224, 224, 512)        0         ['up_sampling2d_62[0][0]']    \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " landmark_output (Dense)     (None, 10)                   650       ['additional_dense1[0][0]']   \n",
      "                                                                                                  \n",
      " previous_illuminance_outpu  (None, 1)                    65        ['additional_dense2[0][0]']   \n",
      " t (Dense)                                                                                        \n",
      "                                                                                                  \n",
      " image_retinex_output (Dens  (None, 224, 224, 3)          1539      ['up_sampling2d_63[0][0]']    \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 16326798 (62.28 MB)\n",
      "Trainable params: 16326798 (62.28 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = tf.keras.applications.VGG16(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "\n",
    "flattened_features = tf.keras.layers.Flatten(name='flattened_features')(base_model.output)\n",
    "\n",
    "additional_dense_layer1 = tf.keras.layers.Dense(64, activation='relu', name='additional_dense1')(flattened_features)\n",
    "additional_dense_layer2 = tf.keras.layers.Dense(64, activation='relu', name='additional_dense2')(additional_dense_layer1)\n",
    "# additional_dense_layer3 = tf.keras.layers.Dense(64, activation='relu', name='additional_dense3')(flattened_features)\n",
    "\n",
    "landmarks = tf.keras.layers.Dense(10, activation='relu', name='landmark_output')(additional_dense_layer1)\n",
    "illum = tf.keras.layers.Dense(1, activation='relu', name='previous_illuminance_output')(additional_dense_layer2)\n",
    "\n",
    "# Reshape layer to the desired shape\n",
    "reshaped_features = tf.keras.layers.Reshape((7, 7, 512))(flattened_features)\n",
    "\n",
    "# Upsampling layers\n",
    "upsample1 = tf.keras.layers.UpSampling2D(size=(8, 8))(reshaped_features)\n",
    "upsample2 = tf.keras.layers.UpSampling2D(size=(4, 4))(upsample1)\n",
    "upsample3 = tf.keras.layers.UpSampling2D(size=(2, 2))(upsample2)\n",
    "\n",
    "retIllum = tf.keras.layers.Dense(3, activation='relu', name='image_retinex_output')(upsample2)\n",
    "\n",
    "task_outputs = None\n",
    "\n",
    "task_outputs = [landmarks, illum, retIllum]\n",
    "\n",
    "multi_task_model = tf.keras.Model(inputs=base_model.input, outputs=task_outputs)\n",
    "\n",
    "# Compile the model with specific loss functions and metrics for each task\n",
    "multi_task_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={\n",
    "        'landmark_output': 'mean_squared_error',\n",
    "        'previous_illuminance_output': 'mean_squared_error',\n",
    "        'illuminance_retinex_output': 'mean_squared_error'\n",
    "    },\n",
    "    metrics={\n",
    "        'landmark_output': ['mse', \"accuracy\"],\n",
    "        'previous_illuminance_output': ['mse', \"accuracy\"],\n",
    "        'image_retinex_output': ['mse', \"accuracy\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "# Summary of the multi-task model\n",
    "multi_task_model.summary()\n",
    "# print(task_outputs)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3d821b23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T10:46:34.826094Z",
     "start_time": "2023-10-31T10:46:34.511620Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzUAAAj7CAYAAABgCfTSAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nOzdb2hdZ34n8N+J/3QpncnQSeJOd5JAcDOEhXhKYLD7Im7iwNKwx/smlv8kyjDUcWV2AmmdF8Vc4RcJpLD2jl8MWEhblhAaWVJe6ZKkC5aH5kV8+yJBemGK02mw1HG393ZgpQksTNLk7AvPudGVrqQrWdLVY30+cEl07nPO+T3PUaLz1X2eo6woiiIAAADSNHZPtysAAAC4E0INAACQNKEGAABImlADAAAkbWe3CwAirl27Fv/jf/yPbpcBW85f/MVfxIEDB7pdBgBbnE9qYAv453/+53jnnXe6Xca2VKvVolardbsM2njnnXfin//5n7tdBgAJ8EkNbCFjY2PdLmHbOXLkSEQY+60oy7JulwBAInxSAwAAJE2oAQAAkibUAAAASRNqAACApAk1AABA0oQaAAAgaUINAACQNKEGAABImlADAAAkTagBAACSJtQAAABJE2oAAICkCTUAAEDShBoAACBpQg0kqr+/P/r7+7tdBgBA1wk1wJrMzc1FlmVr2q9Wq8XQ0FAcPny4bZuZmZk4ffp0ZFkWp0+fjqtXr67pOBshy7K2r822cPy3Sl0A0A07u10AsDavvfZaV8//wQcfrGm/8+fPR0TE66+/3vb9ubm5mJqaikuXLsVf/dVfxfvvvx+HDh2K8fHxyPO84+NslKIoYm5uLr71rW9FRMTs7Gzce++9m1pDxOLxL4oiGo1G7Nmzp6t1AUA3ZEVRFN0uAra70dHROHr0aKTyn+Pc3Fz09vZGtVpdc83lpwgL969Wqy3hZbm2K73XiSNHjkRExNjY2Kr2u9Pz3onlxr+bda23LMtiZGQkenp6ul0KAFvbmOlnkKBGoxGXL19umXa1cFu1Wo0sy+Lw4cMxMzPTbFOtVptthoaGmlO8Pvnkk4iItlOXFm47f/58VKvVlvfWy8JAU+rr61u3c2yE1MZ/bm6uef4sy6K/vz8ajUZcuHCh5XwXLlxo7jP/vfl9KrcfPny4OVVwfl/n5ubi9OnT1oABsHEKoOtGRkaK1fznmOd5EREt+8zfdu3ataIoimJ6erqIiKKvr68oiqL5/vw2s7OzRV9fXxERxY0bN4p6vb7o2OVx5m9b+PVqdbr/7OxsERHF+Pj4HR1nKc8991zx3HPPrXq/rTr+nY5Hec56vb6ozmvXrrV8PV+e50W9Xi+Koijq9XqR53kxPDxcFEVRTExMFBFRTE5OLhqPycnJtsdbTkQUIyMjq9oHgG1pVKiBLWC1oaYo2t+8drKtXZvJyckiIorz58/f0XHutP52JiYmijzPi9nZ2Ts6zlLWK9R0um2jx7/T8ahUKi0hY+F+58+fLyKimJ6ebqmzDDBFURTDw8Nt66xUKi3HXOrarUSoAaBDo6afAbFv376IiHj11Ve7XMliFy9ejLNnz97Vi967Mf6vvfZaXLp0KWZmZlqmmJWeeeaZiIj43//7fze3XblyJf7oj/6o+fXbb78dEYunxy18eMPdfO0A2BqEGmDLunz5cuR5Hvv37+92KXeloaGh+PGPf9x2HdO+ffuir68vTp06FXNzczE3Nxc///nP46GHHmq2Kdf1FEWx6AUAm0moAZq20mL8qampuH79erz00kvdLmXTbMb4nz59OiJuB8ZTp07FT3/603j00UeXref999+PDz74IH74wx+2bVc+5AAAukWoAZo3pc8++2yXK7mt0WjElStXWv4Wz9TUVPOG/G6zWeNfq9Xi4MGDERFx/PjxiIiWT14WKj+tOX78eAwNDS36xGxwcDAiIt56662Ym5uLiK+fhgYAm0mogQQ1Go1F/z5/W3mDWf5z4fsRt39TX7Z56623Is/z5jSk8jf05c12rVZr7lcGi7LtWm5i59c1/9/L4508eTJeffXVlrUa3//+9xfd9C93nI3U7rxbYfwXnmO+Wq0WBw4ciMcee6xl/5mZmZZPWhYeo/x0pt0Utf/6X/9rRNxeQ/Otb30rsiyLPXv2xJEjR5atBQDWXVefUwAURbH6p5/FvEcDl/utdtv8x+4ODg62PKFqenq6+V75KOXy0b3l43zLJ3ZVKpXmtrXUHgueulU+arjd68aNGx0fp1OrffrZUuft9vh3Wld5noX7l09Dm/+0s1Ke5y1jP9/09HRRqVSaj4Au959/zjzPOx7fhWPt6WcAdGA0KworOqHbRkdH4+jRo5uywPpu+ovz6+HIkSMRETE2NrYp50tt/Ofm5uIv//Iv49KlS5t+7izLYmRkJHp6ejb93AAkZcz0MwCWNDo62gx+ALBVCTWwjbRbi8PmSWX8+/v7m2uZZmZm4umnn+52SQCwrJ3dLgDYPHv27Gn59/WcAlVOq1pJKtOuNsJGjv96Kp+INjg4uK0eqQ1AuoQa2EY28iZ6q96gbyWpjNFLL70kzACQFNPPAACApAk1AABA0oQaAAAgaUINAACQNKEGAABImlADAAAkTagBAACSJtQAAABJE2oAAICkCTUAAEDShBoAACBpQg0AAJA0oQYAAEjazm4XAHztyJEj3S5h26nVahFh7AEgZUINbAEPPvhgPPfcc90uY1vav39/y9f/8A//EBERjz32WDfKYZ7nnnsuHnzwwW6XAUACsqIoim4XAbBV9PT0RETE6OholysBADo0Zk0NAACQNKEGAABImlADAAAkTagBAACSJtQAAABJE2oAAICkCTUAAEDShBoAACBpQg0AAJA0oQYAAEiaUAMAACRNqAEAAJIm1AAAAEkTagAAgKQJNQAAQNKEGgAAIGlCDQAAkDShBgAASJpQAwAAJE2oAQAAkibUAAAASRNqAACApAk1AABA0oQaAAAgaUINAACQNKEGAABImlADAAAkTagBAACSJtQAAABJE2oAAICkCTUAAEDShBoAACBpQg0AAJC0nd0uAKBb/uZv/ib++q//Or766qvmths3bkRExB//8R83t91zzz3xp3/6p/H8889vdokAQAeyoiiKbhcB0A1TU1Px/e9/v6O2k5OTsW/fvg2uCABYgzHTz4Bta9++ffG9731vxXZ79+4VaABgCxNqgG2tt7c3du3ateT7u3btih/96EebWBEAsFqmnwHb2qeffhp79+6N5f5X+I//+I+xd+/eTawKAFgF08+A7e2RRx6JP/zDP4wsyxa9l2VZPPHEEwINAGxxQg2w7b344ouxY8eORdt37NgRL774YhcqAgBWw/QzYNtrNBrxne98p+XRzhG3H+V869at+L3f+70uVQYAdMD0M4AHHnggnnzyyZZPa3bs2BEHDx4UaAAgAUINQNx+Clon2wCArcf0M4CI+NWvfhX33XdffPHFFxFx+1HOjUYjvvWtb3W5MgBgBaafAUREfPOb34w/+ZM/iZ07d8bOnTvj2WefFWgAIBFCDcBvvPDCC/Hll1/Gl19+Gc8//3y3ywEAOrSz2wXAdjQ6OtrtEmjjiy++iN27d0dRFPHrX//addqienp6ul0CAFuMNTXQBe3+0CPQGT+2AFjAmhrolpGRkSiKwmuLvd5///3427/923U9puu9Pq+RkZEu/1cLwFZl+hnAPM8880y3SwAAVkmoAZhn507/WwSA1Jh+BgAAJE2oAQAAkibUAAAASRNqAACApAk1AABA0oQaAAAgaUINAACQNKEGAABImlADAAAkTagBAACSJtQAAABJE2oAAICkCTWwxTUajbh8+XIcPnz4jtps1LkBALpNqIEt7ty5c3H8+PGoVqt31GYtTp482dFx5+bmolarxdDQ0F0TgLZKn2q1WvT390eWZZFlWfT398fU1FQ0Go3IsmzT65mZmYnTp09HlmVx+vTpuHr1asv7ZZ3tXhcuXIhqtRpzc3ObXjcAdzehBra4S5curUubtRgfH++o3fnz5+Pdd9+NU6dOrXuw6pat0Kf+/v548803o7e3N4qiiKIo4uWXX46ZmZnYs2fPptczNzcXU1NTcenSpZidnY2DBw/GoUOHWsanKIqo1+vNr2dnZ5u1P/PMMzE0NBS9vb3RaDQ2vX4A7l5ZURRFt4uA7SbLshgZGYmenp6O20fcvmG8kzZrsZrjblQN3bQefVrt9Y6I5icySwXLWq0WBw4c2NSxrlarked5y7alxmep7Y1GI06ePBkREW+99Vbce++9HZ9/dHQ0jh49eld9fwGwLsZ8UgMJaTQaceHChebUn5mZmRX3mZubi8uXLzenAA0NDS36LXm7Nku5evVqy5Si9bZSLcv1Z+EaoGq1GlmWxeHDh2NmZiZqtdqiKVGlclyzLOtoXDdSrVaL119/Pc6ePbtkm/3797d8vRnjsm/fvra19PX1ddy3Bx54IF555ZWoVqvxwQcfdLwfACxHqIGEfPrpp3HmzJmo1+tx69atePjhh1ecxtPb2xufffZZc1pQtVqNkydPtqxr6O3tjevXrzenCX388cfR39/f9nh79+6NwcHBqNfrG/Ib85VqWa4/89cA1Wq1yPM8pqeno1qtxhtvvBH79++PiYmJiIioVCot9Z85cyYqlUpMTk7GQw89tO79Wo133303IiIeeeSRZdvNr78b41J+Dz377LOr6t8TTzwRERHvvffeqvYDgCUVwKaLiGJkZGRV7Rf+53rjxo0iIorBwcEl20xMTBQRUdTr9ea2a9euFRFRDA8PF0VRFMPDw23b5Hm+6LiTk5PN/TqtczVWqqWT/rSrYeG2SqVSREQxOzvb3DY7O1tUKpV171N5jDu93svpxriU583zvKV9p31Yy7iOjIzc8bUA4K406pMaSNSjjz4aERGnTp1ass3Y2FhE3J7yU3rsscciIuLtt99u+ef8Nvv371+0lqNWq8XAwEAcO3ZsHapvb6VaOulPJ5577rmIiHj//feb2z766KPm9tR0a1wuXrwYZ8+eXdW6GADYCEIN3MUGBgYWbStvQMsnVnX6ZK+bN2/GwMBA1Gq19StwgZVq6aQ/ndi3b1/ked5yw/+zn/1syTUjm61co9Lpo4+7MS6XL1+OPM8Xre3pRNmvSqWy6n0BoB2hBhK33CLt8klV7dbdlPuVbaamppY9z7Fjx6JSqcSBAwc27HG8K9XSSX86deLEieYak5mZmfjBD36wymo3TrlG5ebNmx213+xxmZqaiuvXr8dLL720qmOXPvroo4iIeOqpp9a0PwAsJNRAosob/4MHDy7Z5sSJExFx+wEDpfK35EeOHImIr2+IBwYGmu+Vf2BxoVdffTXyPI9z586tQw8WW6mWTvrTqaeffjoiIt5888348MMP48knn7yz4tdRnueR53nbT2BKMzMzceHChYjY3HFpNBpx5cqVeO2115rbpqam2n6/tNNoNOLixYuR53nzXABwx7q9qge2o1jlwvE8z4uIKCYmJoqiKIp6vV7keV6cP3+++XX8ZuH1/MXis7OzRZ7nRZ7nze3Dw8NFX19fs015rHL/iCj6+vqKGzdutBy3XAw+PT3d8oCC+eda2Ha1lqulk/60q3d+XfPHpii+XhhfjuNC69Gnolj99S6Kr8difv9L09PTLWOwWePS7vqUr/Hx8Wa7pcZtcnJyUZ2r4UEBACxh1E8H6IK13OSWT5oqb/TLgFMeb/5rvnq9XgwODjbfGx4eXnSDXq/XmzeylUqleRPd7rjlk7bmb2t3k7vWm8+laumkP+3Ov1xNk5OTRUQsOsd692kt17soboeD8fHxoq+vr3n+PM+LwcHBYnp6uqXtZozL/DoWvpb6npn/On/+fHHt2rVVj0NJqAFgCaNZUfjTzLDZ1vIX5kmX670+RkdH4+jRoxvy95EASNqYNTUAAEDShBoAACBpO7tdAHD3y7Kso3amFQEAayHUABtOWAEANpLpZwAAQNKEGgAAIGlCDQAAkDShBgAASJpQAwAAJE2oAQAAkibUAAAASRNqAACApAk1AABA0oQaAAAgaUINAACQNKEGAABImlADAAAkbWe3C4Dt6tq1a90ugU3ket85YwjAUrKiKIpuFwHbTZZl3S4BkuXHFgALjPmkBrrATdnW1dPTExERo6OjXa4EAOiUNTUAAEDShBoAACBpQg0AAJA0oQYAAEiaUAMAACRNqAEAAJIm1AAAAEkTagAAgKQJNQAAQNKEGgAAIGlCDQAAkDShBgAASJpQAwAAJE2oAQAAkibUAAAASRNqAACApAk1AABA0oQaAAAgaUINAACQNKEGAABImlADAAAkTagBAACSJtQAAABJE2oAAICkCTUAAEDShBoAACBpQg0AAJA0oQYAAEiaUAMAACRNqAEAAJIm1AAAAEkTagAAgKQJNQAAQNJ2drsAgG75+7//+5iammrZ9umnn0ZExODgYMv2xx9/PPbv379ptQEAnRNqgG2r0WjEn/3Zn8WOHTvinntuf3BdFEVERPz4xz+OiIivvvoqvvzyyxgfH+9anQDA8rKi/AkOsM188cUXcd9998WvfvWrZdt94xvfiF/+8pexe/fuTaoMAFiFMWtqgG1r165dcezYsWXDyq5du+L48eMCDQBsYUINsK0dP348Pv/88yXf/+KLL+LEiRObWBEAsFqmnwHb2ldffRW///u/H/V6ve37999/f/zrv/5rc80NALDlmH4GbG/33HNPvPDCC22nl+3evTt++MMfCjQAsMX5SQ1se0tNQfv888/j+PHjXagIAFgN088AImLv3r3xT//0Ty3bHn744bh582Z3CgIAOmX6GUBExAsvvBC7du1qfr179+740Y9+1MWKAIBO+aQGICJ+/vOfxx/8wR+0bLtx40Y8+uijXaoIAOiQT2oAIm5PP3v88ccjy7LIsiwef/xxgQYAEiHUAPzGiy++GDt27IgdO3bEiy++2O1yAIAOmX4G8Bv/8i//Eg8++GAURREzMzPx3e9+t9slAQArG9vZ7QpgO8qyrNslsIIHH3yw2yWwBL+LA2AhoQa65JVXXokDBw50uwwWuHLlSmRZFocOHVq3Yx49etT1XgfXrl2LixcvdrsMALYgoQa65MCBA9HT09PtMligDDPf/va31+2YR48edb3XiVADQDtCDcA86xlmAIDN4elnAABA0oQaAAAgaUINAACQNKEGAABImlADAAAkTagBAACSJtQAAABJE2oAAICkCTUAAEDShBoAACBpQg0AAJA0oQYAAEiaUAMAACRNqIEtrtFoxOXLl+Pw4cN31Gajzg0A0G1CDWxx586di+PHj0e1Wr2jNmtx8uTJjo47NzcXtVothoaG7poANDMzE6dPn44sy+L06dNx9erVrtRRq9Wiv78/siyLLMuiv78/pqamotFoRJZlm17PSuNS1tnudeHChahWqzE3N7fpdQNwdxNqYIu7dOnSurRZi/Hx8Y7anT9/Pt599904derUugerbpibm4upqam4dOlSzM7OxsGDB+PQoUOb3rf+/v548803o7e3N4qiiKIo4uWXX46ZmZnYs2fPptYS0dm4FEUR9Xq9+fXs7Gyz9meeeSaGhoait7c3Go3GptcPwN0rK4qi6HYRsN1kWRYjIyPR09PTcfuI2zeMd9JmLVZz3I2qYbNVq9XI87xl2530bbXXOyKan8gsFSxrtVocOHBgU8d6NeOy1PZGoxEnT56MiIi33nor7r333o7PPzo6GkePHk3++wuAdTfmkxpISKPRiAsXLjSn/szMzKy4z9zcXFy+fLk5BWhoaGjRb8nbtVnK1atXW6YUrbeValmuPwvXAFWr1ciyLA4fPhwzMzNRq9UWTYkqleOaZVns27evbW19fX3r3t92arVavP7663H27Nkl2+zfv7/l61TG5YEHHohXXnklqtVqfPDBBx3vBwDLEWogIZ9++mmcOXMm6vV63Lp1Kx5++OEVp/H09vbGZ5991pwWVK1W4+TJky3rGnp7e+P69evNaUIff/xx9Pf3tz3e3r17Y3BwMOr1+ob8xnylWpbrz/w1QLVaLfI8j+np6ahWq/HGG2/E/v37Y2JiIiIiKpVKS/1nzpyJSqUSk5OT8dBDD7XUVI7Vs88+u+79befdd9+NiIhHHnlk2Xbz609pXJ544omIiHjvvfdWtR8ALKkANl1EFCMjI6tqv/A/1xs3bhQRUQwODi7ZZmJiooiIol6vN7ddu3atiIhieHi4KIqiGB4ebtsmz/NFx52cnGzu12mdq7FSLZ30p10NC7dVKpUiIorZ2dnmttnZ2aJSqbSta2JiosjzvKX9aqzH9V7OVhyXlfqwlu+VkZGRO/r+AuCuNeqTGkjUo48+GhERp06dWrLN2NhYRNye8lN67LHHIiLi7bffbvnn/Db79+9ftJajVqvFwMBAHDt2bB2qb2+lWjrpTyeee+65iIh4//33m9s++uij5vaFLl68GGfPnl3V+o/NZFwA2O6EGriLDQwMLNpW3oCWT6zq9IleN2/ejIGBgajVautX4AIr1dJJfzqxb9++yPO85Yb/Zz/7Wds1I5cvX448zxetYdlI5RqVTh99nNq4lP2qVCqr3hcA2hFqIHHLLdIun1TVbt1NuV/ZZmpqatnzHDt2LCqVShw4cGDDHse7Ui2d9KdTJ06caK4xmZmZiR/84AeL2kxNTcX169fjpZdeWtWx71S5RuXmzZsdtU9tXD766KOIiHjqqafWtD8ALCTUQKLKG/+DBw8u2ebEiRMRcfsBA6Xyt+RHjhyJiK9viAcGBprvlX9gcaFXX3018jyPc+fOrUMPFluplk7606mnn346IiLefPPN+PDDD+PJJ59seb/RaMSVK1fitddea26bmppqOy7rLc/zyPO87ScwpZmZmbhw4UJEpDUujUYjLl68GHmeN88FAHes26t6YDuKVS4cz/O8iIhiYmKiKIqiqNfrRZ7nxfnz55tfx28WXs9fLD47O1vkeV7ked7cPjw8XPT19TXblMcq94+Ioq+vr7hx40bLccvF4NPT0y0PKJh/roVtV2u5WjrpT7t659c1f2yK4uuF8eU4LldH+RofH191v1Z7vefXML//penp6ZYx2GrjstT3wuTk5KI6V8ODAgBYwqifDtAFa7nJLZ80Vd7olwGnPN7813z1er0YHBxsvjc8PLwodNTr9eaNbKVSad5Etztu+aSt+dva3eSu9eZzqVo66U+78y9X0+TkZBERi87R19e3ZJ8Wtu3EWq53UdwOB+Pj4y315HleDA4OFtPT0y1tt8q4LPV+GZKuXbu26nEoCTUALGE0Kwp/mhk221r+wjzpcr3Xx+joaBw9enRD/j4SAEkbs6YGAABImlADAAAkbWe3CwDuflmWddTOtCIAYC2EGmDDCSsAwEYy/QwAAEiaUAMAACRNqAEAAJIm1AAAAEkTagAAgKQJNQAAQNKEGgAAIGlCDQAAkDShBgAASJpQAwAAJE2oAQAAkibUAAAASRNqAACApGVFURTdLgK2myzLul0CJMuPLQAWGNvZ7QpgOxoZGel2CSzhJz/5SURE/Pmf/3mXKwEAOuWTGoB5enp6IiJidHS0y5UAAB0as6YGAABImlADAAAkTagBAACSJtQAAABJE2oAAICkCTUAAEDShBoAACBpQg0AAJA0oQYAAEiaUAMAACRNqAEAAJIm1AAAAEkTagAAgKQJNQAAQNKEGgAAIGlCDQAAkDShBgAASJpQAwAAJE2oAQAAkibUAAAASRNqAACApAk1AABA0oQaAAAgaUINAACQNKEGAABImlADAAAkTagBAACSJtQAAABJE2oAAICkCTUAAEDShBoAACBpQg0AAJC0nd0uAKBb/t//+3/x61//umXb559/HhER//f//t+W7b/1W78Vv/3bv71ptQEAnRNqgG3rf/2v/xU//vGP2773u7/7uy1f//SnP43/9t/+22aUBQCsUlYURdHtIgC64d/+7d/iO9/5Tnz55ZfLttuxY0f8n//zf+L+++/fpMoAgFUYs6YG2Lbuv//+ePrpp2PHjh1LttmxY0ccOnRIoAGALUyoAba1F154IZb7wLooinjhhRc2sSIAYLVMPwO2tc8++yzuv//+RQ8MKO3evTv+7d/+Lb75zW9ucmUAQIdMPwO2t2984xvxX/7Lf4ldu3Ytem/nzp1x+PBhgQYAtjihBtj2nn/++fj3f//3Rdu//PLLeP7557tQEQCwGqafAdve559/Hvfdd1989tlnLdt/53d+J375y1/Gb/3Wb3WpMgCgA6afAezevTuee+652L17d3Pbrl27oqenR6ABgAQINQARceLEifj888+bX3/xxRdx4sSJLlYEAHTK9DOAiPjqq69iz5498ctf/jIiIr797W9HvV5f9m/YAABbgulnABER99xzTzz//POxe/fu2LVrV7zwwgsCDQAkQqgB+I3jx4/H559/buoZACRmZ7cLgBQcOXKk2yWwSX77t387IiL++3//712uhM0yNjbW7RIAuEM+qYEOvPPOO/GLX/yi22WwRqu5fg8//HA8/PDDG1wRW8EvfvGLeOedd7pdBgDrwIMCoANZlsXIyEj09PR0uxTWYDXX7/r16xER8Z/+03/a6LLostHR0Th69Gj4MQiQvDHTzwDmEWYAID2mnwEAAEkTagAAgKQJNQAAQNKEGgAAIGlCDQAAkDShBgAASJpQAwAAJE2oAQAAkibUAAAASRNqAACApAk1AABA0oQaAAAgaUINAACQNKEG1lmj0YjLly/H4cOH76jNRp07RXdrv7aSpca4v78/+vv7u1QVAHRGqIF1du7cuTh+/HhUq9U7arMWJ0+e7Oi4c3NzUavVYmhoKImgsFHj1U6WZS2vWq22ZNtarbao/UbVUb4OHz4cQ0ND0Wg01u1cEZs7xqWZmZk4ffp0ZFkWp0+fjqtXr7a8v9QYZFkWFy5ciGq1GnNzc5tWLwBbl1AD6+zSpUvr0mYtxsfHO2p3/vz5ePfdd+PUqVObehO7Vhs1Xu0URRHT09PNr998880l285/r16vR1EU61pHvV5v+booivjpT38aMzMzsWfPnvjkk0/W7X/+fQgAACAASURBVHxLjfFrr70Wr7322rqdpzQ3NxdTU1Nx6dKlmJ2djYMHD8ahQ4davh8XjsHs7GxzHJ555pkYGhqK3t7edQ94AKRHqIFtaKNuVO8WDz30UETcDn8DAwMxMzOzqM3MzEzs3bu3+fUDDzyw7nW0O+ZDDz0UL7/8ckRE/OQnP1n3c26WDz74IPI8j4iIe++9N44dOxYRseiTw/ljcO+99zb/fd++ffE//+f/jIjbn1D6xAZgexNqYAM1Go24cOFCc3pNu5vjhebm5uLy5cvNaTbtphq1a7OUq1evbsj0qEajEdVqtXkTOjQ01Oznwk8Q1tqnbv8G/plnnomIiA8//HDRex9++GHz/Xbm5uaaY5JlWfT39zf7027K2mqmsZU3+gMDAy3nW+8xbrfOZuG2arXanBa38Pv76tWrcfjw4eZ0sfnnKgPNQn19fcv2fb4HHnggXnnllahWq/HBBx90vB8Adx+hBjbQp59+GmfOnIl6vR63bt2Khx9+eMUb9d7e3vjss8+aU2+q1eqi30T39vbG9evXm1NxPv744yUXc+/duzcGBwfXfXrUnj174vDhw1GtVqNWq8VLL70Us7OzERHxve99ryXYdNqnldpstn379kVfX18cP3580Xt/93d/F/v27Vty37/8y7+MU6dORb1ej+np6Xj99dfj3LlzEXF7WtXg4GBERHN6Vb1ejzzPY3JycsXrVI7J/ACwEWPcbo3W/G21Wi3yPI/p6emoVqvxxhtvNNtVq9U4dOhQnD17NoqiiP/4H/9j7NmzZ8nQVtbw7LPPLtv3hZ544omIiHjvvfdWtR8Ad5kCWFFEFCMjI6tqv/A/rxs3bhQRUQwODi7ZZmJiooiIol6vN7ddu3atiIhieHi4KIqiGB4ebtsmz/NFx52cnGzu12mdq9Fu/8nJySIiivPnz3fcp07a3Emtq71+5T7za7t27VpLHycmJpatq1KpFH19fS3HW9iur6+v2e/z58+39H/hfpOTk0VRFMXs7GxRqVRaatrIMV7rtqXalN8XC01MTBR5nhezs7NLjsFS1vq9MTIyckff/wBsGaP+bw4dWI9Qs3D7cje5883OzhYR0QwteZ53dIN37dq1lpvq1dTZqU762UmfOmnTrVBT/vv8saxUKi3vLVfX9PR0cf78+bbt6vV6s483btxYso6Fr0ql0gw5RbGxY7zWbe3Ot9xY5XneEhw73a+T95ci1ADcNYQa6MRmhZq17teubfmJzlI3ip0cayV3Uu96tem0zjsJNeVYTk9PF/V6veXTr+XqGhwcbAaWpdqtdJ066fdGjvFat5Wf2JVjtfATvPmGh4ebn2Cupn9F8XUwmx80OyXUANw1Rq2pgU223ELocvF0u3U35X5lm6mpqWXPc+zYsahUKnHgwIGuLLhfWG8nfVquTTf90R/9UUTcfjjA1atXm18v5/Lly3Hq1Kn46U9/Go8++mjbNo1GI27duhXnz5+/o+u0Fcd43759MT4+Hrdu3Wo+KGF4eDjOnDnT0m5qaiquX78eL7300prO89FHH0VExFNPPXXHNQOQLqEGNkkZQg4ePLhkmxMnTkTE7QcMlMoF1EeOHImIr29OBwYGmu+Vf8RwoVdffTXyPG8uUN8M5QMCygXfnfSpkzbd9NBDD0WlUonjx4/HrVu3mo98Xk75cIHl2r711ltx5syZOHny5B1dp604xtVqNZ588sk4c+ZMFEUR4+Pjzcc2lxqNRly5cqXl8eJTU1Ntv5fbaTQacfHixcjzPJ5++ul1rR+AxHT7syJIQaxy+lK57qVcTF6v14s8z5tTb8q1FBGtC7dnZ2eLPM+LPM+b24eHh1vWc5THKveP36z3uHHjRstxywXX09PTRUQsmt5TTtuZ33Yt4xLzphiVi9jLNRqd9mmlNkuN12rqXM31K883/1zl9Kn5a1mWq6u8RtPT0y3Tz+r1enOc5o97u2lU86/Rcv3eqDFeaVtZf7s6539/Lvxerdfrbb+Py9f4+HjbMZg/XpOTk4v6s1qmnwHcNaypgU6s9qa4KL5+mlN5I1cGnPJ481/z1ev1YnBwsCUwLAwd9Xq9+QSsSqXSXGTe7rjlU6/mb1vqhnMt41Le6Jd9HRwcbFtvJ31aqs161Nnp9VtuXNo9zWyptmUIqlQqzevV19fXDJkL2y91vE77vhFjfCfb5n9PtAs25YME2r2W+n6e/zp//vyy68U6IdQA3DVGs6JYxz9cAXepLMtiZGQkenp6ul3KllL+vZGt/r8R12/zffLJJ/Ef/sN/WDT97pNPPonvfe97W+J7ZnR0NI4ePbolagHgjoxZUwPAurp8+XI8+uijbdcT7dmzJ4aHh7tQFQB3s53dLgBI0/ynaDUajXjggQe6WA1bydtvvx2fffZZ/Of//J9bgs0nn3wSf/d3f7fmJ50BwFJ8UgMskmXZiq89e/Y028//d3jrrbfiG9/4RrzxxhvN75f+/v74xS9+IdAAsCF8UgMsYo0Bd+Lee++NY8eOxbFjx+LSpUvdLgeAbcAnNQAAQNKEGgAAIGlCDQAAkDShBgAASJpQAwAAJE2oAQAAkibUAAAASRNqAACApAk1AABA0oQaAAAgaUINAACQNKEGAABImlADAAAkLSuKouh2EbDVZVkW+/fvj+9+97vdLoU1eOedd1w/FvnFL34RtVot/BgESN6YUAMdOHLkSLdLYJP8wz/8Q0REPPbYY12uhM0yNjbW7RIAuDNCDcB8PT09ERExOjra5UoAgA6NWVMDAAAkTagBAACSJtQAAABJE2oAAICkCTUAAEDShBoAACBpQg0AAJA0oQYAAEiaUAMAACRNqAEAAJIm1AAAAEkTagAAgKQJNQAAQNKEGgAAIGlCDQAAkDShBgAASJpQAwAAJE2oAQAAkibUAAAASRNqAACApAk1AABA0oQaAAAgaUINAACQNKEGAABImlADAAAkTagBAACSJtQAAABJE2oAAICkCTUAAEDShBoAACBpQg0AAJA0oQYAAEiaUAMAACRtZ7cLAOiWv/mbv4m//uu/jq+++qq57caNGxER8cd//MfNbffcc0/86Z/+aTz//PObXSIA0IGsKIqi20UAdMPU1FR8//vf76jt5ORk7Nu3b4MrAgDWYMz0M2Db2rdvX3zve99bsd3evXsFGgDYwoQaYFvr7e2NXbt2Lfn+rl274kc/+tEmVgQArJbpZ8C29umnn8bevXtjuf8V/uM//mPs3bt3E6sCAFbB9DNge3vkkUfiD//wDyPLskXvZVkWTzzxhEADAFucUANsey+++GLs2LFj0fYdO3bEiy++2IWKAIDVMP0M2PYajUZ85zvfaXm0c8TtRznfunUrfu/3fq9LlQEAHTD9DOCBBx6IJ598suXTmh07dsTBgwcFGgBIgFADELefgtbJNgBg6zH9DCAifvWrX8V9990XX3zxRUTcfpRzo9GIb33rW12uDABYgelnABER3/zmN+NP/uRPYufOnbFz58549tlnBRoASIRQA/AbL7zwQnz55Zfx5ZdfxvPPP9/tcgCADu3sdgGwHY2Ojna7BNr44osvYvfu3VEURfz61792nbaonp6ebpcAwBZjTQ10Qbs/9Ah0xo8tABawpga6ZWRkJIqi8Npir/fffz/+9m//dl2P6Xqvz2tkZKTL/9UCsFWZfgYwzzPPPNPtEgCAVRJqAObZudP/FgEgNaafAQAASRNqAACApAk1AABA0oQaAAAgaUINAACQNKEGAABImlADAAAkTagBAACSJtQAAABJE2oAAICkCTUAAEDShBoAACBpQg1scY1GIy5fvhyHDx++ozYbdW4AgG4TamCLO3fuXBw/fjyq1eodtVmLkydPdnTcmZmZOH36dGRZFqdPn46rV6+uax3dMDc3F7VaLYaGhroa6mq1WvT390eWZZFlWfT398fU1FQ0Go3IsmzT61npWpd1tntduHAhqtVqzM3NbXrdANzdhBrY4i5durQubdZifHx8xTZzc3MxNTUVly5ditnZ2Th48GAcOnRo3QPWZjt//ny8++67cerUqa71pb+/P958883o7e2NoiiiKIp4+eWXY2ZmJvbs2bPp9XRyrYuiiHq93vx6dna2WfszzzwTQ0ND0dvbG41GY9PrB+DulRVFUXS7CNhusiyLkZGR6Onp6bh9xO0bxjtpsxYrHbdarUae55tSSzesR19We70jovmJzFLBslarxYEDBzZ1jFdzrZfa3mg04uTJkxER8dZbb8W9997b8flHR0fj6NGjd8X3FQDraswnNZCQRqMRFy5caE79mZmZWXGfubm5uHz5cnMK0NDQ0KLfkrdrs5SrV6+2TClaeJNb6uvrW13nOqxluf4sXANUrVYjy7I4fPhwzMzMRK1WWzQlqlSOa5ZlHY3rRqrVavH666/H2bNnl2yzf//+lq83Y1z27dvXtpbVXOsHHnggXnnllahWq/HBBx90vB8ALEeogYR8+umncebMmajX63Hr1q14+OGHV5zG09vbG5999llzWlC1Wo2TJ0+2rGvo7e2N69evN6cJffzxx9Hf39/2eHv37o3BwcGo1+ttf2NeHvfZZ59dUx9XqmW5/sxfA1Sr1SLP85ieno5qtRpvvPFG7N+/PyYmJiIiolKptNR/5syZqFQqMTk5GQ899NCaal8v7777bkREPPLII8u2m19/N8Zlrdf6iSeeiIiI9957b1X7AcCSCmDTRUQxMjKyqvYL/3O9ceNGERHF4ODgkm0mJiaKiCjq9Xpz27Vr14qIKIaHh4uiKIrh4eG2bfI8X3TcycnJ5n5LmZiYKPI8L2ZnZzvuX2mlWjrpT7txWLitUqkUEdFS4+zsbFGpVBbV1O54q7Ue13s53RiX8rxLXeuV+rCWcR0ZGbnjawHAXWnUJzWQqEcffTQiIk6dOrVkm7GxsYi4PeWn9Nhjj0VExNtvv93yz/lt9u/fv2gtR61Wi4GBgTh27NiydV28eDHOnj27qrUSpZVq6aQ/nXjuueciIuL9999vbvvoo4+a21PTrXG5k2sNAOtJqIG72MDAwKJt5Q1o+cSqTp/sdfPmzRgYGIharbZkm8uXL0ee54vWe3RqpVo66U8n9u3bF3met9zw/+xnP1tyzchmK9eodPro426My51c67JflUpl1fsCQDtCDSRuuUXa5SL+dutuyv3KNlNTU8ue59ixY1GpVOLAgQNtjzc1NRXXr1+Pl156qePal6p3qVo66U+nTpw40VxjMjMzEz/4wQ9WWe3GKdeo3Lx5s6P2mz0ud3qtP/roo4iIeOqpp9a0PwAsJNRAosob/4MHDy7Z5sSJExFx+wEDpfK35EeOHImIr2+IBwYGmu+Vf2BxoVdffTXyPI9z5861bG80GnHlypV47bXXWuprd4zlrFRLJ/3p1NNPPx0REW+++WZ8+OGH8eSTT65q/42U53nked72E5jSzMxMXLhwISI2d1zu9Fo3Go24ePFi5HnePBcA3LFur+qB7ShWuXA8z/MiIoqJiYmiKIqiXq8XeZ4X58+fb34dv1l4PX+x+OzsbJHneZHneXP78PBw0dfX12xTHqvcPyKKvr6+4saNGy3HLReDT09PtzygoN3+5Wt8fHxV47JcLZ30p129s7OzbcemKL5eGF+O40Lz913Lgw9Kq73eRfH1WMzvf2l6erplDDZrXDq91kuN2+Tk5KI6V8ODAgBYwqifDtAFa7nJLZ80Vd7olwGnPN7813z1er0YHBxsvjc8PLzoBr1erzdvZCuVSvMmut1xyydtzQ8d7W5yI2LRzXgnlqqlk/60q3e5sZmcnFyyzqX6tBZrud5FcTscjI+Pt4xxnufF4OBgMT093dJ2M8alk2u91PtlSLp27dqqx6Ek1ACwhNGsKPxpZthsa/kL86TL9V4fo6OjcfTo0bZ/HwmAbW3MmhoAACBpQg0AAJC0nd0uALj7ZVnWUTvTigCAtRBqgA0nrAAAG8n0MwAAIGlCDQAAkDShBgAASJpQAwAAJE2oAQAAkibUAAAASRNqAACApAk1AABA0oQaAAAgaUINAACQNKEGAABImlADAAAkTagBAACStrPbBcB2de3atW6XwCZyve+cMQRgKVlRFEW3i4DtJsuybpcAyfJjC4AFxnxSA13gpmzr6unpiYiI0dHRLlcCAHTKmhoAACBpQg0AAJA0oQYAAEiaUAMAACRNqAEAAJIm1AAAAEkTagAAgKQJNQAAQNKEGgAAIGlCDQAAkDShBgAASJpQAwAAJE2oAQAAkibUAAAASRNqAACApAk1AABA0oQaAAAgaUINAACQNKEGAABImlADAAAkTagBAACSJtQAAABJE2oAAICkCTUAAEDShBoAACBpQg0AAJA0oQYAAEiaUAMAACRNqAEAAJIm1AAAAEkTagAAgKQJNQAAQNKEGgAAIGk7u10AQLf8/d//fUxNTbVs+/TTTyMiYnBwsGX7448/Hvv379+02gCAzgk1wLbVaDTiz/7sz2LHjh1xzz23P7guiiIiIn784x9HRMRXX30VX375ZYyPj3etTgBgeVlR/gQH2Ga++OKLuO++++JXv/rVsu2+8Y1vxC9/+cvYvXv3JlUGAKzCmDU1wLa1a9euOHbs2LJhZdeuXXH8+HGBBgC2MKEG2NaOHz8en3/++ZLvf/HFF3HixIlNrAgAWC3Tz4Bt7auvvorf//3fj3q93vb9+++/P/71X/+1ueYGANhyTD8Dtrd77rknXnjhhbbTy3bv3h0//OEPBRoA2OL8pAa2vaWmoH3++edx/PjxLlQEAKyG6WcAEbF37974p3/6p5ZtDz/8cNy8ebM7BQEAnTL9DCAi4oUXXohdu3Y1v969e3f86Ec/6mJFAECnfFIDEBE///nP4w/+4A9att24cSMeffTRLlUEAHTIJzUAEbennz3++OORZVlkWRaPP/64QAMAiRBqAH7jxRdfjB07dsSOHTvixRdf7HY5AECHTD8D+I1/+Zd/iQcffDCKooiZmZn47ne/2+2SAICVje3sdgWwHWVZ1u0SWMGDDz7Y7RJYgt/FAbCQUANd8sorr8SBAwe6XQYLXLlyJbIsi0OHDq3bMY8ePep6r4Nr167FxYsXu10GAFuQUANdcuDAgejp6el2GSxQhplvf/vb63bMo0ePut7rRKgBoB2hBmCe9QwzAMDm8PQzAAAgaUINAACQNKEGAABImlADAAAkTagBAACSJtQAAABJE2oAAICkCTUAAEDShBoAACBpQg0AAJA0oQYAAEiaUAMAACRNqAEAAJIm1MAW12g04vLly3H48OE7arNR5wYA6DahBra4c+fOxfHjx6Nard5Rm7U4efJkR8edmZmJ06dPR5Zlcfr06bh69eq61tENW6VPtVot+vv7I8uyyLIs+vv7Y2pqKhqNRmRZtun1rDQuZZ3tXhcuXIhqtRpzc3ObXjcAdzehBra4S5curUubtRgfH1+xzdzcXExNTcWlS5didnY2Dh48GIcOHVr3gLWZtkqf+vv7480334ze3t4oiiKKooiXX345ZmZmYs+ePZtaS0Rn41IURdTr9ebXs7OzzdqfeeaZGBoait7e3mg0GptePwB3r6woiqLbRcB2k2VZjIyMRE9PT8ftI27fMN5Jm7VY6bjVajXyPN+UWjbLevdptdc7IpqfyCwVLGu1Whw4cGBTx3g147LU9kajESdPnoyIiLfeeivuvffejs8/OjoaR48eTfb7CoANM+aTGkhIo9GICxcuNKf+zMzMrLjP3NxcXL58uTkFaGhoaNFvydu1WcrVq1dbphQtvMkt9fX1ra5zHdayXH8WrgGqVquRZVkcPnw4ZmZmolarLZoSVSrHNcuy2Ldv37r2abVqtVq8/vrrcfbs2SXb7N+/v+XrVMblgQceiFdeeSWq1Wp88MEHHe8HAMsRaiAhn376aZw5cybq9XrcunUrHn744RWn8fT29sZnn33WnBZUrVbj5MmTLesaent74/r1681pQh9//HH09/e3Pd7evXtjcHAw6vV629+Yl8d99tln19THlWpZrj/z1wDVarXI8zymp6ejWq3GG2+8Efv374+JiYmIiKhUKi31nzlzJiqVSkxOTsZDDz20rn1arXfffTciIh555JFl282vP6VxeeKJJyIi4r333lvVfgCwpALYdBFRjIyMrKr9wv9cb9y4UUREMTg4uGSbiYmJIiKKer3e3Hbt2rUiIorh4eGiKIpieHi4bZs8zxcdd3JysrnfUiYmJoo8z4vZ2dmO+1daqZZO+tNuHBZuq1QqRUS01Dg7O1tUKpV171N5/ju93svZiuOyUh9W28eiKIqRkZFV7wPAtjDqkxpI1KOPPhoREadOnVqyzdjYWETcnvJTeuyxxyIi4u2332755/w2+/fvX7SWo1arxcDAQBw7dmzZui5evBhnz55d1VqJ0kq1dNKfTjz33HMREfH+++83t3300UfN7QvdSZ82g3EBYLsTauAuNjAwsGhbeQNaPrGq0yd63bx5MwYGBqJWqy3Z5vLly5Hn+aL1Hp1aqZZO+tOJffv2RZ7nLTf8P/vZz9quGbnTPq1FuUal00cfpzYuZb8qlcqq9wWAdoQaSNxyi7TLRfzt1t2U+5Vtpqamlj3PsWPHolKpxIEDB9oeb2pqKq5fvx4vvfRSx7UvVe9StXTSn06dOHGiucZkZmYmfvCDHyxqsx59WotyjcrNmzc7ap/auHz00UcREfHUU0+taX8AWEiogUSVN/4HDx5css2JEyci4vYDBkrlb8mPHDkSEV/fEA8MDDTfK//A4kKvvvpq5Hke586da9neaDTiypUr8dprr7XU1+4Yy1mplk7606mnn346IiLefPPN+PDDD+PJJ59seX+9+rQWeZ5HnudtP4EpzczMxIULFyIirXFpNBpx8eLFyPO8eS4AuGPdXtUD21GscuF4nudFRBQTExNFURRFvV4v8jwvzp8/3/w6frPwev5i8dnZ2SLP8yLP8+b24eHhoq+vr9mmPFa5f0QUfX19xY0bN1qOWy4Gn56ebnlAQbv9y9f4+PiqxmW5WjrpT7t6Z2dn245NUXy9ML4cx+XqWGufimL113t+DfP7X5qenm4Zg602LvOPPf8hApOTk4vqXA0PCgBgCaN+OkAXrOUmt3zSVHmjXwac8njzX/PV6/VicHCw+d7w8PCip1XV6/XmjWylUmneRLc7bvmkrfmho91NbkQsuhnvxFK1dNKfdvUuNzaTk5Nt61zvPq3lehfF7XAwPj7eUk+e58Xg4GAxPT3d0narjMtS75ch6dq1a6seh5JQA8ASRrOi8KeZYbOt5S/Mky7Xe32Mjo7G0aNH2/59JAC2tTFragAAgKQJNQAAQNJ2drsA4O6XZVlH7UwrAgDWQqgBNpywAgBsJNPPAACApAk1AABA0oQaAAAgaUINAACQNKEGAABImlADAAAkTagBAACSJtQAAABJE2oAAICkCTUAAEDShBoAACBpQg0AAJA0oQYAAEhaVhRF0e0iYLvJsqzbJUCy/NgCYIGxnd2uALajkZGRbpfAEn7yk59ERMSf//mfd7kSAKBTPqkBmKenpyciIkZHR7tcCQDQoTFragAAgKQJNQAAQNKEGgAAIGlCDQAAkDShBgAASJpQAwAAJE2oAQAAkibUAAAASRNqAACApAk1AABA0oQaAAAgaUINAACQNKEGAABImlADAAAkTagBAACSJtQAAABJE2oAAICkCTUAAEDShBoAACBpQg0AAJA0oQYAAEiaUAMAACRNqAEAAJIm1AD/n737CY3jvv8//hrrT0po4tLEVtMmNgTVIRSiFkOR+wO7sQ2lpqNTLEu25RIquzKNwV/kQzErfHDAh0rUh4CElFKMaVZ/fNKSpAdLJT5Y24PD7sEUua2wFqvtbAJdxVCIXfvzO7gz2ZV2V7PSakcf6/mAJdbsZ2fen89u7HlpPp9ZAAAAqxFqAAAAAFiNUAMAAADAaoQaAAAAAFYj1AAAAACwGqEGAAAAgNUINQAAAACsRqgBAAAAYDVCDQAAAACr1UddAABE5T//+Y++/PLLgm0PHjyQJP373/8u2P7MM8/o2WefrVltAAAgPEINgE3r97//vd55552iz33zm98s+Pm9997Tr371q1qUBQAAKuQYY0zURQBAFD777DO99NJLevToUdl2dXV1+uc//6lt27bVqDIAAFCBCdbUANi0tm3bpv3796uurq5km7q6Oh04cIBAAwDABkaoAbCpHT9+XOUuWBtjdPz48RpWBAAAKsX0MwCb2v3797Vt27ZlNwzwNTY26rPPPtPzzz9f48oAAEBITD8DsLk999xz+tnPfqaGhoZlz9XX16utrY1AAwDABkeoAbDpHTt2TP/973+XbX/06JGOHTsWQUUAAKASTD8DsOk9ePBAL774ou7fv1+w/etf/7o+//xzPfPMMxFVBgAAQmD6GQA0NjbqrbfeUmNjY7CtoaFB7e3tBBoAACxAqAEASUePHtWDBw+Cnx8+fKijR49GWBEAAAiL6WcAIOnx48dqamrS559/Lkl64YUX5Hle2e+wAQAAGwLTzwBAkrZs2aJjx46psbFRDQ0NOn78OIEGAABLEGoA4H86Ozv14MEDpp4BAGCZ+qgLAGxw+PDhqEtAjTz77LOSpN/85jcRV4JamZiYiLoEAMAacaUGCOHatWu6d+9e1GVglSp5/3bu3KmdO3euc0XYCO7du6dr165FXQYAoAq4UQAQguM4GhsbU3t7e9SlYBUqef9u374tSfre97633mUhYuPj4zpy5Ij4ZxAArDfB9DMAyEOYAQDAPkw/AwAAAGA1Qg0AAAAAqxFqAAAAAFiNUAMAAADAaoQaAAAAAFYj1AAAAACwGqEGAAAAgNUINQAAAACsRqgBAAAAYDVCDQAAAACrEWoAAAAAWI1QAwAAAMBqhBoAAAAAViPUAFWWzWY1Ojqqtra2jHZaAQAAIABJREFUNbVZr2Pb6Gnt10ZSaoz7+vrU19cXUVUAAIRDqAGq7MKFC+rs7FQikVhTm9Xo7u4Otd9MJqPTp0/LcRydPn1a09PTVa2j2tZrvIpxHKfgkUwmS7ZNJpPL2q9XHf6jra1NIyMjymazVTuWVNsx9q30OSw1Bo7jaGBgQIlEQouLizWrFwCwcRFqgCobHBysSpvVmJycXLHN4uKi0um0BgcHlcvltG/fPh04cKCmJ7OVWq/xKsYYo/n5+eDnK1eulGyb/5zneTLGVLUOz/MKfjbG6L333lMmk1FTU5Pu3LlTteOVGuOLFy/q4sWLVTuOL8zncOkY5HK5YBwOHjyokZERdXV1VT3gAQDsQ6gBNpkbN27IdV1J0tatW9XR0SFJTO3Ks2PHDklSf3+/hoaGlMlklrXJZDJqbm4Oft6+fXvV6yi2zx07dujMmTOSpN/+9rdVP2athP0c5o/B1q1bgz+3tLTo/fffl/TkCiVXbABgcyPUAOsom81qYGAgmF5T7OR4qcXFRY2OjgbTbIpNNSrWppTp6emCaTv+ieRSPT09FfctkUgEJ6EjIyNBP5deQVhtn6L+DfzBgwclSTdv3lz23M2bN4Pni1lcXAzGxHEc9fX1Bf0pNmWtkmls/on+0NBQwfGqPcbF1tks3ZZIJIJpcUs/39PT02prawumi+Ufqxqfw+3bt+vs2bNKJBK6ceNG6NcBAJ4+hBpgHc3Nzam3t1ee52lhYUE7d+5c8US9q6tL9+/fD6beJBKJZb+J7urq0u3bt4OpOJ9++mnJxdzNzc0aHh4uOT3K3++hQ4cq6ltTU5Pa2tqUSCSUTCZ18uRJ5XI5SdJrr71WEGzC9mmlNrXW0tKinp4edXZ2Lnvuk08+UUtLS8nX/vrXv9apU6fkeZ7m5+f17rvv6sKFC5KeTKsaHh6WpGB6led5cl1XqVRqxWls/pjkB4D1GONia7TytyWTSbmuq/n5eSUSCV26dClol0gkdODAAZ0/f17GGH3nO99RU1NTydC22s/h7t27JUkfffRRRa8DADxlDIAVSTJjY2MVtV/6v9fs7KyRZIaHh0u2mZqaMpKM53nBtpmZGSPJxONxY4wx8Xi8aBvXdZftN5VKBa8rZWpqyriua3K5XOj+letnKpUykkx/f3/oPoVpU+xYldRZyfvnvya/tpmZmYI+Tk1Nla0rFouZnp6egv0tbdfT0xP0u7+/v6D/S1+XSqWMMcbkcjkTi8UKalrPMV7ttlJt/M/FUuU+hyu996v9bIyNja36MwUA2FDG+dscCKEaoWbp9nInuflyuZyRFIQW13VDneDNzMwUnFSX4rpuwQl7JcL0M0yfwrSJKtT4f84fy1gsVvBcubrm5+dNf39/0Xae5wV9nJ2dLVnH0kcsFgtCjjHrO8ar3VbseOXGqtznkFADAFgBoQYIo1ahZrWvK9bWv6JTLrDE4/HgytFqrKXearUJW+daQo0/lvPz88bzvIKrX+XqGh4eDgJLqXYrvU9h+r2eY7zabf4VO3+sll7By7fS57DcGPjBLD9ohkWoAYCnxjhraoAaK7cQ2l88XWzdjf86v006nS57nI6ODsViMe3Zs6fo/tLptG7fvq2TJ0+Grr0SS+sN06dybaL0ox/9SNKTmwNMT08HP5czOjqqU6dO6b333tOuXbuKtslms1pYWFB/f3/J9ymMjTjGLS0tmpyc1MLCQnCjhHg8rt7e3oJ2a/0c3rp1S5L05ptvrrlmAIC9CDVAjfghZN++fSXbHD16VNKTGwz4/AXUhw8flvTVyenQ0FDwnP8lhkudO3dOrusGC9R92WxW169fL/j+kXQ6XXQflfJvEOAv+A7TpzBtorRjxw7FYjF1dnZqYWEhuOVzOf7NBcq1vXr1qnp7e9Xd3V30fQprI45xIpHQ3r171dvbK2OMJicng9s2+9b6Ocxms7p8+bJc19X+/furWj8AwDJRXysCbKAKpy/56178xeSe5xnXdYOpN/5aCqlw4XYulzOu6xrXdYPt8Xi8YD2Hvy//9frfeo/Z2dmC/foLrufn54301Q0Kir3ef0xOTlY8LsqbYuQvYvfXaITt00ptSo1XJXVW8v75x8s/lj99Kn8tS7m6/DGen58vmH7meV4wTvmL4otNo/K3rdTv9Rrjlbb59Rers9jny/+sep4X+nOYv+/88UqlUsv6UymmnwHAU4M1NUAYlZ4UG/PV3Zz8Ezk/4Pj7y3/k8zzPDA8PFwSGpXeE8jwvuANWLBYLFpkX269/16v8k8pSJ5ylFquXGxf/RN/v6/DwcNF6w/SpVJty4xW2zrDvX7Fx8RW7m1mptn4IisViwfvV09MThMyl7UvtL2zf12OM17It/zNRLNiE+RyWG4f+/v5V3+DCR6gBgKfGuGPMCl+IAECO42hsbEzt7e1Rl7Kh+N83stH/GuH9q707d+7oa1/72rLpd3fu3NFrr722IT4z4+PjOnLkyIaoBQCwJhOsqQEAVNXo6Kh27dpVdD1RU1OT4vF4BFUBAJ5m9VEXAMBO+XfRymaz2r59e4TVYCP54IMPdP/+ff3kJz8pCDZ37tzRJ598sm533AMAbF5cqQGwjOM4Kz6ampqC9vl/Bq5evarnnntOly5dCj4vfX19unfvHoEGALAuuFIDYBnWGGAttm7dqo6ODnV0dGhwcDDqcgAAmwBXagAAAABYjVADAAAAwGqEGgAAAABWI9QAAAAAsBqhBgAAAIDVCDUAAAAArEaoAQAAAGA1Qg0AAAAAqxFqAAAAAFiNUAMAAADAaoQaAAAAAFYj1AAAAACwGqEGAAAAgNUcY4yJughgo3McR62trXr55ZejLgWrcO3aNd4/LHPv3j0lk0nxzyAAWG+CUAOEcPjw4ahLQI385S9/kSS9/vrrEVeCWpmYmIi6BADA2hBqACBfe3u7JGl8fDziSgAAQEgTrKkBAAAAYDVCDQAAAACrEWoAAAAAWI1QAwAAAMBqhBoAAAAAViPUAAAAALAaoQYAAACA1Qg1AAAAAKxGqAEAAABgNUINAAAAAKsRagAAAABYjVADAAAAwGqEGgAAAABWI9QAAAAAsBqhBgAAAIDVCDUAAAAArEaoAQAAAGA1Qg0AAAAAqxFqAAAAAFiNUAMAAADAaoQaAAAAAFYj1AAAAACwGqEGAAAAgNUINQAAAACsRqgBAAAAYDVCDQAAAACrEWoAAAAAWI1QAwAAAMBqhBoAAAAAViPUAAAAALAaoQYAAACA1Qg1AAAAAKxGqAEAAABgtfqoCwCAqPzhD3/Q7373Oz1+/DjYNjs7K0n68Y9/HGzbsmWLfvGLX+jYsWO1LhEAAITgGGNM1EUAQBTS6bS+//3vh2qbSqXU0tKyzhUBAIBVmGD6GYBNq6WlRa+99tqK7Zqbmwk0AABsYIQaAJtaV1eXGhoaSj7f0NCgt99+u4YVAQCASjH9DMCmNjc3p+bmZpX7q/Cvf/2rmpuba1gVAACoANPPAGxur776qn7wgx/IcZxlzzmOo927dxNoAADY4Ag1ADa9EydOqK6ubtn2uro6nThxIoKKAABAJZh+BmDTy2azeumllwpu7Sw9uZXzwsKCvvWtb0VUGQAACIHpZwCwfft27d27t+BqTV1dnfbt20egAQDAAoQaANCTu6CF2QYAADYepp8BgKQvvvhCL774oh4+fCjpya2cs9msvvGNb0RcGQAAWAHTzwBAkp5//nn99Kc/VX19verr63Xo0CECDQAAliDUAMD/HD9+XI8ePdKjR4907NixqMsBAAAh1UddALAZjY+PR10Cinj48KEaGxtljNGXX37J+7RBtbe3R10CAGCDYU0NEIFiX/QIIBz+2QIALMGaGiAqY2NjMsbw2GCPjz/+WH/84x+ruk/e7+o8xsbGIv6/FgCwUTH9DADyHDx4MOoSAABAhQg1AJCnvp6/FgEAsA3TzwAAAABYjVADAAAAwGqEGgAAAABWI9QAAAAAsBqhBgAAAIDVCDUAAAAArEaoAQAAAGA1Qg0AAAAAqxFqAAAAAFiNUAMAAADAaoQaAAAAAFYj1AAAAACwGqEG2OCy2axGR0fV1ta2pjbrdWwAAICoEWqADe7ChQvq7OxUIpFYU5vV6O7uDrXfbDarvr4+OY4jx3E0Ojpa1TqisLi4qGQyqZGRkUhDXTKZLBjbvr4+pdNpZbNZOY5T83oymYxOnz4tx3F0+vRpTU9PFzzv11nsMTAwoEQiocXFxZrXDQB4uhFqgA1ucHCwKm1WY3JycsU22WxWc3Nzunjxoowxisfj6uzs1MDAwLrUVCv9/f368MMPderUqaqHxbD6+vp05coVdXV1yRgjY4zOnDmjTCajpqammtezuLiodDqtwcFB5XI57du3TwcOHCgYH2OMPM8Lfs7lckHtBw8e1MjIiLq6upTNZmtePwDg6eUYY0zURQCbjeM4GhsbU3t7e+j20pMTxrW0WY2V9ptMJtXa2lqTWqJQjb5U+n5LCq7IlAqWyWRSe/bsqekYJxIJua5bsK3U+JTans1m1d3dLUm6evWqtm7dGvr44+PjOnLkyFPxuQIAVNUEV2oAi2SzWQ0MDARTfzKZzIqvWVxc1OjoaDAFaGRkZNlvyYu1KWV6erpgStHSQONPLYrFYqvo4cq1lOvP0jVAiURCjuOora1NmUxGyWRy2ZQonz+ujuOEGtf1lEwm9e677+r8+fMl2xQb9/Uel5aWlqK19PT0hO7b9u3bdfbsWSUSCd24cSP06wAAKIdQA1hkbm5Ovb298jxPCwsL2rlz54rTeLq6unT//v1gWlAikVB3d3fBuoauri7dvn07mCb06aefqq+vr+j+mpubNTw8LM/zlv3GPJPJqL+/P9jnaqxUS7n+5K8BSiaTcl1X8/PzSiQSunTpklpbWzU1NSXpSejKr7+3t1exWEypVEo7duxYVe3V8uGHH0qSXn311bLt8uuPYlz8z9ChQ4cq6t/u3bslSR999FFFrwMAoCQDoOYkmbGxsYraL/3fdXZ21kgyw8PDJdtMTU0ZScbzvGDbzMyMkWTi8bgxxph4PF60jeu6y/abSqWC1y01Pz8ftJVk+vv7Q/fPt1ItYfpTbByWbovFYkaSyeVywbZcLmdisdiymortr1LVeL/LiWJc/OO6rlvQPmwfVjOuY2Nja34vAABPpXGu1ACW2rVrlyTp1KlTJdtMTExIejLlx/f6669Lkj744IOC/+a3aW1tXbaWI5lMamhoSB0dHUWPtWPHDhljlEqlFIvFdO7cubLT2IpZqZYw/QnjrbfekiR9/PHHwbZbt24F220T1bhcvnxZ58+fr2hdDAAA64FQAzzFhoaGlm3zT0D9O1aFvbPX3bt3NTQ0pGQyWbZdS0tLMPWsXOAqZqVawvQnjJaWFrmuW3DC/6c//ankmpFa89eohL31cRTjMjo6Ktd1l63tCWOt664AAFiKUANYrtwibf9OVcXW3fiv89uk0+myx+no6FAsFtOePXtWXMfjX0Wq1Eq1hOlPWEePHg3WmGQyGf3whz+ssNr1469RuXv3bqj2tR6XdDqt27dv6+TJkxXt23fr1i1J0ptvvrmq1wMAsBShBrCUf+K/b9++km2OHj0q6ckNBnz+b8kPHz4s6asT4qGhoeA5/wsWlzp37pxc19WFCxfK1ubvJx6Ph+qLb6VawvQnrP3790uSrly5ops3b2rv3r0VvX49ua4r13WLXoHxZTKZ4LuAajku2WxW169f18WLF4Nt6XS66OelmGw2q8uXL8t13eBYAACsWdSreoDNSBUuHHdd10gyU1NTxhhjPM8zrusGi/E9zwsWXucvFs/lcsZ1XeO6brA9Ho+bnp6eoI2/L//1kkxPT4+ZnZ0t2K+/GNy/IYB/gwK/jvn5+eCYsVis5OLycsrVEqY/xerN5XJFx8aYrxbGl7qpQf5riy2GD6vS99uYr8Yiv/+++fn5gjGo1bgUe3/8x+TkZNCu1LilUqlldVaCGwUAAEoY518HIAKrOcn17zTln+j7AcffX/4jn+d5Znh4OHguHo8vO0H3PC84kY3FYsFJdLH9+nfayj+Zzf+5v7/fzMzMrGZYytYSpj/F6i03NqlUykhadoxiryv2+rBW834b8yQcTE5Omp6enuD4ruua4eHhIET6ajEu+XUsfZT6zFTzs0GoAQCUMO4Yw1czA7W2mm+Yh714v6tjfHxcR44cWfb9SACATW+CNTUAAAAArEaoAQAAAGC1+qgLAPD0cxwnVDumFQEAgNUg1ABYd4QVAACwnph+BgAAAMBqhBoAAAAAViPUAAAAALAaoQYAAACA1Qg1AAAAAKxGqAEAAABgNUINAAAAAKsRagAAAABYjVADAAAAwGqEGgAAAABWI9QAAAAAsBqhBgAAAIDVCDUAAAAArFYfdQHAZjUzMxN1Cagh3u+1YwwBAKU4xhgTdRHAZuM4TtQlANbiny0AwBITXKkBIsBJ2cbV3t4uSRofH4+4EgAAEBZragAAAABYjVADAAAAwGqEGgAAAABWI9QAAAAAsBqhBgAAAIDVCDUAAAAArEaoAQAAAGA1Qg0AAAAAqxFqAAAAAFiNUAMAAADAaoQaAAAAAFYj1AAAAACwGqEGAAAAgNUINQAAAACsRqgBAAAAYDVCDQAAAACrEWoAAAAAWI1QAwAAAMBqhBoAAAAAViPUAAAAALAaoQYAAACA1Qg1AAAAAKxGqAEAAABgNUINAAAAAKsRagAAAABYjVADAAAAwGqEGgAAAABWI9QAAAAAsBqhBgAAAIDVCDUAAAAArEaoAQAAAGA1Qg0AAAAAqxFqAAAAAFitPuoCACAqf/7zn5VOpwu2zc3NSZKGh4cLtr/xxhtqbW2tWW0AACA8Qg2ATSubzeqXv/yl6urqtGXLkwvXxhhJ0jvvvCNJevz4sR49eqTJycnI6gQAAOU5xv8XHAA2mYcPH+rFF1/UF198Ubbdc889p88//1yNjY01qgwAAFRggjU1ADathoYGdXR0lA0rDQ0N6uzsJNAAALCBEWoAbGqdnZ168OBByecfPnyoo0eP1rAiAABQKaafAdjUHj9+rG9/+9vyPK/o89u2bdO//vWvYM0NAADYcJh+BmBz27Jli44fP150elljY6N+/vOfE2gAANjg+JcawKZXagragwcP1NnZGUFFAACgEkw/AwBJzc3N+vvf/16wbefOnbp79240BQEAgLCYfgYAknT8+HE1NDQEPzc2Nurtt9+OsCIAABAWV2oAQNLf/vY3ffe73y3YNjs7q127dkVUEQAACIkrNQAgPZl+9sYbb8hxHDmOozfeeINAAwCAJQg1APA/J06cUF1dnerq6nTixImoywEAACEx/QwA/ucf//iHXnnlFRljlMlk9PLLL0ddEgAAWNlEfdQVAJuR4zhRl4AVvPLKK1GXgBL4XRwAYClCDRCRs2fPas+ePVGXgSWuX78ux3F04MCBqu3zyJEjvN9VMDMzo8uXL0ddBgBgAyLUABHZs2eP2tvboy4DS/hh5oUXXqjaPo8cOcL7XSWEGgBAMYQaAMhTzTADAABqg7ufAQAAALAaoQYAAACA1Qg1AAAAAKxGqAEAAABgNUINAAAAAKsRagAAAABYjVADAAAAwGqEGgAAAABWI9QAAAAAsBqhBgAAAIDVCDUAAAAArEaoAQAAAGA1Qg0AAAAAqxFqgA0um81qdHRUbW1ta2qzXscGAACIGqEG2OAuXLigzs5OJRKJNbVZje7u7lD7zWaz6uvrk+M4chxHo6OjVa0jCplMRqdPn5bjODp9+rSmp6cjqSOZTBaMbV9fn9LptLLZrBzHqXk9K42LX2exx8DAgBKJhBYXF2teNwDg6UaoATa4wcHBqrRZjcnJyRXbZLNZzc3N6eLFizLGKB6Pq7OzUwMDA+tSUy0sLi4qnU5rcHBQuVxO+/bt04EDB6oeGlfS19enK1euqKurS8YYGWN05swZZTIZNTU11bQWKdy4GGPkeV7wcy6XC2o/ePCgRkZG1NXVpWw2W/P6AQBPL8cYY6IuAthsHMfR2NiY2tvbQ7eXnpwwrqXNaqy032QyqdbW1prUUiuJREKu6xZsW0ufKn2/JQVXZEoFy2QyqT179tR0jCsZl1Lbs9msuru7JUlXr17V1q1bQx9/fHxcR44csfZzBQBYNxNcqQEsks1mNTAwEEz9yWQyK75mcXFRo6OjwRSgkZGRZb8lL9amlOnp6YIpRUsDjT+1KBaLraKHK9dSrj9L1wAlEgk5jqO2tjZlMhklk8llU6J8/rg6jqOWlpaitfX09KyqT5VKJpN69913df78+ZJtio27DeOyfft2nT17VolEQjdu3Aj9OgAAyiHUABaZm5tTb2+vPM/TwsKCdu7cueI0nq6uLt2/fz+YFpRIJNTd3V2wrqGrq0u3b98Opgl9+umn6uvrK7q/5uZmDQ8Py/O8Zb8xz2Qy6u/vD/a5GivVUq4/+WuAksmkXNfV/Py8EomELl26pNbWVk1NTUl6Erry6+/t7VUsFlMqldKOHTsKavLH6tChQ6vqU6U+/PBDSdKrr75atl1+/TaNy+7duyVJH330UUWvAwCgJAOg5iSZsbGxitov/d91dnbWSDLDw8Ml20xNTRlJxvO8YNvMzIyRZOLxuDHGmHg8XrSN67rL9ptKpYLXLTU/Px+0lWT6+/tD98+3Ui1h+lNsHJZui8ViRpLJ5XLBtlwuZ2KxWNG6pqamjOu6Be0rUY33u5yNOC4r9aHSPhpjzNjYWMWvAQBsCuNcqQEstWvXLknSqVOnSraZmJiQ9GTKj+/111+XJH3wwQcF/81v09raumwtRzKZ1NDQkDo6Oooea8eOHTLGKJVKKRaL6dy5c2WnsRWzUi1h+hPGW2+9JUn6+OOPg223bt0Kti91+fJlnT9/vqL1H7XEuAAANjtCDfAUGxoaWrbNPwH171gV9o5ed+/e1dDQkJLJZNl2LS0twdSzcoGrmJVqCdOfMFpaWuS6bsEJ/5/+9Keia0ZGR0fluu6yNSzryV+jEvbWx7aNy1rXXQEAsBShBrBcuUXa/p2qiq278V/nt0mn02WP09HRoVgspj179qy4jse/ilSplWoJ05+wjh49GqwxyWQy+uEPf7isTTqd1u3bt3Xy5MmK9r1W/hqVu3fvhmpv27jcunVLkvTmm2+u6vUAACxFqAEs5Z/479u3r2Sbo0ePSnpygwGf/1vyw4cPS/rqhHhoaCh4zv+CxaXOnTsn13V14cKFsrX5+4nH46H64lupljD9CWv//v2SpCtXrujmzZvau3dvwfPZbFbXr1/XxYsXg23pdLrouFSb67pyXbfoFRhfJpMJvgvIpnHJZrO6fPmyXNcNjgUAwJpFvaoH2IxU4cJx13WNJDM1NWWMMcbzPOO6brAY3/O8YOF1/mLxXC5nXNc1rusG2+PxuOnp6Qna+PtS3iL/np4eMzs7W7BffzG4f0MA/wYFfh3z8/PBMWOxWMnF5eWUqyVMf4rVm8vlio6NMV8tjF96U4NidfiPycnJivtV6fudX0N+/33z8/MFY7DRxiV/3/k3EUilUsvqrAQ3CgAAlDDOvw5ABFZzkuvfaco/0fcDjr+//Ec+z/PM8PBw8Fw8Hl92tyrP84IT2VgsFpxEF9uvf6et/JPZ/J/7+/vNzMzMaoalbC1h+lOs3nJjk0qljKRlx+jp6Sl64l6sbRireb+NeRIOJicnC+pxXdcMDw8HIdK3Ucal1PPV+GwQagAAJYw7xvDVzECtreYb5mEv3u/qGB8f15EjR5Z9PxIAYNObYE0NAAAAAKsRagAAAABYrT7qAgA8/RzHCdWOaUUAAGA1CDUA1h1hBQAArCemnwEAAACwGqEGAAAAgNUINQAAAACsRqgBAAAAYDVCDQAAAACrEWoAAAAAWI1QAwAAAMBqhBoAAAAAViPUAAAAALAaoQYAAACA1Qg1AAAAAKxGqAEAAABgNUINAAAAAKs5xhgTdRHAZuM4TtQlANbiny0AwBIT9VFXAGxGY2NjUZeAEn77299Kkv7v//4v4koAAEBYXKkBgDzt7e2SpPHx8YgrAQAAIU2wpgYAAACA1Qg1AAAAAKxGqAEAAABgNUINAAAAAKsRagAAAABYjVADAAAAwGqEGgAAAABWI9QAAAAAsBqhBgAAAIDVCDUAAAAArEaoAQAAAGA1Qg0AAAAAqxFqAAAAAFiNUAMAAADAaoQaAAAAAFYj1AAAAACwGqEGAAAAgNUINQAAAACsRqgBAAAAYDVCDQAAAACrEWoAAAAAWI1QAwAAAMBqhBoAAAAAViPUAAAAALAaoQYAAACA1Qg1AAAAAKxGqAEAAABgNUINAAAAAKsRagAAAABYjVADAAAAwGqEGgAAAABWI9QAAAAAsFp91AUAQFT+85//6MsvvyzY9uDBA0nSv//974LtzzzzjJ599tma1QYAAMIj1ADYtH7/+9/rnXfeKfrcN7/5zYKf33vvPf3qV7+qRVkAAKBCjjHGRF0EAEThs88+00svvaRHjx6VbVdXV6d//vOf2rZtW40qAwAAFZhgTQ2ATWvbtm3av3+/6urqSrapq6vTgQMHCDQAAGxghBoAm9rx48dV7oK1MUbHjx+vYUUAAKBSTD8DsKndv39f27ZtW3bDAF9jY6M+++wzPf/88zWuDAAAhMT0MwCb23PPPaef/exnamhoWPZcfX292traCDQAAGxwhBoAm96xY8f03//+d9n2R48e6dixYxFUBAAAKsH0MwCb3oMHD/Tiiy/q/v37Bdu//vWv6/PPP9czzzwTUWUAACAEpp8BQGNjo9566y01NjYG2xoaGtTe3k6gAQDAAoQaAJB09OhRPXjwIPj54cOHOnr0aIQVAQCAsJh+BgCSHj9+rKamJn3++eeSpBdeeEGe55X9DhsAALAhMP0MACRpy5YtOnbsmBobG9XQ0KDhAcFpAAAgAElEQVTjx48TaAAAsAShBgD+p7OzUw8ePGDqGQAAlqmPugBgMzp8+HDUJaCEZ599VpL0m9/8JuJKUMrExETUJQAANhiu1AARuHbtmu7duxd1GShi586d2rlzZ1X3yftdHffu3dO1a9eiLgMAsAFxowAgAo7jaGxsTO3t7VGXgiVu374tSfre975XtX3yflfH+Pi4jhw5Iv7ZAgAsMcH0MwDIU80wAwAAaoPpZwAAAACsRqgBAAAAYDVCDQAAAACrEWoAAAAAWI1QAwAAAMBqhBoAAAAAViPUAAAAALAaoQYAAACA1Qg1AAAAAKxGqAEAAABgNUINAAAAAKsRagAAAABYjVADAAAAwGqEGmCDy2azGh0dVVtb25rarNexAQAAokaoATa4CxcuqLOzU4lEYk1tVqO7uzvUfrPZrPr6+uQ4jhzH0ejoaFXriMJG6VMymSyoo6+vT+l0WtlsVo7j1LyeTCaj06dPy3EcnT59WtPT0wXP+3UWewwMDCiRSGhxcbHmdQMAnm6EGmCDGxwcrEqb1ZicnFyxTTab1dzcnC5evChjjOLxuDo7OzUwMLAuNdXCRulTX1+frly5oq6uLhljZIzRmTNnlMlk1NTUVNNaJGlxcVHpdFqDg4PK5XLat2+fDhw4UBB6jTHyPC/4OZfLBbUfPHhQIyMj6urqUjabrXn9AICnl2OMMVEXAWw2juNobGxM7e3todtLT04Y19JmNVbabzKZVGtra01qqZVq96nS91tScEWmVLBMJpPas2dPTcc4kUjIdd2CbaXGpdT2bDar7u5uSdLVq1e1devW0McfHx/XkSNHrP1cAQDWzQRXagCLZLNZDQwMBFN/MpnMiq9ZXFzU6OhoMAVoZGRk2W/Ji7UpZXp6umBK0dKTf39qUSwWW0UPV66lXH+WrgFKJBJyHEdtbW3KZDJKJpPLpkT5/HF1HEff/va3q9qnSiWTSb377rs6f/58yTbFxn29x6WlpaVoLT09PaH7tn37dp09e1aJREI3btwI/ToAAMoh1AAWmZubU29vrzzP08LCgnbu3LniNJ6uri7dv38/mBaUSCTU3d1dsK6hq6tLt2/fDqYJffrpp+rr6yu6v+bmZg0PD8vzvGW/Mc9kMurv7w/2uRor1VKuP/lrgJLJpFzX1fz8vBKJhC5duqTW1lZNTU1JehJQ8uvv7e1VLBZTKpXSjh07qtqnSn344YeSpFdffbVsu/z6az0u0ldh79ChQxX1b/fu3ZKkjz76qKLXAQBQkgFQc5LM2NhYRe2X/u86OztrJJnh4eGSbaampowk43lesG1mZsZIMvF43BhjTDweL9rGdd1l+02lUsHrlpqfnw/aSjL9/f2h++dbqZYw/Sk2Dku3xWIxI8nkcrlgWy6XM7FYrOp98o+/1ve7nFqPS/5xXdctaB+2D5X20RhjxsbGKn4NAGBTGOdKDWCpXbt2SZJOnTpVss3ExISkJ1N+fK+//rok6YMPPij4b36b1tbWZWs5ksmkhoaG1NHRUfRYO3bskDFGqVRKsVhM586dKzuNrZiVagnTnzDeeustSdLHH38cbLt161aw3VeNPtVCrcfFd/nyZZ0/f76idTEAAKwHQg3wFBsaGlq2zT8B9e9YFfY20Hfv3tXQ0JCSyWTZdi0tLcE0rXKBq5iVagnTnzBaWlrkum7BCf+f/vSnkmtG1tKn1fDXqIS99XEU4zI6OirXdZet7Qmj1muUAABPP0INYLlyi7T9O1UVW3fjv85vk06nyx6no6NDsVhMe/bsWXEdj38VqVIr1RKmP2EdPXo0WGOSyWT0wx/+sGz71fZpNfw1Knfv3g3Vvtbjkk6ndfv2bZ08ebKifftu3bolSXrzzTdX9XoAAJYi1ACW8k/89+3bV7LN0aNHJT25wYDP/y354cOHJX11Qjw0NBQ853/B4lLnzp2T67q6cOFC2dr8/cTj8VB98a1US5j+hLV//35J0pUrV3Tz5k3t3bu3bPvV9mk1XNeV67pFr8D4MplM8L05tRyXbDar69ev6+LFi8G2dDpd9PNSTDab1eXLl+W6bnAsAADWLOpVPcBmpAoXjruuaySZqakpY4wxnucZ13WDheue5wULr/MXi+dyOeO6rnFdN9gej8dNT09P0Mbfl/IWxPf09JjZ2dmC/fqLwf3F8/4NCvw65ufng2PGYrGSi8vLKVdLmP4UqzeXyxUdG2O+Whi/9AYA1eyTMZW/335fXNct6L9vfn6+YAxqNS7F3h//MTk5GbTL33f+TQRSqdSyOivBjQIAACWM868DEIHVnOT6d5ryT/T9gOPvL/+Rz/M8Mzw8HDwXj8eX3a3K87zgRDYWiwUn0cX2699pK/9kNv/n/v5+MzMzs5phKVtLmP4Uq7fc2KRSKSNp2TGq3afVvN/GPAkHk5OTpqenJ6jFdV0zPDwcBC5fLcYlv46lj1KfmWqOI6EGAFDCuGMMX80M1NpqvmEe9uL9ro7x8XEdOXJk2fcjAQA2vQnW1AAAAACwGqEGAAAAgNXqoy4AwNPPcZxQ7ZhWBAAAVoNQA2DdEVYAAMB6YvoZAAAAAKsRagAAAABYjVADAAAAwGqEGgAAAABWI9QAAAAAsBqhBgAAAIDVCDUAAAAArEaoAQAAAGA1Qg0AAAAAqxFqAAAAAFiNUAMAAADAaoQaAAAAAFYj1AAAAACwmmOMMVEXAWw2juOotbVVL7/8ctSloAauXbvG+10F9+7dUzKZFP9sAQCWmCDUABE4fPhw1CWghL/85S+SpNdffz3iSlDKxMRE1CUAADYWQg0A5Gtvb5ckjY+PR1wJAAAIaYI1NQAAAACsRqgBAAAAYDVCDQAAAACrEWoAAAAAWI1QAwAAAMBqhBoAAAAAViPUAAAAALAaoQYAAACA1Qg1AAAAAKxGqAEAAABgNUINAAAAAKsRagAAAABYjVADAAAAwGqEGgAAAABWI9QAAAAAsBqhBgAAAIDVCDUAAAAArEaoAQAAAGA1Qg0AAAAAqxFqAAAAAFiNUAMAAADAaoQaAAAAAFYj1AAAAACwGqEGAAAAgNUINQAAAACsRqgBAAAAYDVCDQAAAACrEWoAAAAAWI1QAwAAAMBqhBoAAAAAViPUAAAAALAaoQYAAACA1Qg1AAAAAKxWH3UBABCVP/zhD/rd736nx48fB9tmZ2clST/+8Y+DbVu2bNEvfvELHTt2rNYlAgCAEBxjjIm6CACIQjqd1ve///1QbVOplFpaWta5IgAAsAoTTD8DsGm1tLTotddeW7Fdc3MzgQYAgA2MUANgU+vq6lJDQ0PJ5xsaGvT222/XsCIAAFAppp8B2NTm5ubU3Nyscn8V/vWvf1Vzc3MNqwIAABVg+hmAze3VV1/VD37wAzmOs+w5x3G0e/duAg0AABscoQbApnfixAnV1dUt215XV6cTJ05EUBEAAKgE088AbHrZbFYvvfRSwa2dpSe3cl5YWNC3vvWtiCoDAAAhMP0MALZv3669e/cWXK2pq6vTvn37CDQAAFiAUAMAenIXtDDbAADAxsP0MwCQ9MUXX+jFF1/Uw4cPJT25lXM2m9U3vvGNiCsDAAArYPoZAEjS888/r5/+9Keqr69XfX29Dh06RKABAMAShBoA+J/jx4/r0aNHevTokY4dOxZ1OQAAIKT6qAsAbDA+Ph51CaiBhw8fqrGxUcYYffnll7zvm0R7e3vUJQAA1og1NUAIxb6YEcDTgX8GAcB6rKkBwhobG5MxhoeFj0rev48//lh//OMfI6+Zx/o/xsbGIv5bBQBQLUw/A4A8Bw8ejLoEAABQIUINAOSpr+evRQAAbMP0MwAAAABWI9QAAAAAsBqhBgAAAIDVCDUAAAAArEaoAQAAAGA1Qg0AAAAAqxFqAAAAAFiNUAMAAADAaoQaAAAAAFYj1AAAAACwGqEGAAAAgNUINQAAAACsRqgBqiybzWp0dFRtbW1rarNex7bR09qvjaTUGPf19amvry+iqgAACIdQA1TZhQsX1NnZqUQisaY2q9Hd3R1qv9lsVn19fXIcR47jaHR0tKp1VNt6jVcx/pj4j2QyWbJtMplc1n696vAfbW1tGhkZUTabrdqxpNqOsS+Tyej06dNyHEenT5/W9PR0wfOlxsBxHA0MDCiRSGhxcbFm9QIANi5CDVBlg4ODVWmzGpOTkyu2yWazmpub08WLF2WMUTweV2dnpwYGBtalpmpYr/Eqxhij+fn54OcrV66UbJv/nOd5MsZUtQ7P8wp+NsbovffeUyaTUVNTk+7cuVO145Ua44sXL+rixYtVO45vcXFR6XRag4ODyuVy2rdvnw4cOFAQqpaOQS6XC8bh4MGDGhkZUVdXV9UDHgDAPoQaYJOZm5tTa2tr8HNHR4ck6dy5c1GVtOHs2LFDktTf36+hoSFlMpllbTKZjJqbm4Oft2/fXvU6iu1zx44dOnPmjCTpt7/9bdWPWSs3btyQ67qSpK1btwafw6XT3/LHYOvWrcGfW1pa9P7770t6coWSKzYAsLkRaoB1lM1mNTAwEEyvKXZyvNTi4qJGR0eDaTbFphoVa1PK9PR0wbSd/EDj70uSYrFYxX1LJBLBSejIyEjQz6VXEFbbp6h/A3/w4EFJ0s2bN5c9d/PmzeD5YhYXF4MxcRxHfX19QX+KTVmrZBqbf6I/NDRUcLxqj3GxdTZLtyUSiWBa3NLP9/T0tNra2oLpYvnH8gPNUj09PWX7nm/79u06e/asEomEbty4Efp1AICnD6EGWEdzc3Pq7e2V53laWFjQzp07VzxR7+rq0v3794OpN4lEYtlvoru6unT79u1gKs6nn35acjF3c3OzhoeHi06PymQy6u/vD/ZZiaamJrW1tSmRSCiZTOrkyZPK5XKSpNdee60g2ITt00ptaq2lpUU9PT3q7Oxc9twnn3yilpaWkq/99a9/rVOnTsnzPM3Pz+vdd9/VhQsXJD2ZVjU8PCxJwfQqz/Pkuq5SqdSK09j8MckPAOsxxsXWaOVvSyaTcl1X8/PzSiQSunTpUtAukUjowIEDOn/+vIwx+s53vqOmpqaSoc2v4dChQ2X7vtTu3bslSR999FFFrwMAPGUMgBVJMmNjYxW1X/q/1+zsrJFkhoeHS7aZmpoykoznecG2mZkZI8nE43FjjDHxeLxoG9d1l+03lUoFr1tqfn4+aCvJ9Pf3h+5fuX6mUqmC/YXpU5g2xY5VSZ2VvH/+a/Jrm5mZKejj1NRU2bpisZjp6ekp2N/Sdj09PUG/+/v7C/q/9HWpVMoYY0wulzOxWKygpvUc49VuK9Wm1OdsamrKuK5rcrlcyTEoZbWfjbGxsVV/pgAAG8o4f5sDIVQj1CzdXu4kN18ulzOSgtDium6oE7yZmZmCk+pSUqlUcJLsB66wwvQzTJ/CtIkq1Ph/zh/LWCxW8Fy5uubn501/f3/Rdp7nBX2cnZ0tWcfSRywWC0KOMes7xqvdVux45cbKdd2C4Bj2dWGeL4VQAwBPDUINEEatQs1qX1esrX9Fp9SJYj7/KlKlJ3hrqbdabcLWuZZQ44/l/Py88Tyv4OpXubqGh4eDwFKq3UrvU5h+r+cYr3abf8XOH6ulV/DyxePxsoG63Bj4wSw/aIZFqAGAp8Y4a2qAGiu3ENpfPF1s3Y3/Or9NOp0ue5yOjg7FYjHt2bNnxXU8u3btKvv8aiytN0yfyrWJ0o9+9CNJT24OMD09HfxczujoqE6dOqX33nuv5Phms1ktLCyov78/1PtUykYc45aWFk1OTmphYSG4UUI8Hldvb29Bu3Q6rdu3b+vkyZOrOs6tW7ckSW+++eaaawYA2ItQA9SIH0L27dtXss3Ro0clPbnBgM9fQH348GFJX52cDg0NBc/5X2K41Llz5+S6brBAvRR/P/F4PFRfyvFvEOAv+A7TpzBtorRjxw7FYjF1dnZqYWEhuOVzOf7NBcq1vXr1qnp7e9Xd3R3qfSplI45xIpHQ3r171dvbK2OMJicng9s2+7LZrK5fv17wPTjpdLroZ7mYbDary5cvy3Vd7d+/v6r1AwAsE/W1IsAGqnD6kr/uxV9M7nmecV03mHrjr6WQChdu53I547qucV032B6PxwvWc/j78l+v/633mJ2dLdivv+DavyGAP73Hr2N+fj44ZiwWW9X0Hf9Y/hQjf1/+Go2wfVqpTanxqqTOSt4//3j5x/KnT+WvZSlXl/8ezc/PF0w/8zwvGKf8RfHFplH521bq93qN8Urb/PqL1Zn/+Vz6WfU8r+jn2H9MTk4WHYP88UqlUsv6UymmnwHAU4M1NUAYlZ4UG/PV3Zz8Ezk/4Pj7y3/k8zzPDA8PFwSGpXeE8jwvWNwfi8WCRebF9uvf9Sr/hDH/5/7+/lDrborx9+GfYPrhqVi9YfpUqk258QpbZ9j3r9hJtq/Y3cxKtfVDUCwWC96vnp6eZXedW2l/Yfu+HmO8lm35n4liwca/kUCxR6nPc7U+tz5CDQA8NcYdY1b4QgQAchxHY2Njam9vj7qUDcX/vpGN/tcI71/t3blzR1/72teWTb+7c+eOXnvttQ3xmRkfH9eRI0c2RC0AgDWZYE0NAKCqRkdHtWvXrqLriZqamqqydgsAgHz1URcAwE75d9HKZrPavn17hNVgI/nggw90//59/eQnPykINnfu3NEnn3yy6judAQBQCldqACzjOM6Kj6ampqB9/p+Bq1ev6rnnntOlS5eCz0tfX5/u3btHoAEArAuu1ABYhjUGWIutW7eqo6NDHR0dGhwcjLocAMAmwJUaAAAAAFYj1AAAAACwGqEGAAAAgNUINQAAAACsRqgBAAAAYDVCDQAAAACrEWoAAAAAWI1QAwAAAMBqhBoAAAAAViPUAAAAALAaoQYAAACA1Qg1AAAAAKxGqAEAAABgtfqoCwBsMTMzE3UJWAPePyzFZwIAnh6OMcZEXQSw0TmOE3UJANYJ/wwCgPUmuFIDhMBJz+bR3t4uSRofH4+4EgAAEBZragAAAABYjVADAAAAwGqEGgAAAABWI9QAAAAAsBqhBgAAAIDVCDUAAAAArEaoAQAAAGA1Qg0AAAAAqxFqAAAAAFiNUAMAAADAaoQaAAAAAFYj1AAAAACwGqEGAAAAgNUINQAAAACsRqgBAAAAYDVCDQAAAACrEWoAAAAAWI1QAwAAAMBqhBoAAAAAViPUAAAAALAaoQYAAACA1Qg1AAAAAKxGqAEAAABgNUINAAAAAKsRagAAAABYjVADAAAAwGqEGgAAAABWI9QAAAAAsBqhBgAAAIDVCDUAAAAArEaoAQAAAGA1Qg0AAAAAqxFqAAAAAFitPuoCACAqf/7zn5VOpwu2zc3NSZKGh4cLtr/xxhtqbW2tWW0AACA8Qg2ATSubzeqXv/yl6urqtGXLkwvXxhhJ0jvvvCNJevz4sR49eqTJycnI6gQAAOU5xv8XHAA2mYcPH+rFF1/UF198Ubbdc889p88//1yNjY01qgwAAFRggjU1ADathoYGdXR0lA0rDQ0N6uzsJNAAALCBEWoAbGqdnZ168OBByecfPnyoo0eP1rAiAABQKaafAdjUHj9+rG9/+9vyPK/o89u2bdO//vWvYM0NAADYcJh+BmBz27Jli44fP150elljY6N+/vOfE2gAANjg+JcawKZXagragwcP1NnZGUFFAACgEkw/AwBJzc3N+vvf/16wbefOnbp79240BQEAgLCYfgYAknT8+HE1NDQEPzc2Nurtt9+OsCIAABAWV2oAQNLf/vY3ffe73y3YNjs7q127dkVUEQAACIkrNQAgPZl+9sYbb8hxHDmOozfeeINAAwCAJQg1APA/J06cUF1dnerq6nTixImoywEAACEx/QwA/ucf//iHXnnlFRljlMlk9PLLL0ddEgAAWNlEfdQVAJuR4zhRl4AVvPLKK1GXgBL4XRwAYClCDRCRs2fPas+ePVGXgSWuX78ux3F04MCBqu3zyJEjvN9VMDMzo8uXL0ddBgBgAyLUABHZs2eP2tvboy4DS/hh5oUXXqjaPo8cOcL7XSWEGgBAMYQaAMhTzTADAABqg7ufAQAAALAaoQYAAACA1Qg1AAAAAKxGqAEAAABgNUINAAAAAKsRagAAAABYjVADAAAAwGqEGgAAAABWI9QAAAAAsBqhBgAAAIDVCDUAAAAArEaoAQAAAGA1Qg0AAAAAqxFqgA0um81qdHRUbW1ta2qzXscGAACIGqEG2OAuXLigzs5OJRKJNbVZje7u7lXtd2RkRI7jVLWWWltcXFQymdTIyEikoS6ZTKqvr0+O48hxHPX19SmdTiubzUYyxplMRqdPn5bjODp9+rSmp6cLnvfrLPYYGBhQIpHQ4uJizesGADzdCDXABjc4OFiVNqsxOTlZ8WvS6bROnTq1DtXUVn9/vz788EOdOnWq6mExrL6+Pl25ckVdXV0yxsgYozNnziiTyaipqanm9SwuLiqdTmtwcFC5XE779u3TgQMHCsbHGCPP84Kfc7lcUPvBgwc1MjKirq4uZbPZmtcPAHh6EWoAVM3i4qKuXbsWdRlVcfHiRV28eDGy4/tXZAYHB7Vr165g+/bt2+W6rmZmZmpe040bN+S6riRp69at6ujokKRlV7K2b98e/Hnr1q3Bn1taWvT+++9LenIVkCs2AIBqIdQAFslmsxoYGAim/mQymRVfs7i4qNHR0WAK0MjIyLLfkhdrU8r09HTBlKJ877//vs6cObO6zoWspVx/lq4BSiQSchxHbW1tymQySiaTy6ZE+fxxdRwn1Liup2QyqXfffVfnz58v2aa1tbXg51qMS0tLS9Faenp6Qvdt+/btOnv2rBKJhG7cuBH6dQAAlEOoASwyNzen3t5eeZ6nhYUF7dy5c8VpPF1dXbp//34wLSiRSCz7LXlXV5du374dTBP69NNP1dfXV3R/zc3NGh4elud5MsYE26enp/X//t//K/gt/WqsVEu5/uSvAUomk3JdV/Pz80okErp06ZJaW1s1NTUlSYrFYgX19/b2KhaLKZVKaceOHWvqw1p9+OGHkqRXX321bLv8+qMYF/8zdOjQoYr6t3v3bknSRx99VNHrAAAoyQCoOUlmbOz/s3d/oXGd+d3AfxP/2ZelWZduYu/uu40huFlCYdUSCHYX4iYOLA09uorlv0pZmqQy3UCKc1HMCF84kIvarC8WLOSUYkqjP87NakjSi8ihvojUgoN0YYq3rbFE/LYzG+hoDYW165z3wjmTGWkkjWR5Rsf6fGCwdeY5Z37PM2PrfHWe52hkRe3n/3O9du1aGhHp4ODgom3Gx8fTiEjL5XJt28TERBoR6dDQUJqmaTo0NNS0TZIkC447NTVV269euVyu1bFYLa1YrpZW+tPstedvKxaLaUSk1Wq1tq1arabFYnFBTavty/xj3O/7vZROjEv2ukmSNLRvtQ+rGdeRkZH7fi8AeCiNulIDOZWts1hqUf7FixcjonGNw9NPPx0REe+9917Dn/Vtdu/eveAmAZOTkzEwMFBbR1HvF7/4Rbz22mur6UaD5WpppT+tePnllyMi4qOPPqptu3LlSm173nRqXM6ePRsnTpxoWDcDAJ0g1MBDbGBgYMG27AQ0u2NVq3f2unHjRgwMDMTk5GTD9lKpFD/+8Y/vs9Kvj7WUVvrTiq6urkiSpOGE/5NPPll0zUi7ZWtUWl1I34lxGR4ejiRJFqztaUXWr2KxuOJ9AaAZoQZybqlF2tmdqpqtu8n2y9pMT08v+ToHDx6MYrEYe/bsaThed3d37Ny5s+ki85X+HpXlammlP606fPhwbY3J7OxsPPvssyva/0HK1qjcuHGjpfbtHpfp6em4evXqqq/OXblyJSIinn/++VXtDwDzCTWQU9mJ/969exdtc/jw4Yi4d4OBTPZT8v3790fE1yfEAwMDteeyX7A431tvvRVJksTJkydr29KvFvTXP+qfW4nlammlP6164YUXIiLiwoUL8emnn8Zzzz23ov0fpCRJIkmSpldgMrOzs3HmzJmIaO+4VCqV+Pjjjxtudz09Pd3089JMpVKJs2fPRpIktdcCgPvWyRU9sFHFCheOJ0mSRkQ6Pj6epum9hflJkqSnT5+ufR1fLbyuXyxerVbTJEnSJElq24eGhtK+vr5am+xY2f4Rkfb19aXXrl1rOG62GHxmZqbhBgWL9W81/70sVUsr/WlWb7VabTo2afr1wvhsHOer37fZYvhWrfT9TtOvx6K+/5mZmZmGMWjXuDR7f7LH2NhYrd1i4zY1NbWgzpVwowAAFjHquwN0wGpOcrM7TWUn+lnAyY5X/6iX3Zkse25oaGjBCXq5XK6dyBaLxdpJdLPjZnfaWiq4rDbULFVLK/1pVu9SYzM1NZVGxILXaLbf/fRpNe93mt4LB2NjY2lfX1/t9ZMkSQcHB9OZmZmGtu0Yl/o65j8W+8zUP06fPp1OTEyseBwyQg0AixgtpOkK54cA961QKMTIyEj09PR0uhTawPu9NkZHR+PAgQMrntYIwEPvojU1AABArgk1AABArm3udAHAw6/VWzubVgQArIZQAzxwwgoA8CCZfgYAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAORaIU3TtNNFwEZTKBQ6XQLklm9bAMxzcXOnK4CNaGRkpNMlsIif/exnERHxV3/1Vx2uBABolSs1AHV6enoiImJ0dLTDlQAALbpoTQ0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQjfbo2wAACAASURBVA0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrmztdAECn/M///E/85je/adh2+/btiIj47//+74bt3/jGN+Kb3/xm22oDAFon1AAb1t/93d/FT3/606bP/c7v/E7D1z//+c/jL//yL9tRFgCwQoU0TdNOFwHQCb/61a/iu9/9bty9e3fJdps2bYr//M//jMcff7xNlQEAK3DRmhpgw3r88cfjhRdeiE2bNi3aZtOmTbFv3z6BBgDWMaEG2NCOHj0aS12wTtM0jh492saKAICVMv0M2NBu3boVjz/++IIbBmS2bt0av/rVr+Jb3/pWmysDAFpk+hmwsT366KPxp3/6p7Fly5YFz23evDm6u7sFGgBY54QaYMM7cuRI/O///u+C7Xfv3o0jR450oCIAYCVMPwM2vNu3b8djjz0Wt27datj+W7/1W/HFF1/EN77xjQ5VBgC0wPQzgK1bt8bLL78cW7durW3bsmVL9PT0CDQAkANCDUBEHD58OG7fvl37+s6dO3H48OEOVgQAtMr0M4CI+PLLL2PHjh3xxRdfRETEt7/97SiXy0v+DhsAYF0w/QwgIuKRRx6JI0eOxNatW2PLli1x9OhRgQYAckKoAfjKoUOH4vbt26aeAUDObO50AbAR7d+/v9MlsIhvfvObERHxN3/zNx2uhMVcvHix0yUAsM64UgMd8P7778fnn3/e6TJoYufOnbFz5841Pab3e218/vnn8f7773e6DADWITcKgA4oFAoxMjISPT09nS6Fea5evRoREb//+7+/Zsf0fq+N0dHROHDgQPi2BcA8F00/A6izlmEGAGgP088AAIBcE2oAAIBcE2oAAIBcE2oAAIBcE2oAAIBcE2oAAIBcE2oAAIBcE2oAAIBcE2oAAIBcE2oAAIBcE2oAAIBcE2oAAIBcE2oAAIBcE2pgnatUKjE8PBzd3d331eZBvTYAQKcJNbDOnTx5Mg4dOhSlUum+2qzGq6++uqrjnj9/PgqFwprW0m6zs7Nx7NixKBQKcezYsbh06VJH6picnIz+/v4oFApRKBSiv78/pqeno1KpdGSMlxuXrM5mjzNnzkSpVIq5ubm21w3Aw02ogXXu3Llza9JmNcbGxla8z/T0dLz++usPoJr2mZubi+np6Th37lxUq9XYu3dv7Nu3b81D43L6+/vjwoUL0dvbG2maRpqm8cYbb8Ts7Gzs2LGjrbVEtDYuaZpGuVyufV2tVmu1v/jii3H+/Pno7e2NSqXS9voBeHgJNcCamZubi/fff7/TZdy3y5cvR5IkERGxbdu2OHjwYEREW6fhZVdkzp07F0899VRt+/bt2yNJkpiYmGhbLZlWx2X79u21v2/btq32966urnj33Xcj4t5VQFdsAFgrQg3kSKVSiTNnztSm/szOzi67z9zcXAwPD9emAJ0/f37BT8mbtVnMpUuXGqYU1Xv33XfjjTfeWF3nWqxlqf7MXwNUKpWiUChEd3d3zM7OxuTk5IIpUZlsXAuFQnR1dTWtra+v77761qrJycl4++2348SJE4u22b17d8PXeRmX7du3x5tvvhmlUikuX77c8n4AsBShBnLk+vXrcfz48SiXy3Hz5s3YuXPnstN4ent749atW7VpQaVSacFPyXt7e+Pq1au1aUKfffZZ9Pf3Nz3erl27YnBwMMrlcqRpWtt+6dKl+NGPftTwU/rVWK6WpfpTvwZocnIykiSJmZmZKJVK8c4778Tu3btjfHw8IiKKxWJD/cePH49isRhTU1PxxBNPNNSUjdVLL710X31r1QcffBAREU8++eSS7errz9O4PPPMMxER8eGHH65oPwBYVAq0XUSkIyMjK2o//5/rtWvX0ohIBwcHF20zPj6eRkRaLpdr2yYmJtKISIeGhtI0TdOhoaGmbZIkWXDcqamp2n71yuVyrY7FamnFcrW00p9mrz1/W7FYTCMirVartW3VajUtFotN6xofH0+TJGlovxJr8X4vZT2Oy3J9WM1nZGRkZFWfKwAeeqOu1EBOZessllqUf/HixYhoXOPw9NNPR0TEe++91/BnfZvdu3cvuEnA5ORkDAwM1NZR1PvFL34Rr7322mq60WC5WlrpTytefvnliIj46KOPatuuXLlS2z7f2bNn48SJEw3rQ9YT4wLARifUwENsYGBgwbbsBDS7Y1Wrd/S6ceNGDAwMxOTkZMP2UqkUP/7xj++z0q+PtZRW+tOKrq6uSJKk4YT/k08+abpmZHh4OJIkWbCG5UHK1qi0upA+b+OS9atYLK54XwBoRqiBnFtqkXZ2p6pm626y/bI209PTS77OwYMHo1gsxp49exqO193dHTt37my6yHylv0dluVpa6U+rDh8+XFtjMjs7G88+++yCNtPT03H16tU1uQq1EtkalRs3brTUPm/jcuXKlYiIeP7551e1PwDMJ9RATmUn/nv37l20zeHDhyPi3g0GMtlPyffv3x8RX58QDwwM1J7LfsHifG+99VYkSRInT56sbUu/WtBf/6h/biWWq6WV/rTqhRdeiIiICxcuxKeffhrPPfdcw/OVSiU+/vjjOHXqVG3b9PR003FZa0mSRJIkTa/AZGZnZ+PMmTMRka9xqVQqcfbs2UiSpPZaAHDfOrmiBzaqWOHC8SRJ0ohIx8fH0zS9tzA/SZL09OnTta/jq4XX9YvFq9VqmiRJmiRJbfvQ0FDa19dXa5MdK9s/ItK+vr702rVrDcfNFoPPzMw03KBgsf6t5r+XpWpppT/N6q1Wq03HJk2/XhifjeNSdWSPsbGxFfdrpe93fQ31/c/MzMw0jMF6G5f6Y9ffRGBqampBnSvhRgEALGLUdwfogNWc5GZ3mspO9LOAkx2v/lEvuzNZ9tzQ0NCCu1WVy+XaiWyxWKydRDc7bnanraWCy2pDzVK1tNKfZvUuNTZTU1NpRCx4jb6+vqYn7s3atmI173ea3gsHY2NjDfUkSZIODg6mMzMzDW3Xy7gs9nwWkiYmJlY8DhmhBoBFjBbSdIXzQ4D7VigUYmRkJHp6ejpdCm3g/V4bo6OjceDAgRVPawTgoXfRmhoAACDXhBoAACDXNne6AODh1+qtnU0rAgBWQ6gBHjhhBQB4kEw/AwAAck2oAQAAck2oAQAAck2oAQAAck2oAQAAck2oAQAAck2oAQAAck2oAQAAck2oAQAAck2oAQAAck2oAQAAck2oAQAAck2oAQAAcq2Qpmna6SJgoykUCrF79+74/ve/3+lSaIP333/f+70GPv/885icnAzftgCY56JQAx2wf//+TpfAIv71X/81IiKefvrpDlfCYi5evNjpEgBYX4QagHo9PT0RETE6OtrhSgCAFl20pgYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMi1zZ0uAKBT/uEf/iH+9m//Nr788svatmvXrkVExB//8R/Xtj3yyCPx53/+53HkyJF2lwgAtKCQpmna6SIAOmF6ejr+4A/+oKW2U1NT0dXV9YArAgBW4aLpZ8CG1dXVFT/4wQ+Wbbdr1y6BBgDWMaEG2NB6e3tjy5Ytiz6/ZcuW+MlPftLGigCAlTL9DNjQrl+/Hrt27Yql/iv8t3/7t9i1a1cbqwIAVsD0M2Bje/LJJ+MP//APo1AoLHiuUCjEM888I9AAwDon1AAb3iuvvBKbNm1asH3Tpk3xyiuvdKAiAGAlTD8DNrxKpRLf/e53G27tHHHvVs43b96M73znOx2qDABogelnANu3b4/nnnuu4WrNpk2bYu/evQINAOSAUAMQ9+6C1so2AGD9Mf0MICJ+/etfx2OPPRZ37tyJiHu3cq5UKvHbv/3bHa4MAFiG6WcAERHf+ta34k/+5E9i8+bNsXnz5njppZcEGgDICaEG4CtHjx6Nu3fvxt27d+PIkSOdLgcAaNHmThcAG9Ho6GinS6CJO3fuxNatWyNN0/jNb37jfVqnenp6Ol0CAOuMNTXQAc1+0SPQGt+2AJjHmhrolJGRkUjT1GOdPT766KP4x3/8xzU9pvd7bR4jIyMd/lcLwHpl+hlAnRdffLHTJQAAKyTUANTZvNl/iwCQN6afAQAAuSbUAAAAuSbUAAAAuSbUAAAAuSbUAAAAuSbUAAAAuSbUAAAAuSbUAAAAuSbUAAAAuSbUAAAAuSbUAAAAuSbUAAAAuSbUwDpXqVRieHg4uru776vNg3ptAIBOE2pgnTt58mQcOnQoSqXSfbVZjVdffXVVxz1//nwUCoU1raXdKpVK9Pf3R6FQiEKhEMPDwx2pY3JysqGO/v7+mJ6ejkql0pExnp2djWPHjkWhUIhjx47FpUuXGp7P6mz2OHPmTJRKpZibm2t73QA83IQaWOfOnTu3Jm1WY2xsbMX7TE9Px+uvv/4AqmmfSqUS169fj1OnTkWapjE0NBSHDh2KM2fOtLWO/v7+uHDhQvT29kaappGmabzxxhsxOzsbO3bsaGstERFzc3MxPT0d586di2q1Gnv37o19+/Y1hN40TaNcLte+rlartdpffPHFOH/+fPT29kalUml7/QA8vIQaYM3Mzc3F+++/3+ky7tv169dj9+7dta8PHjwYERFvvfVW22rIrsicO3cunnrqqdr27du3R5IkMTEx0bZaMpcvX44kSSIiYtu2bbVxmT89cfv27bW/b9u2rfb3rq6uePfddyPi3lVAV2wAWCtCDeRIpVKJM2fO1Kb+zM7OLrvP3NxcDA8P16YAnT9/fsFPyZu1WcylS5caphTVe/fdd+ONN95YXedarGWp/sxfA1QqlaJQKER3d3fMzs7G5OTkgilRmWxcC4VCfO9731vwmhERxWLxvvrWqsnJyXj77bfjxIkTi7apD10R7RmXrq6uprX09fW13Lft27fHm2++GaVSKS5fvtzyfgCwFKEGcuT69etx/PjxKJfLcfPmzdi5c+ey03h6e3vj1q1btWlBpVJpwU/Je3t74+rVq7VpQp999ln09/c3Pd6uXbticHAwyuVypGla237p0qX40Y9+1PBT+tVYrpal+lO/BmhycjKSJImZmZkolUrxzjvvxO7du2N8fDwi7gWU+vqPHz8exWIxpqam4oknnqhtn52djdOnT9deux0++OCDiIh48sknl2xXX3+7xyXi67D30ksvrah/zzzzTEREfPjhhyvaDwAWlQJtFxHpyMjIitrP/+d67dq1NCLSwcHBRduMj4+nEZGWy+XatomJiTQi0qGhoTRN03RoaKhpmyRJFhx3amqqtl+9crlcq2OxWlqxXC2t9KfZa8/fViwW04hIq9VqbVu1Wk2LxWLDfjMzM7V9IyI9ffr0ivuUvf79vt9Lafe41L9ukiQN7Vvtw2o+IyMjI6v6XAHw0Bt1pQZyKltnsdSi/IsXL0ZE4xqHp59+OiIi3nvvvYY/69vs3r17wU0CJicnY2BgoLaOot4vfvGLeO2111bTjQbL1dJKf1rx8ssvR0TERx99VNt25cqV2vbME088EWmaxtTUVBSLxXjrrbeWnJrXKe0el8zZs2fjxIkTDetmAKAThBp4iA0MDCzYlp2AZnesavV2zTdu3IiBgYGYnJxs2F4qleLHP/7xfVb69bGW0kp/WtHV1RVJkjSc8H/yySeLrhnp6uqqTT1rx53dsjUqrS6k78S4DA8PR5IkC9b2tKLda5QAePgJNZBzSy3Szu5U1WzdTbZf1mZ6enrJ1zl48GAUi8XYs2dPw/G6u7tj586dTReZr/T3qCxXSyv9adXhw4dra0xmZ2fj2WefXbJ9/R3IHrRsjcqNGzdaat/ucZmeno6rV6+u+urclStXIiLi+eefX9X+ADCfUAM5lZ347927d9E2hw8fjoh7NxjIZD8l379/f0R8fUI8MDBQey77BYvzvfXWW5EkSZw8ebK2Lf1qQX/9o/65lViullb606oXXnghIiIuXLgQn376aTz33HNLts9eZ2hoaEWvsxpJkkSSJE2vwGRmZ2drvzenneNSqVTi448/jlOnTtW2TU9PN/28NFOpVOLs2bORJEnttQDgvnVwQQ9sWLHCheNJkqQRkY6Pj6dpem9hfpIktYXr5XK5tvC6frF4tVpNkyRJkySpbR8aGkr7+vpqbbJjRd2C+L6+vvTatWsNx80Wg2eL5+tvDNCsf6v572WpWlrpT7N6q9Vq07FJ068Xxs+/AUA2tjMzM7VjFIvFRRfML2el73fWlyRJGvqfmZmZaRiDdo1Ls/cne4yNjdXa1R+7/iYCU1NTC+pcCTcKAGARo747QAes5iQ3u9NUdqKfBZzsePWPetmdybLnhoaGFtytqlwu105ki8Vi7SS62XGzO20tFVxWG2qWqqWV/jSrd6mxmZqaSiNiwWuMjY0tuOvZxMTEqvqT1bDS9ztN74WDsbGxtK+vr1ZLkiTp4OBgLXBl2jEu9XXMfyz2mVnLcRRqAFjEaCFNVzg/BLhvhUIhRkZGoqenp9Ol0Abe77UxOjoaBw4cWPG0RgAeehetqQEAAHJNqAEAAHJtc6cLAB5+rd7a2bQiAGA1hBrggRNWAIAHyfQzAAAg14QaAAAg14QaAAAg14QaAAAg14QaAAAg14QaAAAg14QaAAAg14QaAAAg14QaAAAg14QaAAAg14QaAAAg14QaAAAg14QaAAAg1zZ3ugDYqCYmJjpdAm3k/b5/xhCAxRTSNE07XQRsNIVCodMlQG75tgXAPBddqYEOcFK2fvX09ERExOjoaIcrAQBaZU0NAACQa0INAACQa0INAACQa0INAACQa0INAACQa0INAACQa0INAACQa0INAACQa0INAACQa0INAACQa0INAACQa0INAACQa0INAACQa0INAACQa0INAACQa0INAACQa0INAACQa0INAACQa0INAACQa0INAACQa0INAACQa0INAACQa0INAACQa0INAACQa0INAACQa0INAACQa0INAACQa0INAACQa0INAACQa0INAACQa0INAACQa0INAACQa0INAACQa5s7XQBAp/zzP/9zTE9PN2y7fv16REQMDg42bP/hD38Yu3fvblttAEDrhBpgw6pUKvEXf/EXsWnTpnjkkXsXrtM0jYiIn/70pxER8eWXX8bdu3djbGysY3UCAEsrpNl3cIAN5s6dO/HYY4/Fr3/96yXbPfroo/HFF1/E1q1b21QZALACF62pATasLVu2xMGDB5cMK1u2bIlDhw4JNACwjgk1wIZ26NChuH379qLP37lzJw4fPtzGigCAlTL9DNjQvvzyy/je974X5XK56fOPP/54/Nd//VdtzQ0AsO6YfgZsbI888kgcPXq06fSyrVu3xp/92Z8JNACwzvlODWx4i01Bu337dhw6dKgDFQEAK2H6GUBE7Nq1K/7jP/6jYdvOnTvjxo0bnSkIAGiV6WcAERFHjx6NLVu21L7eunVr/OQnP+lgRQBAq1ypAYiIf//3f4/f+73fa9h27dq1eOqppzpUEQDQIldqACLuTT/74Q9/GIVCIQqFQvzwhz8UaAAgJ4QagK+88sorsWnTpti0aVO88sornS4HAGiR6WcAX/l//+//xe/+7u9GmqYxOzsb3//+9ztdEgCwvIubO10B5EGhUOh0CbTZ7/7u73a6BNrEz/YA8k+ogRa9+eabsWfPnk6XwSocOHCg5ffv448/jkKhEPv27WtDZXTSxMREnD17ttNlALAGhBpo0Z49e6Knp6fTZbAKBw4caPn9y8LMt7/97QddFuuAUAPwcBBqAOoIMwCQP+5+BgAA5JpQAwAA5JpQAwAA5JpQAwAA5JpQAwAA5JpQAwAA5JpQAwAA5JpQAwAA5JpQAwAA5JpQAwAA5JpQAwAA5JpQAwAA5JpQAwAA5JpQA2usUqnE8PBwdHd331ebB/XaefSw9ms9WWyM+/v7o7+/v0NVAUBrhBpYYydPnoxDhw5FqVS6rzar8eqrr67quOfPn49CobCmtaylBzVezRQKhYbH5OTkom0nJycXtH9QdWSP7u7uOH/+fFQqlTV7rYj2jnFmdnY2jh07FoVCIY4dOxaXLl1qeH6xMSgUCnHmzJkolUoxNzfXtnoBWL+EGlhj586dW5M2qzE2Nrbifaanp+P1119/ANWsnQc1Xs2kaRozMzO1ry9cuLBo2/rnyuVypGm6pnWUy+WGr9M0jZ///OcxOzsbO3bsiF/+8pdr9nqLjfGpU6fi1KlTa/Y6mbm5uZieno5z585FtVqNvXv3xr59+xpC1fwxqFartXF48cUX4/z589Hb27vmAQ+A/BFqYAObm5uL999/v9NlrDtPPPFEREScPn06BgYGYnZ2dkGb2dnZ2LVrV+3r7du3r3kdzY75xBNPxBtvvBERET/72c/W/DXb5fLly5EkSUREbNu2LQ4ePBgRsWD6W/0YbNu2rfb3rq6uePfddyPi3hVKV2wANjahBh6gSqUSZ86cqU2vaXZyPN/c3FwMDw/Xptk0m2rUrM1iLl26tOj0qHfffbd2gryavpVKpdpJaDaF7dixYwuuIKy2T53+CfyLL74YERGffvrpguc+/fTT2vPNzM3N1cakUChEf39/rT/NpqytZBpbdqI/MDDQ8HprPcbN1tnM31YqlWrT4uZ/vi9duhTd3d216WL1r5UFmvn6+vqW7Hu97du3x5tvvhmlUikuX77c8n4APHyEGniArl+/HsePH49yuRw3b96MnTt3Lnui3tvbG7du3apNvSmVSgt+Et3b2xtXr16tTcX57LPPFl3MvWvXrhgcHFwwPerSpUvxox/9aNVXGHbs2BHd3d1RKpVicnIyXnvttahWqxER8YMf/KAh2LTap+XatFtXV1f09fXFoUOHFjz3T//0T9HV1bXovn/9138dr7/+epTL5ZiZmYm33347Tp48GRH3plUNDg5GRNSmV5XL5UiSJKamppadxpaNSX0AeBBj3GyNVv22ycnJSJIkZmZmolQqxTvvvFNrVyqVYt++fXHixIlI0zT+7//9v7Fjx45FQ1tWw0svvbRk3+d75plnIiLiww8/XNF+ADxkUmBZEZGOjIysqP38f17Xrl1LIyIdHBxctM34+HgaEWm5XK5tm5iYSCMiHRoaStM0TYeGhpq2SZJkwXGnpqZq+9Url8u1OharZbX9nJqaSiMiPX36dMt9aqXNamvM9l3J+5ftU1/bxMREQx/Hx8eXrKtYLKZ9fX0Nx5vfrq+vr9bv06dPN/R//n5TU1NpmqZptVpNi8ViQ00PcoxXu22xNtnnYr7x8fE0SZK0Wq0uOgaLWe1nY2RkZNWfKQDWlVH/m0ML1iLUzN++1EluvWq1mkZELbQkSdLSCd7ExETDSXW9+kCzVL3LaaWfrfSplTadCjXZ3+vHslgsNjy3VF0zMzPp6dOnm7Yrl8u1Pl67dm3ROuY/isViLeSk6YMd49Vua/Z6S41VkiQNwbHV/Vp5fjFCDcBDQ6iBVrQr1Kx2v2Ztsys6808Ux8bG0pmZmZZedzn3U+9atWm1zvsJNdlYzszMpOVyueHq11J1DQ4O1gLLYu0We59aOf5ybdZijFe7Lbtil43V/Ct49YaGhhYE7Vb6l6ZfB7P6oNkqoQbgoTFqTQ202VILobPF083W3WT7ZW2mp6eXfJ2DBw9GsViMPXv2NByvu7s7du7c2XRR+lr+npX59bbSp6XadNIf/dEfRcS9mwNcunSp9vVShoeH4/XXX4+f//zn8dRTTzVtU6lU4ubNm3H69OkF79NKrMcx7urqirGxsbh582btRglDQ0Nx/PjxhnbT09Nx9erVeO2111b1OleuXImIiOeff/6+awYgv4QaaJMshOzdu3fRNocPH46IezcYyGQLqPfv3x8RX5+cDgwM1J7LfonhfG+99VYkSVJboB4RtZsL1D/qn7tf2Q0CsgXfrfSplTad9MQTT0SxWIxDhw7FzZs3a7d8Xkp2c4Gl2v793/99HD9+PF599dUF79NKrMcxLpVK8dxzz8Xx48cjTdMYGxur3bY5U6lU4uOPP274PTjT09NNP8vNVCqVOHv2bCRJEi+88MKa1g9AznTwMhHkRqxw+lK27iVbTF4ul9MkSWpTb7K1FBGNC7er1WqaJEmaJElt+9DQUMN6juxY2f7x1XqPa9euNRw3W3A9MzOTRsSqp/csJdsvm2KULWLP1mi02qfl2iw2XiupcyXvX/Z69a+VTZ+qX8uyVF3ZezQzM9Mw/axcLtfGqX5RfLNpVNm25fr9oMZ4uW1Z/c3qrP98zv+slsvlpp/j7DE2NtZ0DOrHa2pqakF/Vsr0M4CHhjU10IqVnhSn6dd3c8pO5LKAkx2v/lEvuzNZfWCYf0eocrlcuwNWsVisLTJvdtzsrldLBZf7DTXZCWYWnprV20qfFmuz1Hi1Wmer71+zk+xMs7uZLdY2C0HFYrH2fvX19dVC5vz2ix2v1b4/iDG+n231n4lmwSa7kUCzx2Kf5/rH6dOnF12H1CqhBuChMVpI0zWYbwIPuUKhECMjI9HT09PpUtaVbA3Oev9vxPvXfr/85S/j//yf/7Ng+t0vf/nL+MEPfrAuPjOjo6Nx4MCBdVELAPflojU1AKyp4eHheOqpp5quJ9qxY0cMDQ11oCoAHmabO10AkE/1d9GqVCqxffv2DlbDevLee+/FrVu34sc//nFDsPnlL38Z//RP/7TqO50BwGJcqQEWqL/d82KPHTt21NrX/x3+/u//Ph599NF45513ap+X/v7++PzzztuP5wAAIABJREFUzwUaAB4IV2qABawx4H5s27YtDh48GAcPHoxz5851uhwANgBXagAAgFwTagAAgFwTagAAgFwTagAAgFwTagAAgFwTagAAgFwTagAAgFwTagAAgFwTagAAgFwTagAAgFwTagAAgFwTagAAgFwTagAAgFwrpGmadroIWO8KhUKnSwAeEN8GAXLv4uZOVwB5MDIy0ukSaJOf/exnERHxV3/1Vx2uBABolSs1AHV6enoiImJ0dLTDlQAALbpoTQ0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrmztdAECn/M///E/85je/adh2+/btiIj47//+74bt3/jGN+Kb3/xm22oDAFon1AAb1t/93d/FT3/606bP/c7v/E7D1z//+c/jL//yL9tRFgCwQoU0TdNOFwHQCb/61a/iu9/9bty9e3fJdps2bYr//M//jMcff7xNlQEAK3DRmhpgw3r88cfjhRdeiE2bNi3aZtOmTbFv3z6BBgDWMaEG2NCOHj0aS12wTtM0jh492saKAICVMv0M2NBu3boVjz/++IIbBmS2bt0av/rVr+Jb3/pWmysDAFpk+hmwsT366KPxp3/6p7Fly5YFz23evDm6u7sFGgBY54QaYMM7cuRI/O///u+C7Xfv3o0jR450oCIAYCVMPwM2vNu3b8djjz0Wt27datj+W7/1W/HFF1/EN77xjQ5VBgC0wPQzgK1bt8bLL78cW7durW3bsmVL9PT0CDQAkANCDUBEHD58OG7fvl37+s6dO3H48OEOVgQAtMr0M4CI+PLLL2PHjh3xxRdfRETEt7/97SiXy0v+DhsAYF0w/QwgIuKRRx6JI0eOxNatW2PLli1x9OhRgQYAckKoAfjKoUOH4vbt26aeAUDObO50AbAR7d+/v9MlsIhvfvObERHxN3/zNx2uhMVcvHix0yUAsM64UgMd8P7778fnn3/e6TJoYufOnbFz5841Pab3e218/vnn8f7773e6DADWITcKgA4oFAoxMjISPT09nS6Fea5evRoREb//+7+/Zsf0fq+N0dHROHDgQPi2BcA8F00/A6izlmEGAGgP088AAIBcE2oAAIBcE2oAAIBcE2oAAIBcE2oAAIBcE2oAAIBcE2oAAIBcE2oAAIBcE2oAAIBcE2oAAIBcE2oAAIBcE2oAAIBcE2oAAIBcE2pgnatUKjE8PBzd3d331eZBvTYAQKcJNbDOnTx5Mg4dOhSlUum+2qzGq6++2vJxp6eno1Ao1B7Hjh1b01rabW5uLiYnJ+P8+fMdDXWTk5PR399fG9f+/v6Ynp6OSqUShUKh7fXMzs7GsWPHau/xpUuXGp6v/wzMf5w5cyZKpVLMzc21vW4AHm5CDaxz586dW5M2qzE2NtZy23/5l39p+Pqll15a63La6vTp0/HBBx/E66+/vuZhsVX9/f1x4cKF6O3tjTRNI03TeOONN2J2djZ27NjR9nrm5uZieno6zp07F9VqNfbu3Rv79u1rGJ80TaNcLte+rlartdpffPHFOH/+fPT29kalUml7/QA8vIQaYE185zvfqZ28pmkaSZJ0uqT7curUqTh16lTHXj+7InPu3Ll46qmnatu3b98eSZLExMRE22u6fPly7X3dtm1bHDx4MCJiwZWs7du31/6+bdu22t+7urri3XffjYh7VwFdsQFgrQg1kCOVSiXOnDlTm/ozOzu77D5zc3MxPDxcmwJ0/vz5BT8lb9ZmMZcuXWqYUhRxb0pSd3d39Pf3x+Tk5H31cblalurP/DVApVIpCoVCdHd3x+zsbExOTi6YEpXJxrVQKLQ0rg/S5ORkvP3223HixIlF2+zevbvh63aMS1dXV9Na+vr6Wu7b9u3b480334xSqRSXL19ueT8AWIpQAzly/fr1OH78eJTL5bh582bs3Llz2Wk8vb29cevWrdq0oFKptOCn5L29vXH16tXaVZbPPvss+vv7mx5v165dMTg4GOVyOdI0jYh762kiIt5+++3Ys2dPdHd3r3p60XK1LNWf+jVAk5OTkSRJzMzMRKlUinfeeSd2794d4+PjERFRLBZr9UdEHD9+PIrFYkxNTcUTTzyxqtrXygcffBAREU8++eSS7err78S4ZJ+hlU41fOaZZyIi4sMPP1zRfgCwqBRou4hIR0ZGVtR+/j/Xa9eupRGRDg4OLtpmfHw8jYi0XC7Xtk1MTKQRkQ4NDaVpmqZDQ0NN2yRJsuC4U1NTtf3mq1ar6dTUVFosFhvqWonlammlP83GYf62rMZqtdpQf7FYXFBTs+Ot1Fq830vpxLhkr5skSUP7VvuwmnEdGRm57/cCgIfSqCs1kFPZOovXX3990TYXL16MiMY1Dk8//XRERLz33nsNf9a32b1794KbBExOTsbAwEBtHcV827Zti66urjh16lQMDg6uanH9crW00p9WvPzyyxER8dFHH9W2XblypbY9bzo1LmfPno0TJ040rJsBgE4QauAhNjAwsGBbdgKahY5Ww8eNGzdiYGCgpTUzPT09qwo1y+3TSn9a0dXVFUmSNJzwf/LJJ4uuGWm3bI1KqwvpOzEuw8PDkSTJgrU9rcj6VSwWV7wvADQj1EDOLbVIO7tTVbP1Ldl+WZtsXcxiDh48GMViMfbs2bPseplt27ataPH4/HoXq6WV/rTq8OHDtTUms7Oz8eyzz66w2gcnW6Ny48aNltq3e1ymp6fj6tWr8dprr63o2JkrV65ERMTzzz+/qv0BYD6hBnIqO/Hfu3fvom0OHz4cEfduMJDJfkq+f//+iPj6hHhgYKD2XPYLFud76623IkmSOHny5JK1zc3N1Y6/EsvV0kp/WvXCCy9ERMSFCxfi008/jeeee27F9T4oSZJEkiRNr8BkZmdn48yZMxHR3nGpVCrx8ccfN9zuenp6uuVftlqpVOLs2bORJEnttQDgvnV6VQ9sRLHCheNJkqQRkY6Pj6dpmqblcjlNkiQ9ffp07ev4auF1/WLxarWaJkmSJklS2z40NJT29fXV2mTHyvaPiLSvry+9du1aw3GzxeAzMzMNNwIYGhqq1ZU9PzY2tqpxWaqWVvrTrN5qtdp0bNL064Xx2TjOV79vs8XwrVrp+52mX49Fff8zMzMzDWPQrnFp9v5kj/r3fLFxm5qaWlDnSrhRAACLGPXdATpgNSe52Z2mshP9+iAx/wSzXrlcTgcHB2vPDQ0NLThBL5fLtRPZYrFYO4ludtzsTlv1J7PZ34vFYjo1NbWaIVm2llb606zepcZmamoqjYgFr9Fsv2b7t2o173ea3gsHY2NjaV9fX+31kyRJBwcH05mZmYa27RiX+jrmPxb7zNQ/Tp8+nU5MTKx4HDJCDQCLGC2kad0vJADaolAoxMjISPT09HS6FNrA+702RkdH48CBA+HbFgDzXLSmBgAAyDWhBgAAyLXNnS4AePgVCoWW2plWBACshlADPHDCCgDwIJl+BgAA5JpQAwAA5JpQAwAA5JpQAwAA5JpQAwAA5JpQAwAA5JpQAwAA5JpQAwAA5JpQAwAA5JpQAwAA5JpQAwAA5JpQAwAA5JpQAwAA5FohTdO000XARlMoFGL37t3x/e9/v9Ol0Abvv/++93sNfP755zE5ORm+bQEwz0WhBjpg//79nS6BRfzrv/5rREQ8/fTTHa6ExVy8eLHTJQCwvgg1APV6enoiImJ0dLTDlQAALbpoTQ0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrQg0AAJBrmztdAECn/MM//EP87d/+bXz55Ze1bdeuXYuIiD/+4z+ubXvkkUfiz//8z+PIkSPtLhEAaEEhTdO000UAdML09HT8wR/8QUttp6amoqur6wFXBACswkXTz4ANq6urK37wgx8s227Xrl0CDQCsY0INsKH19vbGli1bFn1+y5Yt8ZOf/KSNFQEAK2X6GbChXb9+PXbt2hVL/Vf4b//2b7Fr1642VgUArIDpZ8DG9uSTT8Yf/uEfRqFQWPBcoVCIZ555RqABgHVOqAE2vFdeeSU2bdq0YPumTZvilVde6UBFAMBKmH4GbHiVSiW++93vNtzaOeLerZxv3rwZ3/nOdzpUGQDQAtPPALZv3x7PPfdcw9WaTZs2xd69ewUaAMgBoQYg7t0FrZVtAMD6Y/oZQET8+te/jsceeyzu3LkTEfdu5VypVOK3f/u3O1wZALAM088AIiK+9a1vxZ/8yZ/E5s2bY/PmzfHSSy8JNACQE0INwFeOHj0ad+/ejbt378aRI0c6XQ4A0KLNnS4ANqLR0dFOl0ATd+7cia1bt0aapvGb3/zG+7RO9fT0dLoEANYZa2qgA5r9okegNb5tATCPNTXQKSMjI5Gmqcc6e3z00Ufxj//4j2t6TO/32jxGRkY6/K8WgPXK9DOAOi+++GKnSwAAVkioAaizebP/FgEgb0w/AwAAck2oAQAAck2oAQAAck2oAQAAck2oAQAAck2oAQAAck2oAQAAck2oAQAAck2oAQAAck2oAQAAck2oAQAAck2oAQAAck2ogXWuUqnE8PBwdHd331ebB/XaAACdJtTAOnfy5Mk4dOhQlEql+2qzGq+++mrLx52eno5CoVB7HDt2bE1rabfZ2dk4duxYrS+XLl3qSB2Tk5PR399fG9f+/v6Ynp6OSqUShUKh7fUsNy71n4H5jzNnzkSpVIq5ubm21w3Aw02ogXXu3Llza9JmNcbGxlpu+y//8i8NX7/00ktrXU7bzM3NxfT0dJw7dy6q1Wrs3bs39u3bt+ahcTn9/f1x4cKF6O3tjTRNI03TeOONN2J2djZ27NjR1loiWhuXNE2jXC7Xvq5Wq7XaX3zxxTh//nz09vZGpVJpe/0APLyEGmBNfOc736mdvKZpGkmSdLqkVbt8+XKt/m3btsXBgwcjIto6DS+7InPu3Ll46qmnatu3b98eSZLExMRE22rJtDou27dvr/1927Zttb93dXXFu+++GxH3rgK6YgPAWhFqIEcqlUqcOXOmNvVndnZ22X3m5uZieHi4NgXo/PnzC35K3qzNYi5dutQwpSji3pSk7u7u6O/vj8nJyfvq43K1LNWf+WuASqVSFAqF6O7ujtnZ2ZicnFwwJSqTjWuhUIiurq6mtfX19d1X31o1OTkZb7/9dpw4cWLRNrt37274Oi/jsn379njzzTejVCrF5cuXW94PAJYi1ECOXL9+PY4fPx7lcjlu3rwZO3fuXHYaT29vb9y6das2LahUKi34KXlvb29cvXq1dpXls88+i/7+/qbH27VrVwwODka5XI40TSPi3nqaiIi333479uzZE93d3aueXrRcLUv1p34N0OTkZCRJEjMzM1EqleKdd96J3bt3x/j4eEREFIvFWv0REcePH49isRhTU1PxxBNPNNSUjVW7ptR98MEHERHx5JNPLtmuvv48jcszzzwTEREffvjhivYDgEWlQNtFRDoyMrKi9vP/uV67di2NiHRwcHDRNuPj42lEpOVyubZtYmIijYh0aGgoTdM0HRoaatomSZIFx52amqrtN1+1Wk2npqbSYrHYUNdKLFdLK/1pNg7zt2U1VqvVhvqLxWLTusbHx9MkSRrar8RavN9LWY/jslwfVtrHNE3TkZGRFe8DwIYw6koN5FS2zuL1119ftM3FixcjonGNw9NPPx0REe+9917Dn/Vtdu/eveAmAZOTkzEwMFBbRzHftm3boqurK06dOhWDg4OrWlS/XC2t9KcVL7/8ckREfPTRR7VtV65cqW2f7+zZs3HixImG9SHriXEBYKMTauAhNjAwsGBbdgKahY5Ww8eNGzdiYGCgpTUzPT09qwo1y+3TSn9a0dXVFUmSNJzwf/LJJ03XjAwPD0eSJAvWsDxI2RqVVhfS521csn4Vi8UV7wsAzQg1kHNLLdLO7lTVbH1Ltl/WJlsXs5iDBw9GsViMPXv2LLteZtu2bataVL9cLa30p1WHDx+urTGZnZ2NZ599dkGb6enpuHr1arz22msrOvb9ytao3Lhxo6X2eRuXK1euRETE888/v6r9AWA+oQZyKjvx37t376JtDh8+HBH3bjCQyX5Kvn///oj4+oR4YGCg9lz2Cxbne+uttyJJkjh58uSStc3NzdWOvxLL1dJKf1r1wgsvRETEhQsX4tNPP43nnnuu4flKpRIff/xxnDp1qrZtenq6Lb9UNEmSSJKk6RWYzOzsbJw5cyYi8jUulUolzp49G0mS1F4LAO5bp1f1wEYUK1w4niRJGhHp+Ph4mqZpWi6X0yRJ0tOnT9e+jq8WXtcvFq9Wq2mSJGmSJLXtQ0NDaV9fX61Ndqxs/4hI+/r60mvXrjUcN1sMPjMz03AjgKGhoVpd2fNjY2OrGpelammlP83qrVarTccmTb9eGJ+N41J1ZI/V9G2l73d9DfX9z8zMzDSMwXobl/pj199EYGpqakGdK+FGAQAsYtR3B+iA1ZzkZneayk7064PE/BPMeuVyOR0cHKw9NzQ0tOBuVeVyuXYiWywWayfRzY6b3Wmr/mQ2+3uxWEynpqZWMyTL1tJKf5rVu9TYTE1NpRGx4DX6+vqanrg3a9uK1bzfaXovHIyNjTXUkyRJOjg4mM7MzDS0XS/jstjzWUiamJhY8ThkhBoAFjFaSNO6X0gAtEWhUIiRkZHo6enpdCm0gfd7bYyOjsaBAwfCty0A5rloTQ0AAJBrQg0AAJBrmztdAPDwKxQKLbUzrQgAWA2hBnjghBUA4EEy/QwAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMg1oQYAAMi1zZ0uADaqiYmJTpdAG3m/758xBGAxhTRN004XARtNoVDodAmQW75tATDPRVdqoAOclK1fPT09ERExOjra4UoAgFZZUwMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOSaUAMAAOTa5k4XANAp//zP/xzT09MN265fvx4REYODgw3bf/jDH8bu3bvbVhsA0DqhBtiwKpVK/MVf/EVs2rQpHnnk3oXrNE0jIuKnP/1pRER8+eWXcffu3RgbG+tYnQDA0gpp9h0cYIO5c+dOPPbYY/HrX/96yXaPPvpofPHFF7F169Y2VQYArMBFa2qADWvLli1x8ODBJcPKli1b4tChQwINAKxjQg2woR06dChu37696PN37tyJw4cPt7EiAGClTD8DNrQvv/wyvve970W5XG76/OOPPx7/9V//VVtzAwCsO6afARvbI488EkePHm06vWzr1q3xZ3/2ZwINAKxzvlMDG95iU9Bu374dhw4d6kBFAMBKmH4GEBG7du2K//iP/2jYtnPnzrhx40ZnCgIAWmX6GUBExNGjR2PLli21r7du3Ro/+clPOlgRANAqV2oAIuLf//3f4/d+7/catl27di3+P3v3Hh1Fff9//LUkXEoroFZQQFBLQcESLbRfUBTlUpW6UZEkhFv1CCH5qi1+8Xvg29+m9Bw8FdpQoaeVECjV2jY3am3WC8rFipekipqI1gMKsoFgd7GSFLwRwvz+oLNukt3NbrLZ2dl9Ps7JgczOzr7nMzObee18PrMjR460qCIAABAhrtQAgHSm+9nYsWPlcDjkcDg0duxYAg0AADZBqAGA/1iwYIHS0tKUlpamBQsWWF0OAACIEN3PAOA/jhw5ogsvvFCGYai+vl5Dhw61uiQAANCxynSrKwBSkcPhsLoEdODCCy+0ugSEwGdxAIC2CDWARZYsWaKJEydaXQba2L59uxwOh6ZOnRqzZebk5LC9Y6C6ulpr1661ugwAQAIi1AAWmThxorKzs60uA22YYebcc8+N2TJzcnLY3jFCqAEABEOoAYAAsQwzAAAgPrj7GQAAAABbI9QAAAAAsDVCDQAAAABbI9QAAAAAsDVCDQAAAABbI9QAAAAAsDVCDQAAAABbI9QAAAAAsDVCDQAAAABbI9QAAAAAsDVCDQAAAABbI9QAAAAAsDVCDQAAAABbI9QACc7n86msrEyZmZldmqe7XhsAAMBqhBogwa1YsUK5ublyu91dmqczFi5cGPFy6+rq5HA4/D8FBQUxrSXefD6fCgsL/etTVlZmSR01NTWt6igsLFRdXZ18Pp8cDkfc66mvr1dBQYF/G+/cubPV44H7QNufNWvWyO12q6mpKe51AwCSG6EGSHDr16+PyTydUVVVFfG8r776aqvfZ8yYEety4sbn8+nAgQNauXKlDMNQaWmpcnNztWbNmrjWUVhYqEcffVTz58+XYRgyDEP33nuv6uvrNWjQoLjWIklNTU2qq6vT+vXr1djYqMmTJ2vq1KmtQq9hGPJ6vf7fGxsb/bVPmzZNGzdu1Pz58+Xz+eJePwAgeRFqAMTE+eef7z95NQxDTqfT6pI67cCBA5owYYL/99mzZ0uS7r///rjVYF6RWb9+vUaOHOmfPnDgQDmdTlVXV8etFtOuXbv827V///7+dmnbPXHgwIH+//fv39///4yMDG3atEnSmauAXLEBAMQKoQawEZ/PpzVr1vi7/tTX13f4nKamJpWVlfm7AG3cuLHdp+TB5gll586drboUSWe6JGVmZqqwsFA1NTVdWseOagm3Pm3HALndbjkcDmVmZqq+vl41NTXtukSZzHZ1OBwaPHhwu9eUJJfL1aV1i1RNTY0eeOAB/fjHPw45T2DokuLTLhkZGUFryc/Pj3jdBg4cqCVLlsjtdmvXrl0RPw8AgHAINYCNHDhwQEuXLpXX61VDQ4OGDx/eYTee+fPn6/jx4/5uQW63u92n5PPnz9c777zjv8ryxhtvqLCwMOjyRowYoZKSEnm9XhmGIenMeBpJeuCBBzRx4kRlZmZ2untRR7WEW5/AMUA1NTVyOp3yeDxyu9168MEHNWHCBO3YsUPSmYBi1i9JS5culcvlUm1trYYNG+afXl9fr6KiIv9rx8NTTz0lSbrkkkvCzhdYf7zbRfoy7EXb1XDcuHGSpKeffjqq5wEAEJIBIO4kGeXl5VHN3/Zw3bt3ryHJKCkpCTnPjh07DEmG1+v1T6uurjYkGaWlpYZhGEZpaWnQeZxOZ7vl1tbW+p/XVmNjo1FbW2u4XK5WdUWjo1oiWZ9g7dB2mlljY2Njq/pdLler53k8Hv9zJRlFRUVRr5P5+l3d3uHEu10CX9fpdLaaP9J1iHYdDcMwysvLo34OACAlVHClBrApc5xFXl5eyHkqKysltR7jcNlll0mS/vSnP7X6N3CeCRMmtLtJQE1NjYqLi/3jKNrq37+/MjIytHLlSpWUlHTqTmwd1RLJ+kRi1qxZkqRnnnnGP+3111/3TzcNGzZMhmGotrZWLpdL999/f9iueVaJd7uY1q5dqx//+Metxs0AAGAFQg2QxIqLi9tNM09AzdARafg4ePCgiouLIxozk52d3alQ09FzIlmfSGRkZMjpdLY64X/++edDjhnJyMjwdz0LFyJjxRyjEulAeivapaysTE6ns93YnkjEe4wSACD5EWoAmws3SNu8U1Ww8S3m88x5zHExocyePVsul0sTJ07scLxM//79oxo83rbeULVEsj6RmjNnjn+MSX19vb773e+GnT/wDmTdzRyjcvDgwYjmj3e71NXV6Z133tGiRYuiWrbp9ddflyRdf/31nXo+AABtEWoAmzJP/CdPnhxynjlz5kg6c4MBk/kpeVZWlqQvT4iLi4v9j5lfsNjW/fffL6fTqRUrVoStrampyb/8aHRUSyTrE6kpU6ZIkh599FG98soruvbaa8POb75OaWlpVK/TGU6nU06nM+gVGFN9fb3/e3Pi2S4+n0/bt2/XypUr/dPq6uoi/rJVn8+ntWvXyul0+l8LAIAus3pUD5CKFOXAcafTaUgyduzYYRiGYXi9XsPpdPoHrnu9Xv/A68DB4o2NjYbT6TScTqd/emlpqZGfn++fx1yWAgbE5+fnG3v37m21XHMwuDl43rwRQGlpqb8u8/GqqqpOtUu4WiJZn2D1NjY2Bm0bw/hyYHzbGwCYbevxePzLcLlcIQfMdyTa7W2ui9PpbLX+Jo/H06oN4tUuwbaP+RO4zQOXHXgTgdra2nZ1RoMbBQAAQqjgrwNggc6c5Jp3mjJP9AODRNsTzEBer9coKSnxP1ZaWtrublVer9d/Iutyufwn0cGWa95pK/Bk1vy/y+UyamtrO9MkHdYSyfoEqzdc29TW1hqS2r1G4DqZJ/fV1dWdXqfObG/DOBMOqqqqjPz8fH8tTqfTKCkp8QcuUzzaJbCOtj+h9plYtiOhBgAQQoXDMAK+kABAXDiUHIhyAAAgAElEQVQcDpWXlys7O9vqUhAHbO/YqKioUE5OjvizBQBoo5IxNQAAAABsjVADAAAAwNbSrS4AQPJzOBwRzUe3IgAA0BmEGgDdjrACAAC6E93PAAAAANgaoQYAAACArRFqAAAAANgaoQYAAACArRFqAAAAANgaoQYAAACArRFqAAAAANgaoQYAAACArRFqAAAAANgaoQYAAACArRFqAAAAANgaoQYAAACArRFqAAAAANiawzAMw+oigFTjcDisLgGwLf5sAQDaqEy3ugIgFZWXl1tdAkJ46KGHJEn33XefxZUAAIBIcaUGAAJkZ2dLkioqKiyuBAAARKiSMTUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDW0q0uAACs8umnn+qLL75oNe3kyZOSpGPHjrWa3rt3b/Xt2zdutQEAgMgRagCkrN/97ne65557gj52zjnntPr917/+te6+++54lAUAAKLkMAzDsLoIALDC0aNHdcEFF6ilpSXsfGlpafrwww913nnnxakyAAAQhUrG1ABIWeedd56mTJmitLS0kPOkpaVp6tSpBBoAABIYoQZASps3b57CXbA2DEPz5s2LY0UAACBadD8DkNKOHz+u8847r90NA0y9evXS0aNH1a9fvzhXBgAAIkT3MwCp7ayzztLNN9+snj17tnssPT1dmZmZBBoAABIcoQZAyps7d65OnTrVbnpLS4vmzp1rQUUAACAadD8DkPJOnjypr3/96zp+/Hir6V/72tf00UcfqXfv3hZVBgAAIkD3MwDo1auXZs2apV69evmn9ezZU9nZ2QQaAABsgFADAJLmzJmjkydP+n9vbm7WnDlzLKwIAABEiu5nACDp9OnTGjRokD766CNJ0rnnniuv1xv2O2wAAEBCoPsZAEhSjx49NHfuXPXq1Us9e/bUvHnzCDQAANgEoQYA/iM3N1cnT56k6xkAADaTbnUBgB1kZWVZXQLipG/fvpKkX/ziFxZXgniprKy0ugQAQBdxpQaIwJYtW3T48GGry0AnRbP9hg8fruHDh3dzRUgEhw8f1pYtW6wuAwAQA9woAIiAw+FQeXm5srOzrS4FnRDN9nvnnXckSWPGjOnusmCxiooK5eTkiD+DAGB7lXQ/A4AAhBkAAOyH7mcAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QA8SYz+dTWVmZMjMzuzRPd722HSXreiWSUG1cWFiowsJCi6oCACAyhBogxlasWKHc3Fy53e4uzdMZCxcujHi5dXV1cjgc/p+CgoKY1hJL3dVewQS2icPhUE1NTch5a2pq2s3fXXWYP5mZmdq4caN8Pl/MXkuKbxub6uvrVVBQ4N//du7c2erxUG3gcDi0Zs0aud1uNTU1xa1eAEDiItQAMbZ+/fqYzNMZVVVVEc/76quvtvp9xowZsS4nZrqrvYIxDEMej8f/+6OPPhpy3sDHvF6vDMOIaR1er7fV74Zh6Ne//rXq6+s1aNAg7du3L2avF6qNV65cqZUrV8bsdUxNTU2qq6vT+vXr1djYqMmTJ2vq1KmtQlXbNmhsbPS3w7Rp07Rx40bNnz8/5gEPAGA/hBogRZ1//vn+E0TDMOR0Oq0uKWEMGzZMklRUVKTi4mLV19e3m6e+vl4jRozw/z5w4MCY1xFsmcOGDdO9994rSXrooYdi/prxsmvXLv8+179/f82ePVuS2nV/C2yD/v37+/+fkZGhTZs2STpzhZIrNgCQ2gg1QDfy+Xxas2aNv3tNsJPjtpqamlRWVubvZhOsq1GweULZuXNnu+5R9fX1yszMVGFhYdjuVR2tm9vt9p+Ebty40b+eba8gdHadrP4Eftq0aZKkV155pd1jr7zyiv/xYJqamvxt4nA4VFhY6F+fYF3WounGZp7oFxcXt3q9WLdxsHE2bae53W5/t7i2+/fOnTuVmZnp7y4W+FqhQnR+fn7YdQ80cOBALVmyRG63W7t27Yr4eQCA5EOoAbrRgQMHtHTpUnm9XjU0NGj48OEdnqjPnz9fx48f93e9cbvd7T6Jnj9/vt555x3/VZY33ngj5GDuESNGqKSkpFX3qLq6OknSAw88oIkTJyozMzPqADFo0CBlZmbK7XarpqZGixYtUmNjoyRp1KhRrYJNpOvU0TzxlpGRofz8fOXm5rZ77IUXXlBGRkbI5y5fvlx5eXnyer3yeDx64IEHtGLFCklnulWVlJRIkr97ldfrldPpVG1tbYfd2Mw2CQwA3dHGwcZoBU6rqamR0+mUx+OR2+3Wgw8+6J/P7XZr6tSp+vGPfyzDMDRkyBANGjQoZGgza4i2G+S4ceMkSU8//XRUzwMAJBkDQIckGeXl5VHN3/bw2rt3ryHJKCkpCTnPjh07DEmG1+v1T6uurjYkGaWlpYZhGEZpaWnQeZxOZ7vl1tbW+p/XVmNjo1FbW2u4XK5WdUUj2DrU1tYakoyioqKI1ymSeYK9VjR1RrP9zOcE1lZdXd1qHXfs2BG2LpfLZeTn57daXtv58vPz/etdVFTUav3bPq+2ttYwjDPbzdxmZk3d2cadnRZqHnO/aGvHjh2G0+k0GhsbQ7ZBKJ3dN8rLyzu9TwEAEkoF7+ZABGIRatpOD3eSG6ixsdGQ5A8tTqczohO86urqVifV4ZSUlPiXH41I1jOSdYpkHqtCjfn/wLZ0uVytHgtXl8fjMYqKioLO5/V6/eu4d+/ekHW0/XG5XP6QYxjd28adnRbs9cK1ldPpbBUcI31eJI+HQqgBgKRBqAEiEa9Q09nnBZvXvKIT6kQxkHlyG62u1BureSKtsyuhxmxLj8djeL3eVle/wtVlhkXzKl2w+TraTpGsd3e2cWenmVfszLZqewUvUGlpadgrheHawNx3A4NmpAg1AJA0KhhTA8RZuIHQ5uDpYONbzOeZ85jjYkKZPXu2XC6XJk6c2OF4mf79+0c1QDsSbeuNZJ3CzWOlq666StKZmwPs3LnT/3s4ZWVlysvL069//WuNHDky6Dw+n08NDQ0qKiqKaDuFkohtnJGRoaqqKjU0NPhvlFBaWqqlS5e2mq+urk7vvPOOFi1a1KnXef311yVJ119/fZdrBgDYF6EGiBMzhEyePDnkPHPmzJF05gYDJnMAdVZWlqQvT06Li4v9j5lfYtjW/fffL6fT6R+gHkpTU5N/+V1l3iDAHPAdyTpFMo+Vhg0bJpfLpdzcXDU0NPhv+RyOeXOBcPM+9thjWrp0qRYuXBjRdgolEdvY7Xbr2muv1dKlS2UYhqqqqvy3bTb5fD5t37691ffg1NXVRfxFsD6fT2vXrpXT6dSUKVNiWj8AwGasvlYE2IGi7L5kjnsxB5N7vV7D6XT6u96YYymk1gO3GxsbDafTaTidTv/00tLSVuM5zGWZz9d/xnvs3bu31XLNAdcej8eQvrwRQGlpqb8u8/GqqqpOt4sCuhiZg9gDx+dEsk4dzROqvaKpM5rtZ75e4GuZ3acCx7KEq8vcRh6Pp1X3M6/X62+nwEHxwbpRmdM6Wu/uauOOppn1B6szcP9su696vd6g+7H5E7g/Bi47sL1qa2vbrU+06H4GAEmDMTVAJKI9KTaML+/mZJ7IBQaJtidxgbxer1FSUtIqMLS9I5TX6/XfAcvlcvkHmQdbrnnXq8ATRvP/bQecR8tcjnmCaYanYPVGsk6h5gnXXpHWGen2C3aSbQp2N7NQ85ohyOVy+bdXfn6+P2S2nT/U8iJd9+5o465MC9wnggUb80YCwX5C7c+BP0VFRRGNFwuHUAMASaPCYRgdfCECADkcDpWXlys7O9vqUhKK+X0jif42wvaLv3379qlPnz7tut/t27dPo0aNSoh9pqKiQjk5OQlRCwCgSyoZUwMAiKmysjKNHDky6HiiQYMGqbS01IKqAADJLN3qAgDYU+BdtHw+nwYOHGhhNUgkf/rTn3T8+HHdcMMNrYLNvn379MILL3T6TmcAAITClRoA7Tgcjg5/Bg0a5J8/8P/AY489prPOOksPPvigf38pLCzU4cOHCTQAgG7BlRoA7TDGAF3Rv39/zZ49W7Nnz9b69eutLgcAkAK4UgMAAADA1gg1AAAAAGyNUAMAAADA1gg1AAAAAGyNUAMAAADA1gg1AAAAAGyNUAMAAADA1gg1AAAAAGyNUAMAAADA1gg1AAAAAGyNUAMAAADA1gg1AAAAAGyNUAMAAADA1hyGYRhWFwEkOofDoQkTJmjo0KFWl4JO2LJlC9sP7Rw+fFg1NTXizyAA2F4loQaIQFZWltUlIE7effddSdJll11mcSWIl8rKSqtLAAB0DaEGAAJlZ2dLkioqKiyuBAAARKiSMTUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbC3d6gIAwCp//OMf9dvf/lanT5/2T9u7d68k6brrrvNP69Gjh+666y7NnTs33iUCAIAIOAzDMKwuAgCsUFdXpyuuuCKieWtra5WRkdHNFQEAgE6opPsZgJSVkZGhUaNGdTjfiBEjCDQAACQwQg2AlDZ//nz17Nkz5OM9e/bUnXfeGceKAABAtOh+BiClHThwQCNGjFC4t8L33ntPI0aMiGNVAAAgCnQ/A5DaLrnkEl155ZVyOBztHnM4HBo3bhyBBgCABEeoAZDyFixYoLS0tHbT09LStGDBAgsqAgAA0aD7GYCU5/P5dMEFF7S6tbN05lbODQ0NOv/88y2qDAAARIDuZwAwcOBAXXvtta2u1qSlpWny5MkEGgAAbIBQAwA6cxe0SKYBAIDEQ/czAJD073//W1//+tfV3Nws6cytnH0+nwYMGGBxZQAAoAN0PwMASerXr59uuukmpaenKz09XTNmzCDQAABgE4QaAPiPefPmqaWlRS0tLZo7d67V5QAAgAilW10AkIwqKiqsLgGd0NzcrF69eskwDH3xxRdsR5vKzs62ugQAQJwxpgboBsG+yBFAfPBnDQBSDmNqgO5SXl4uwzD4sdnPM888o61bt1peR7Cf8vJySbK8jkT9MdsHAJB66H4GAAGmTZtmdQkAACBKhBoACJCeztsiAAB2Q/czAAAAALZGqAEAAABga4QaAAAAALZGqAEAAABga4QaAAAAALZGqAEAAABga4QaAAAAALZGqAEAAABga4QaAAAAALZGqAEAAABga4QaAAAAALZGqAEAAABga4QaIIHU1NSooKBADodDt99+u/7v//5PmZmZVpcVUz6fT2VlZZ1ar8D2KSgoUF1dXTdUmHq6sk0AAEgE6VYXAOCMnTt3aurUqfJ4PFq/fr3OPvtsPf7441Evp6mpSQMGDJBhGGGnWWXFihUqLi6O+nlt26esrEyFhYWqqqrqdC2J1C7dweFwRDRffn5+p7ZJou9rAIDUwZUaIEFUVlZKkoYNGyZJOnbsWKeWs2vXroimWWX9+vWdel7b9pk9e3aXAo2UWO3SHQzDUGNjY6vfA3927NghqfPbJNH3NQBA6iDUAAmiM5+Ut9XU1KSNGzd2OM2OYtE+gZKlXTrSv3//kI9NmTKl08tN5n0NAGA/hBrAYg6Ho1U3oba/t2WeOJrzFRYWyufzSZKKiorkdrtbLSfYNJPP59OaNWvkcDiUmZmpnTt3+qcHjrFwu93+eerr61vVE2oZgfWWlZX5H9+3b19M2yfc60fbVoE/bV/PnObz+eR2u5WZmammpiYVFBSosLAw4vYwH9u4caN8Pl/EXcRizXzdcN3EEm1fAwAgJANAzEkyysvLo35O20My2LT8/HxDkuH1eg2Px2NIMvLz86NejtfrNZxOp1FaWmoYhmHs2LHDkGTU1tYaTqfT/5zq6mrDMIygrxVuGSan02nk5+cbjY2NhmEYRmlpadB6OtM+Hb1+tG3l9XrbTTOfZ05r2za1tbX+ZXZUT1FRkeHxeAzDMIzGxkbD5XJF1Q7l5eVRt1uw9TTXKdw8hpFY+1okOts+AADbq+DdH+gG3RlqXC5X2BPLSJdjhou287lcroiX09EyqqqqDEnG3r17/Y83NjbGLNR09PqxaqtQzzODWqT1mAHBZIaoSHU11LT9CTZPoETa1yJBqAGAlEWoAbpDd4Yak8fjMYqKijp9ohn4CXmwk91IltPRMsxP+iNZ1450Zh1MXW2rSJ4XST1me5SWlrYLRJGI95WawPmt3tciQagBgJRVwZgawIY2btyoe+65R06ns9PLMMc+GG3uiGVEcSvejpYR68H90b6+FJu2ilU99913n5xOp3JzczVgwACtWbOm22sKxryDXCQSZV8DACAcvqcGsJmysjLl5eXJ4/FEdXIayr59+zRy5EjLl9Edrx/rtupqPSNHjlRVVZXq6upUXFys+++/X5K0dOnSuNVmiiRQJOK+BgBAMFypAWwmNzdXUnSftgdTUlIiSXrsscfU1NQk6cs7VMVqGebjdXV1Xaq1s68fq7aKVT0Oh0NNTU3KyMjQ+vXrVVtb6w82iSiR9jUAAMKKV0c3IJUoyjE1tbW1/jEE5qD6wDtxBQ4uN8cneDweY+/eve3mMR/3er1GUVFRyGmByw/88Xg8rR4zx34EDvA3XyvcMgzjy7EbTqfTP82885UU+d2tgrVPJK/fmbYyx72Yr1NdXd2q3mB3SIu0HunM4PjA9jFfNxKdGTMSuN1CjeOxw77WXe0DAEgK3CgA6A7RhJpgJ3vBfkzmCb7L5TK8Xq//DlXmiXLbx0NNM4wzJ9XmbYUDlxHstUPVE2oZgY+bQcEMBebtfSM5Ye2oPcK9fmfayuPx+E/Mq6qqDMMwWtUbWIPT6WxXb7h6Ak/2JUUVaAwj+pP2jtou1HymRNvXYt0+AICkUeEwDEZqArHmcDhUXl6u7Oxsq0tBEqmoqFBOTg4D7EOgfQAgZVUypgYAAACArRFqAAAAANgat3QGYBmHwxHRfHQnAgAA4RBqAFiGsAIAAGKB7mcAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbC3d6gKAZFVdXW11CbC5zz77TF/5ylf8v5v7VEVFhVUlJTSOOQBIXQ7DMAyriwCSjcPhsLoEIGXxZw0AUk4l3c+AbmAYBj9d/Dl16pSWLFkih8OhFStWWF5PvH8+/fRTLVu2TD169NCMGTN0+PBhy2uyyw8AIPVwpQZAwjlx4oTmzJmj5557Tps2bdK8efOsLskyL7/8su68804dPXpUq1evVl5entUlAQCQaLhSAyCxHDhwQBMmTNDu3bu1a9eulA40knT11VertrZWixcvVkFBgW6++WYdOXLE6rIAAEgohBoACeOVV17RxIkTlZ6erpqaGn33u9+1uqSE0LdvX61atUovvPCC9u7dqzFjxqikpMTqsgAASBiEGgAJYfPmzbr++ut1zTXX6OWXX9awYcOsLinhTJo0SXV1df6rNk6nk6s2AACIUAPAYoZh6Kc//anuuusu5efnq6KiQl/96letLithmVdt/va3v+ndd9/V5Zdfrscee8zqsgAAsBShBoBlTpw4odtuu02rVq3So48+qnXr1qlHD96WInHNNdfojTfeUFZWln7wgx8oOztbR48etbosAAAswd3PAFji8OHDyszM1KFDh/T444/rmmuusbok23ruuee0aNEiff7553r44Yd1++23W10SAADxxN3PAMRfdXW1xo8fr5aWFu3evZtA00Xf+973tGfPHt16663KyspSdna2PvroI6vLAgAgbgg1AOKqrKxMU6ZM0bhx4/Tiiy9q+PDhVpeUFPr166cNGzbomWeeUU1NjcaMGaPHH3/c6rIAAIgLQg2AuDBvCJCbm6u8vDy53W7169fP6rKSzg033OC/anP77bcrOztb//rXv6wuCwCAbsWYGgDd7sSJE5o/f76eeeYZbdiwQT/4wQ+sLiklbN26VYsWLVJzc7PWr1+v2267zeqSAADoDoypAdC9GhoadN111+mll17Ss88+S6CJoxtvvFFvv/22brnlFs2cOZOrNgCApEWoAdBtampqNH78eJ08eVKvvfaaJk+ebHVJKad///7asGGDnn76ab3yyiu6/PLL9cQTT1hdFgAAMUWoAdAtysvLNWXKFF155ZV68cUXddFFF1ldUkq76aab9PbbbyszM1O33XabsrOz9fHHH1tdFgAAMUGoARBTgTcEWLRokZ588kn179/f6rIgacCAAdqwYYOeeuopvfLKKxozZoyqqqqsLgsAgC4j1ACImU8++US33367HnzwQf32t7/VunXr1KMHbzOJZsaMGf6rNrfccgtXbQAAtsfdzwDERENDg2699VZ98MEH2rJli6677jqrS0IEnnrqKeXl5ckwDG3YsEFOp9PqkgAAiBZ3PwPQdW+++aYmTJigzz//XK+99hqBxka+//3v6+2335bT6VRmZqays7N17Ngxq8sCACAqhBoAXVJRUaFJkyZpzJgxeumll3TxxRdbXRKidPbZZ2vDhg1yu9166aWXNGbMGD355JNWlwUAQMQINQA6xTAMrV69WrNnz9a8efO4IUASuPnmm1VXV6err75amZmZWrx4sY4fP251WQAAdIgxNQCi9vnnn+uuu+5SRUWF1q1bp//+7/+2uiTEWGVlpe6++2717dtXmzZt0rRp06wuCQCAUBhTAyA6R44c0bXXXqutW7fqueeeI9AkqaysLL399tsaP368vve973HVBgCQ0Ag1ACJWW1urCRMmqLGxUa+88oquv/56q0tCNxo4cKC2bNmi8vJyPf744xo7dqx27txpdVkAALRDqAEQkS1btujqq6/WZZddpldffVWjRo2yuiTEiXnV5tvf/ramTZumxYsX68SJE1aXBQCAH6EGQFjmDQFycnI0b948PfXUUxowYIDVZSHOBg0apD//+c8qLy/Xn//8Z40dO1bPP/+81WUBACCJUAMgjM8//1wLFiyQy+XSunXrtGHDBqWnp1tdFiyUlZWld955R1dccYWmTp3KVRsAQELg7mcAgvrwww9166236v3331dlZaWmTJlidUlIMJWVlSooKFC/fv20efNmvnQVAGAV7n4GoL26ujpNmDBBH3/8sV5++WUCDYIyx9qMHTtWU6ZM0eLFi/XJJ59YXRYAIAURagC08uc//1lXXXWVRo0apddee02XXnqp1SUhgZ1//vl64oknVF5eri1btmjs2LF64YUXrC4LAJBiCDUAJH15Q4Ds7GxuCIComWNtLr/8cl1//fVctQEAxBVjagDoiy++0KJFi1RaWqo1a9bohz/8odUlwcYqKyu1ePFinXPOOdq8ebOuvfZaq0sCACQ3xtQAqe6jjz7S9OnT9eSTT2rr1q0EGnSZedVm9OjR/qs2n376qdVlAQCSGKEGSGFvvfWWxo8frw8//FAvv/yypk6danVJSBIXXHCBqqqqVFZWpsrKSo0dO1Yvvvii1WUBAJIUoQZIUU8//bSuueYaDRs2TNXV1brsssusLglJKCsrS2+++aYuvvhiXX/99Vq+fLm++OILq8sCACQZQg2QhA4ePKg9e/aEfHzdunVyOp2aPXu2duzYoa9//etxrA6pZvjw4Xruuef08MMP6+GHH9a3v/1tvfbaa1aXBQBIIoQaIAn98Ic/1E033SSfz9dq+hdffKE77rhDS5cu1c9+9jNt2LBBPXv2tKhKpBKHw6G8vDzt2bNHF1xwga666qoOr9r87Gc/0/79++NYJQDArrj7GZBktm7dqptuuklpaWkaN26cdu3apd69e+ujjz7S7bffrjfffFN/+tOfdPPNN1tdKlKUYRjauHGj7r//fg0bNkyPPPKIxo8f32qev//977rqqqt0+eWX69VXX1Xv3r0tqhYAYAPc/QxIJidPntTdd9+ttLQ0tbS06I033tDChQu1Z88efec731FDQ4NqamoINLCUedXmrbfe0qBBgzRx4sRWV20+//xzzZ07Vw6HQ//4xz+0dOlSiysGACQ6rtQASeRnP/uZfvKTn6ilpcU/rUePHjrvvPM0evRobdmyReecc46FFQKtmVdtli5dqosuukiPPPKIysrKtHbtWp06dco/3x//+EfNmTPHwkoBAAmsklADJIlDhw5p5MiR+vzzz9s95nA4tGXLFs2cOdOCyoCO7d+/X3feeafq6up04sQJnT592v+Yw+FQnz599Oabb2rUqFEWVgkASFB0PwOSxY9+9KNWV2jamjt3rurq6uJYERC5b3zjG9q6dasGDBggh8PR6jHDMC1nybUAACAASURBVHTq1Cnddttt+uyzzyyqEACQyAg1QBLYvn27/vKXv6i5uTno44ZhqLm5WTNmzJDX641zdUBkXC6Xjhw5EjScNzc367333tP//u//WlAZACDR0f0MsLmTJ09q9OjROnjwYNgrNaaJEyfq+eef525SSCjV1dWaNGlSq25noTC+BgDQBt3PALtbu3Zt2EDTo0cPORwOfe1rX1N+fr4eeughAg0Syqeffqo5c+Yoks/YzDunvf/++3GoDABgF+lWFwCg8xoaGvTTn/40aKBJT09XS0uLJk+erDvuuEOzZs1S3759LagSCO/gwYOaNGmSPvnkEx09elTp6Wf+NAXe/cxkGIa++OILzZw5U6+99hoBHQAgie5ngK1lZ2friSee8I+l6dmzp5qbm3XxxRdrwYIFuuOOO3TRRRdZWyQQhQMHDmj79u3atm2btm3bpqamJvXs2VMtLS2tuqalp6crLy9Pv/nNbyysFgCQILilM2BXzz//vKZMmeK/U1Tfvn01d+5c3XnnnZowYYLF1QFdd/r0adXV1Wnnzp3atm2bdu3apc8++0y9evVSc3OzDMNQRUWFsrKyrC4VAGAtQk2yqqioUE5OjtVlAHExa9YsVVZWWl1Gl3HcAvHHaRCQFCoZU5PkysvLrS4BQVRXV2vt2rWd3j6vv/66Dh06pGuuuUbnnntujKuzl4ceesjqEmKO4zYyJ0+e1L59+3TixAmuTkYhJydHS5Ys0cSJE60uxVLm+zCA5ECoSXLZ2dlWl4AQ1q5d2+ntw3b9UjJcoWmL7YvulJOTo4kTJ7KfSYQaIIlwS2cAAAAAtkaoAQAAAGBrhBoAAAAAtkaoAQAAAGBrhBoAAAAAtkaoAQAAAGBrhBoAAAAAtkaoAQAAAGBrhBoAAAAAtkaoAQAAAGBrhBoAAAAAtkaoAQAAAGBrhBoAAAAAtkaoAQAAAGBrhBp0G5/Pp7KyMmVmZlpdCqLAdkt+kWzjtvMUFhaqsLCww2VHOl+sxXq/5TgAAHtJt7oAJK8VK1aouLjY6jIi1tTUpHfffVd79uyR2+1WVVVV0Pncbrc2btwot9stp9OpOXPmaPbs2d1am8PhCDrd6XRq8uTJcjqdGjlyZExey27bDdGLZBtHMk9TU5MGDBggwzBiWV6nxHq/TbTjINL3p1gL9d4jSUVFRRo5cqSuvfZa9e/fPy71AEBIBpJSeXm5kQibV1JC1BEJl8tluFyusDUXFRUZkoza2lrDMAyjtrbWkGQUFRVF9Vqd2T5er7ddbV6v11+zWVMs2Gm7zZo1y5g1a5bVZcREPI/bSLZxR/NUVVUl1H4S6/02kY6DSN6fIiXJKC8vj3j+wPeexsZG//Ta2lrD6XQaTqfT8Hq9XarJConydxJATFTQ/Qz4j5UrV2rlypVh57n//vslSRkZGa3+feGFF7q3OEkDBw4MOs2sKZE+VUbya2pq0saNG60uI2VE8v7UXQLfewKvyGRkZGjTpk2SpIULF6qpqSnutQGAiVADSWf6j7vdbmVmZqqpqUkFBQWt+sX7fD6tWbNGDodDmZmZ2rlzZ6vnm49t3LhRPp8vaJcFt9sth8OhgoIC+Xw+/3Tz5MjhcMjhcKiwsND/eGBdkvzzFRQUaN++fe3WIVyNsVBUVCRJqqmpkSTV19dLkmUnG9KXJxnBQg3bLfmF2w6B85SVlfnbuO026GieYONLioqK5Ha7Jcn/2uHGoQQuP3CfC7Z8c5/LzMz0H2ORrmtnRdJGofbVSOuXwh9zdjwWBg4cqCVLlsjtdmvXrl2tHqO9AMSV1deK0D2ivazudDr93Quqq6uN2tpaIz8/3zCMM10PnE6nUVpaahiGYezYsaNVd6eioiLD4/EYhmEYjY2N/i4ShmG0WqZhGMbevXsNSf5lG4Zh5OfnG5IMr9dreDyeVo+bzw9cRmNjo/85e/fujajGaKiD7h3m+lVXVxulpaWd6nbR2W4PwWoz26xtF7hU2W6p3v0s3HYwOZ1OIz8/3991qLS0tN2+FG6ewPeHQMGWEer4cTqdRklJiWEYX253p9NpNDY2tnv/MQwj6Lp0tK4dHbvhdNRG4fbVSOsPd8xFeix0ZR0DlxFN97OOXrexsbHduiZKe4VD9zMgqVRwNCepzrxZm39kAvtMG8aXf9zbzutyufz/DzyxN/tfBy4z2OuYXC5X2BOTYMtoO5aloxqjEclJg3ly5XK52rVXJGIVasL1aU+V7Zbqoaaj7WCOezGDpGF8eRJqzhfJPJHsE6GmmSecgftbdXW1Icl/Utpd+1wkIln/SI6njuoPd8xFeiwkYqgJ9niitFc4hBogqRBqklVXQk1bgZ+qtf0xjC9P8EtLS9ud4Ed60mMYZz6pMwfiR3KiEji9oxpj0Q6moqIi/7q6XC7/p83R6GqoCfzZsWNH0HlTZbuleqgxhdoO5nZuK9i+EG6eroSaYMs3Q4PT6YxqWeHWtbPHfCTr39G+Gkn94Y65SI8Fu4SaRGmvcAg1QFIh1CSrWIaajv5Q7N27t9UfmMBuUJH+US4pKTGcTqe/m1O0J8ex+EPf0esZxpefDpp/YM16zW41kYrVlRqn0xnyk8lU2W6Emu7bDrEKNbFcfmfWtSOx2FcjqT/aYy6aWqMR61BjBtTA96JEaa9wCDVAUiHUJKvuCDWBXTOCMcfhBP7xieQPlxkUzL7T0ZyUtR3D0VGNkQj3x7LtY227qEQqVqHG7FceLNikynZL9VDTle0Qr1Bjnpi27SIZbF8It6zOrmtHoln/UPtqNGEu3DHX0bGQiKHG7F4YeNU4UdorHEINkFS4pTM6VlJSIkl67LHH/LfsNO86I52581FTU5MyMjK0fv161dbW+m8zHInc3FxJ0rBhwyJ+jnlnohkzZkRUY6w4nc5Wv5t3Hms7PV4GDhyoTZs2qa6urt23uLPdUkNH28Fs47q6upDLiGSerpgzZ44k6cCBA/5p5vbOysqKeDmd2eciEU0bdWVfDXfM2fVY8Pl8Wrt2rZxOp6ZMmeKfTnsBiDurYxW6R7SfQAX7YsdgjwX+BH5a6nK5/L+b/d0Dn2d+Qht4ZcOcZn6K6/F4WnUpMR83fzcHFAeOZYm0xkgF1hdsnIz5iaRZiznYOdS4llC6+uWbbT/xNgfgl5SU+B9Lle2W6ldqOtoO5l2lnE6nv13N/Vj/uVLS0TwzZ84Muu8FXoEJte8YhuG/w1ngDS1KS0tb3WGx7XEX7T4X7vjoSCRtFG5fjbT+UMdc2zYIdSx09P4UKUV5pSbU64a7UUkitFdHuFIDJBW6nyWraN+sA/8oBJ50mjwej/92muZJUOBzzZMaBekeEBiWgk0zT8hdLpfh9Xr9dzhq28Uk8FagJSUl7f6oh6sx2jZoW2OgHTt2+LtC5OfnRx1oDKNr2ydUbWY7Bm6DVNhuqR5qOtoOhnGmjQP32cDb4QaGn1DzhNrv2r52uP3T6/UaJSUl/scCB3/HYp/r6LjtSKRtFGxfjbT+UMdcYA3hjtdI3p8iEU2oCfW6Zv3mLZmDsbK9IkGoAZJKhcMwDENIOhUVFcrJyVEybF7zy9aSYV1MybR9QonXdjO7L1VWVnbr68RDKuwXsJ7D4VB5ebmys7OtLsVSHG9AUqlkTA0AAAAAWyPUIKH5fL6g/0diY7sBAIB4Sre6ACCcQYMGtfp/Z7oJmN2gOkIXhNiJxXYDuopjHwBSB6EGCS0WJxucsMQfbY5EwH4IAKmD7mcAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbI1QAwAAAMDWCDUAAAAAbC3d6gLQvRwOh9UlIAy2T2zMmjXL6hJiiv0C3S0nJ0c5OTlWlwEAMUOoSVJXXXWVysvLrS4jafzqV79SY2OjfvKTn1hdCoK48MILrS4hJjhu7am6ulpr165N+m3X0tKi8vJyvfDCC2pqatLYsWM1ZcoUjR8/XunpnE4AsJbDMAzD6iKARPbRRx9p6NChKi4u1h133GF1OQASTEVFhXJycpQqf05bWlr0/PPPq6SkRH/5y1901llnKSsrS/fcc4++9a1vWV0egNRUyZgaoAObNm3SV77yFWVnZ1tdCgBYLi0tTdOmTVNFRYU8Ho+WLVum7du3a+zYsRo/frxKSkr0ySefWF0mgBRDqAHCOH36tEpKSnTnnXeqb9++VpcDAAll8ODBWrZsmd577z1t27ZNo0eP1pIlSzR48GAtXrxYb7zxhtUlAkgRhBogjK1bt+qDDz7QwoULrS4FABJWjx49NG3aNP3+979XQ0ODfvGLX6i6ulrjxo3TmDFjtHr1an388cdWlwkgiRFqgDCKi4s1ZcoUjR492upSAMAWzj77bOXl5emtt97S7t27NWnSJK1cuVJDhgxRdna2tm/fnjLjjwDED6EGCOHQoUN6+umnlZ+fb3UpAGBL48aN04YNG3TkyBGtW7dO+/fv1/Tp03XZZZdp9erV8vl8VpcIIEkQaoAQSkpKdN555+nWW2+1uhQAsLV+/fopLy9Pr7/+ut5++23deuut+vnPf64LL7xQ2dnZcrvdamlpsbpMADZGqAGCOHXqlDZv3qy77rpLPXv2tLocAEgaY8aM0apVq9TQ0KA//OEPOnbsmG655RZddNFFWr58uTwej9UlArAhQg0QxBNPPKF//vOf3CAAALpJnz59lJWVpW3btundd9/V3Llz9bvf/U6XXHKJpk+frsrKSjU3N1tdJgCbINQAQaxfv14zZszQRRddZHUpAJD0Ro0apVWrVunQoUMqKytTnz59lJubq2HDhmn58uV6//33rS4RQIIj1ABtvP/++3r++edVUFBgdSkAkFJ69eqlrKwsud1uHTx4UEuWLFFZWZm++c1v+r/Y89NPP7W6TAAJiFADtPHwww/rwgsv1A033GB1KQCQsoYOHaply5bpwIED2rZtmy655BLdc889GjJkiBYvXqza2lqrSwSQQAg1QIDPPvtMjz76qPLz85WWlmZ1OQCQ8swv9qyoqFB9fb2WL1+uHTt26Morr/RfvTlx4oTVZQKwGKEGCFBRUaHjx4/rjjvusLoUAEAb559/vpYtW6b3339fu3fv1rhx43Tfffdp8ODBWrBggbZv3251iQAsQqgBAhQXF2vmzJm64IILrC4FABCG+cWeDQ0NKioq0ltvvaXp06dr9OjRWr16tT766COrSwQQR4Qa4D/q6upUU1Oj/Px8q0sBAERowIABysvLU21trXbv3q1rrrlGDzzwgIYOHars7Gxt375dhmFYXSaAbkaoAf7j4Ycf1qWXXqrJkydbXQoAoBPMqzdHjhxRSUmJjh07punTp2vUqFH66U9/qkOHDlldIoBuQqgBJB0/flylpaXKz8+Xw+GwuhwAQBecddZZWrBggbZt26Z//OMfmjlzpn7zm9/o4osv9n+x56lTp6wuE0AMEWoASY899phOnTql+fPnW10KACCGLrvsMq1atUqHDx9WaWmpJCknJ0fDhw/X8uXL9cEHH1hcIYBYINQAkjZu3Kjc3Fydc845VpcCAOgGvXv3VlZWlrZt26a9e/dq/vz5euSRRzRixAj/1ZuTJ09aXSaATiLUIOW9/PLLqq2t5QYBAJAivvnNb2rVqlVqaGjQs88+q7PPPltz5szR+eefr8WLF2vPnj1WlwggSoQapLzi4mJdccUV+s53vmN1KQCAOEpLS/N/safH49GyZcu0bds2jR071v/Fnp988onVZQKIAKEGKe1f//qXtmzZorvvvtvqUgAAFho8eLD/iz23bdumSy65RPfee6+GDBmixYsX64033rC6RABhEGqQ0jZv3qzevXsrNzfX6lIAAAmgR48e/qs3//znP/Xzn/9c1dXVGjdunMaPH69169bp448/trpMAG0QapCyDMPQxo0btWDBAn31q1+1uhwAQII5++yzlZeXp7feeku7d+/WuHHj9P/+3//TkCFD+GJPIMEQapCytm3bpvfee095eXlWlwIASHDmF3s2NDRo3bp12r9/v6ZPn67LLrtMq1ev1tGjR60uEUhphBqkrOLiYl177bW6/PLLrS4FAGAT/fv3V15enl5//XXt3r1bN9xwg37+859r6NChys7OltvtVktLi9VlAimHUIOU9OGHH+rJJ5/kNs4AgE4bN26c1q1bp4aGBv3hD3/QsWPHdMstt+iiiy7S8uXL5fF4rC4RSBmEGqSkDRs2qH///po5c6bVpQAAbK5Pnz7+L/b8xz/+oblz52rz5s265JJL/F/s2dzcbHWZQFIj1CDlnDp1Sr/97W911113qXfv3laXAwBIIpdeeqlWrVqlw4cPq6ysTH369FFubq6GDx+u5cuXa//+/VaXCCQlQg1SjtvtVkNDgxYuXGh1KQCAJNWrVy9lZWXJ7Xbr4MGD+tGPfqSysjKNHDlSkyZNUklJiT777DOrywSSBqEGKae4uFg33nijRowYYXUpAIAUMHToUC1btkwHDhzQs88+q8GDB+uee+7R4MGDtXjxYtXV1VldImB7hBokrVOnTrWbtn//fm3fvp0bBAAA4i7wiz09Ho+WL1+uHTt26IorrtD48eNVUlKiEydOWF0mYEuEGiStu+66S9nZ2frb3/7m/3K0DRs2aPDgwfr+979vcXUA7Ki5uVnHjh1r9fPJJ59IUrvpjY2NFleLRHbBBRdo2bJl2rdvn1588UWNGzdOS5Ys0ZAhQ7RgwQJt377d6hIBW3EYfBUuktSMGTO0detWGYahESNGKD8/X6tXr9a9996rwsJCq8sDYEP//Oc/NXTo0Ii+h+S6667T888/H4eqkCwaGxtVUVGhhx9+WHV1dRo9erQWLFighQsX6txzz7W6PCCRVXKlBknr2LFj/is0+/fv17Jly/Txxx/r73//O5+AAeiU888/X9dee6169Aj/59PhcCg3NzdOVSFZDBgwQHl5eaqtrdXu3bs1adIkPfDAAxoyZIiys7O1fft28Vk0EByhBknr3//+t///hmGopaVFLS0tevbZZzV9+nSNHTtWmzZt8ncdAYBIzJ8/Xw6HI+w8PXr00O233x6nipCMxo0bpw0bNqihoUG/+tWvdOTIEU2fPl2XXnqpVq9eLa/Xa3WJQEIh1CBpHT9+POh08wYCb7/9thYtWqQ//vGP8SwLgM3dfvvtSktLC/l4WlqabrzxRroLISb69eunvLw8vfTSS3rnnXd022236Re/+IWGDBni/2LPYDfGCWfPnj1c8UHSIdQgaXV0BxmHw6Fly5YpLy8vThUBSAb9+vXTjTfeqPT09KCPG4ahefPmxbkqpILRo0dr1apVamhoUGlpqSQpJydHF110kZYvX64PPvigw2WcPn1aTqdTP/jBD3Ty5MnuLhmIG24UgKTVu3fvkG/Y6enpmjlzpkpLSzvsGw8AbVVWVionJyfop919+vTR0aNH9bWvfc2CypBq9u3bp82bN+uRRx7R0aNHNWXKFOXl5enWW29Vz549282/fft2TZ8+XWlpabrqqqv017/+VWeffbYFlQMxxY0CkJxaWlpCBpqePXtqwoQJ+v3vf0+gAdApTqdTffv2bTc9PT1dt912G4EGcTNy5Ej/1Ztnn31WZ599tubMmaMLL7xQP/rRj/T222+3mr+kpEQ9e/ZUS0uLampqdOWVV2rfvn0WVQ/EDmd0SEqhxtP07NlTF198sdxut3r37h3nqgAkiz59+mjmzJntPgk/deqU5s6da1FVSGVpaWn+L/Y8ePCg7rvvPlVVVelb3/qW/4s9Dx06pCeeeELNzc2SznzvUkNDg7773e/qxRdftHgNgK6h+xmS0qFDhzRs2LBW09LT03XOOefotddea/cYAERr69atuummm1pN69evn44ePapevXpZVBXwJfOOn5s2bdKTTz6pb3zjG3r//ffb3VigR48eSktL0yOPPKI5c+ZYVC3QJXQ/Q3Jqe6WmR48e6tWrl5577jkCDYCYmDZtms455xz/7z179tTs2bMJNEgYaWlpmjFjhh5//HHV19frs88+C/rFsadPn1Zzc7Pmzp2rn/70p/EvFIgBQg2SUrBQ89e//lUZGRkWVQQg2aSnp2v27Nn+LmjNzc18yo2EtX//fnk8ng5v5bxy5Urdeeed/i5qgF0QapCUAkONw+HQ5s2bNW3aNAsrApCMcnNz/Sd/gwYN0jXXXGNxRUBwGzduDHo3tLZOnz6tP/zhD5o+fboaGxvjUBkQG4QaJKXAULN69WrNnz/fwmoAJKurr75agwcPliTNnz+fOyoiIf373/9WeXl5xFdfTp06pZdfflkTJkyQx+Pp5uqA2Oj2GwVUV1frl7/8ZXe+BNCOx+PRa6+9pm984xu68sorrS4HNvA///M/mjhxoqU18H5pT3v27NHevXs1depUvu/DhiorK7tt2b/85S9VXV3dbcuP1AcffKDXX389onkdDof//4ZhqHfv3po0aRL7NhJKkOO2MvjXIcfQoUOHtGXLFs2aNau7Xwrwa25u1gUXXKArrrhCkrRlyxZNmDBBQ4cOtbgyJKItW7YoKyvL8lDD+6U9DRs2TEeOHLHNSR/vh2ccPnxYNTU13foa1dXVqqmp0YQJE7r1dTpy8cUX6+KLL1ZLS4taWlp06tQpGYah5uZm/7+nT5/WqVOngs7z4Ycf6qtf/So3wbAQx+0Z4Y7bbg81pu78JARo629/+5v+67/+S1/5ylcknfnk6b777lN2drbFlSERBX4ymQh4v7SfiooK27y/8H54RkVFhXJycrr9dSZMmMAxjS7juD0j3HEbt1ADxNN1111ndQkAUkiqn2gAgNUY0QgAAADA1gg1AAAAAGyNUAMAAADA1gg1AAAAAGyNUAMAAADA1gg1AAAAAGyNUAMAAADA1gg1AAAAAGyNUAMAAADA1gg1AAAAAGyNUAMAAADA1gg1AAAAAGyNUAMAAADA1gg1Ccrn86msrEyZmZmtphcWFqqwsNCiqoILVSvsyU77HjonkmO27TyRbn+r9pNYvw/xvgYrxHu/s3o/t/r1kVwINQlqxYoVys3Nldvtjttr1tfXq6CgQA6HQwUFBdq5c2dEz+tsrXV1dXI4HP6fgoKCmNTTVYE1Bf6EezwaNTU17darqakp6uV0l0Tc90JtE4fDoTVr1sjtdqupqSlu9dpdJNs4knmSeb+14jgIJ9HeDzn2uke89zur9/OFCxcm1HEmde/7WrzeM1P2uDW6WXl5uRGHl0lKkuLWdo2NjUZVVZX//6WlpYYk/7SOdKbWkpIS//PavlZX6wlWX3l5ecTze71ef11erzfk48EeC6e6utqQZJSWlvqn1dbWGk6nM6GOk0Tc9wK3SWNjo3+62X5OpzPq7WGKdv/oLvF8v4xkG3c0T1VVVVLvt/E8DsJJpPfDWB97VorH8TZr1ixj1qxZUT0n3vud1fu51a/fVne+r3Vl2Ry3Z4Q5biu4UgNJ0q5du+R0OiVJ/f9/e/cfI0ddP378NXe9UjFtCdgrkrsiFY6kUaqgFIvBiMZ8NOz9oe31igRjBNlL0FAhmpC91IREjNkGQozAXRNDGr1f/90ZE43XP/oHV65irjFi7gR0j7ZhNwh3JEZs6b2/f9z3PZ2dnZl9z+zMzs7e85Fscje7+57XzPvHzGt+7fbtMjg4KCKS6Cnh66+/XpRS9kvPP614nLq7uz3/dk/zei/ISy+9JCJiL4+IyN69e+Wpp56KEmZbMK1r57revn27/ffevXvl+PHjIrJ+1K8tjz61oNXVVRkdHU07jA2hlcZD+h7aWZLjWrPHzI3Yb1suqal3uY/I+jWYMzMz9oA+Ojpqn5JfWloKPc9jx46JZVkyOjoqlUrFno9ugHrew8PDUqlU7Bic14HOzMzYMSwvL4uIyPj4eM20RuL3uvbUL47+/v6qeYqInDx5Uvr7++3Tj3pZRKQqoXDK5/M101ZXV+1l6+/vj7TOl5eXpb+/X4aHh+X06dM174eJpxWY1un58+dFZP3SO6e9e/dW/U/bC1fX3d3d8thjj8nMzIycOnXK+HvtIKitOD9Tr88Gfcar/ovFon3JiJ530PXxzvKd461X+X5tyWRZozJZR5VKxd5e9Pf325eAhekLftuboPJbeTwM6ntpra92FGWb4LU+w/TzMNsWkxj1tmd1dVWGhoZ87707efJk5Mu7g8YZk/1Lv3Gt3nYzatlpadt+m+JpIk/O02VaqVSqmqb/FhE1NzenlFo/JZ/P55WIqMXFReP5FYtFVSqV7DIKhYI9H11euVy2Y8jn80opZV8uJCJqYWFBKXXl0qJ8Pm/H5f5emPjd68E5T69pQfPUpzz1Z/TlC+7ytJWVFd/LG3K5nMrn8/bpTGdZpnQ8+lXvNGhQPCYkwuVF9ZbJ+Z5pnS4sLNifGxkZqTol7ETb867roDrR33PO21SU9pGEKJfDBLUVzaTPBn3Gq/6VMmsnzvdGRkaUUuvjvL78YWVlxbgt1VvWsONQmHWkY9aXjs7Oztp90DT+oO1NUPlurTYeevW9VlpffrJ0+ZnpNiFoferPmvTzsNuWsDEuLCzY77mXt1QqqZGRkUiXRgWNMyb7l17xmGw3o5YdBv12XdDlZy2X1CjlXQkmDUPvMBaLxVDzcnYc3TCVUqpQKARuME3iDDPNHX+c5ft9xm9dzc7O2gOBk95BdSaOumOEreeVlRW1sLBgN3g9EIWJx1Tcg4F+v97nvdrk4uKiPRiKrN9f414u2p53XZvUSZQNRpT2kYQo42W9tmLSZ00+00ib0Bsu51jrvr/MpKwo/cKEyfLrnT93fIVCwTj+oO1NvfKdWnU8dL7fSuvLT5aSmji2CUn3c9MY3e3W+bmFhYWqe07DiGucSXK7GXWM0t+l326gpCZouh+9c+m1Y6mVSiVVDL5x3QAAIABJREFULBYjN3zTae7pcZavl9MkBqXWj3boTNzJq5x6ZZkYGRlRuVzO932/eEwlNRiYfN5v+tzcXFVy43XUlbbnH2OU94O+l9WkRvNrKyZ91uQzjbQJr/L1zpTu92Haaph+YcJk+Z1HKd0v0/iDtjf1ynd/thXHQ+f7rbS+/GQpqdEa2SYk3c+jxOicrreJUcU1ziS53Yw6Runv0m9JagItLi5WrWT30WO9s724uBi54SfZQUyn6SMK+mhF0FmtsbEx37Mmca13Nz3weAmKx1RSg4HJ5+uVMzc3Z7dBZ2JD26sVtC51GwpzxMdZbpaTmrBtxT292W2ikfKjLGs9jayjoDLc04K2N6axt+J46NX3WmV9BclaUtPoNiHpfh4lRud0fRQ/asIe1/IlOYY10m7pt+s2XFITJdPX13c6K0B3MH1tYNSGH7aD+F1n2ui06elp++iJ85pG93oI2ik06dhRedVbvXhMxT0YKKVqziyZ1qnXGUH3tbe0PW9BdaIvPZidnQ0sw6/crCY1UdqKe3rSOzt6A+e+Rj5sm4u6rPWEWX6/ezbD9DWv7U298vX3WnE89Op7rbC+6slSUhPHNiHpfh7HWKQvR496P00c40yS282oY5T+Lv12AyU1+shAmJsnRWqf3+3XuaM2fNNp7vjjLH96erru9dflcrnm6LnzZj6lrvy2jPumrkY6qlLrRwzcO6Mm8ZiKMhj4LatS63XlPlpqWqd+O90i/qfIaXv+89Pf1zeERhGlfSQhjvHS/b9JnzX5TBw7O84jsPoooe4PUcb9uHYYwqyjQqFgt2dnuzWN3297U6/8tMfDsH0v7fVlIktJTRzbhKT7edT+6ZyuHxwSpV0nNc74TYuy3WxkX4l+uy5zSY37SUz6Ri+R2oxYH/HVT1gIu1OjV64+sqCvBVXqStZfKpWqTqWWy2XPHzXy+sFGvx9xrBe/aVlecThv/NOf0/+7X/l83l4ev2sdnUmiPqOQy+XsdaazfWf9BBkbG6vasS+VSp4/tGgSj6kog4Fet85lVWp9ICsUCp5Hg+q1Sf2Z2dnZqvrSg7He2ND2auvaWTY/vnlFUFtRyqzP1vvMN7/5Tc+25DwyWiwWfduc3lFx1tHY2Jg9Xpi2JdN+EbYdmKwjZ/nOV6lUCtUX/LY39cpPczyM0vfSXF+mWjGp8WvHYbcJXuszTD+Psm0xjdFvefU8dZxhL7OsN84oZbZ/6R7XlDLbvkct2xT9dl3mkppSqVRzj4G+ZMW9o+R8zFzQI3L9OBuXSPW1fzq71Duw+skezkuFnJ3UdJpJ/I2U7zXN/Tg+5yufz1fdsO5+uU8Tlkol+/N6Y++unyDOxzkXCgXPMyFh4jERdjDQyuWyfWRBv/weNWnSJnV96DM9zvXgXC7aXnVd+72v+2wjN03r8rOa1AS1Fc2kzwZ9xq8tueft9zmlavuS8ybSMG0pTL8Iw3Qd6ctjnOs4TP/w294ElZ/meNhI30trfZlqxaTGtK9F2SYoFb6fhynbNEavA33OspyJVtj6CRpn9PLX2790L4czzqDte9SyTdFv1wUlNdb/DyAxk5OTcujQIYl7NvqHfBIOPzHNjn9paUm2bNkiu3btqpl+6623ZnY9mrIsSyYmJmRgYCDReYi0fpuk7dVqRvswkdR4CTi1SntPWzP628GDB0VEZGpqKrF5IHmtsH2n364L6LdTHWkEhOYaHx+Xvr6+mp1KEZGdO3fK2NhYClFhI6DtAQCAZshkUlOpVDz/zopmx//b3/5WRkdHZXl5uWr60tKSTE5OyuDgYOIxtLustEnaHgAA5rKyfUdGk5qdO3d6/u1kWZbRKw0m8cfpxIkTsnXrVnn66aft5R4eHpZz587Jww8/HOu8Wnm9J6nZdRpVO7c9wG2jjkdAO0m7H2dl+w6RTWkHEIXJNY2tfE16s2Pbvn27DA4OyuDgoDz//POJzquV13uSsrLc7dz2ALes9EsA/tLux2nPH+YyeaYGAAAAADSSGgAAAACZRlIDAAAAINNIagAAAABkGkkNAAAAgEwjqQEAAACQaSQ1AAAAADKNpAYAAABAppHUAAAAAMg0khoAAAAAmUZSAwAAACDTSGoAAAAAZBpJDQAAAIBM29SsGR08eLBZswI8PfPMMzI1NZV2GEBdjJdIGuOhyLlz55oyn9OnT9OnEQv6bXC/7fzpT3/60yRn/v7778vq6mqSs0Abe+2116RcLkt3d3dD5ezZs0e2bdsWU1RoN3v27JH/+7//k97e3lTjYLzMvr/97W/yzjvvyI4dO9IOxRfj4bpt27bJnj17ZGBgILF5NCtxQvuj364L6LevWUoplUZQgIl9+/bJvn375Lnnnks7FACo63vf+5689dZb8sc//jHtUABgI5ninhq0rIsXL8rZs2fl85//fNqhAICRnp4eeeutt9IOAwA2HJIatKyFhQX53//+J3feeWfaoQCAkd7eXpIaAEgBSQ1a1vz8vFxzzTVyyy23pB0KABjp7e2V//znP7KyspJ2KACwoZDUoGWdOXNG7rzzTunooJkCyAb9sAnO1gBAc7G3iJb1yiuvcOkZgEwhqQGAdJDUoCWtrq7KP/7xDx4SACBTtm7dKtu3byepAYAmI6lBS5qfn5e1tTWSGgCZw8MCAKD5SGrQkubn52XXrl3y8Y9/PO1QACAUHusMAM1HUoOWpB8SAABZ09vbyy/JA0CTkdSgJZ05c4ZLzwBkEpefAUDzkdSg5bz11lty4cIFztQAyCSd1Cil0g4FADYMkhq0nPn5eeno6JDbb7897VAAILTe3l754IMP5N///nfaoQDAhkFSg5Zz5swZ2bNnj2zbti3tUAAgNH6rBgCaj6QGLWd+fl727duXdhgAEElPT4+IkNQAQDOR1KClrK2tyauvvspDAgBk1tVXXy3XXnstSQ0ANBFJDVrKa6+9Ju+//z4PCQCQaTzWGQCai6QGLWV+fl62bNkin/rUp9IOBQAi47HOANBcJDVoKWfOnJE77rhDurq60g4FACIjqQGA5iKpQUuZn5/n0jMAmUdSAwDNRVKDlvHBBx/IX//6Vx4SACDzent75fz587K2tpZ2KACwIZDUoGX85S9/kUuXLnGmBkDm9fT0yMWLF6VSqaQdCgBsCCQ1aBnz8/Ny7bXXyu7du9MOBQAawg9wAkBzkdSgZZw5c0b27dsnlmWlHQoANKSnp0csyyKpAYAmIalBy+AhAQDaxVVXXSU7duzgt2oAoElIatAS3n33XXnjjTdIagC0DZ6ABgDNQ1KDlvDKK6+IUko+97nPpR0KAMSCpAYAmoekBi1hfn5edu/eLd3d3WmHAgCxIKkBgOYhqUFLOHPmDJeeAWgrPT09JDUA0CQkNWgJZ86c4Uc3AbSV3t5euXDhgnz44YdphwIAbY+kBqn75z//KZVKhTM1ANpKb2+vXL58Wd5+++20QwGAtkdSg9TNz89LZ2enfPazn007FACIjf4BTh7rDADJI6lB6s6cOSOf/vSn5aMf/WjaoQBAbG644Qbp7OzkvhoAaAKSGjTVb37zG/n9738v77zzjj2NH90E0I66urpk586dVUnNBx98IK+//rqUy+UUIwOA9rMp7QCwsfzud7+T8fFxEVl/MtDdd98tS0tLsn//fvnvf/8rH/nIR1KOEACiuXjxoly4cEHOnTsnpVJJzp8/Lx0dHTI6Oiq//vWv5fz58/Lee++JiMgf/vAH+drXvpZyxADQPiyllEo7CGwcTz75pBSLRbl06ZKIiFiWJZ2dnfLhhx9KZ2en9PX1yT333CN33nmnfPvb35arrroq5YgBwMwvfvEL+clPfiIiIh0dHbJp0/pxw4sXL1Z9rqOjQ9577z3Ztm1b02MEgDY1xeVnaKqbbrpJnHm0Usp+3Only5fl73//u7z44osyMTFBQgMgU4aGhuxEZW1tTS5evFiT0IiI9PX1kdAAQMxIatBUn/jEJ+r+ZsOmTZvkueeea1JEABCPrVu3ypEjR+wzNF66urrk3nvvbWJUALAxkNSgqW666abA9zdt2iRPPvmk3HrrrU2KCADi88Mf/lC6urp83798+bLs37+/iREBwMZAUoOmuvHGG6Wjw7vZdXR0SHd3t31NOgBkzbXXXis/+MEPfBObtbU1khoASABJDZqqq6tLduzY4fne2tqavPjii3L11Vc3OSoAiM+PfvQjsSzL873rrruu7hlrAEB4JDVout27d9dM6+rqkvvuu0/uu+++FCICgPjs3LlTHnnkkZqzNR0dHfKlL30ppagAoL2R1KDp+vr6am6ktSyLhwMAaBs//vGPxf2LCZ2dnfLFL34xpYgAoL2R1KDpbrrpJuns7LT/7+zslKNHj3JJBoC20dPTIw8++GDV2ZpLly7J3XffnWJUANC+SGrQdDfddJP945udnZ3S29srjz/+eMpRAUC8nnzySbl8+bL9/+bNm+Uzn/lMihEBQPsiqUHT7d69W9bW1kRk/fGmv/rVr/ihTQBt55Of/KQMDAzYZ2vuuOMO2bx5c8pRAUB7IqlB0+nLzCzLkm9961vy9a9/PeWIACAZhUJBPvzwQ7EsS+655560wwGAtlXzs8fnzp2Tl19+OY1YsEEopaSzs1M6Ozvlq1/9qkxOTqYdEjJo//790tPTk0jZjIOI0+233y6vvvqqiAjjHRKV5LgItDpLuR7PMjk5KYcOHUorHgAwMjExIQMDA4mUzTgIIIuSHBeBFjdVc6ZGcz+KEojT448/Lj//+c99f3XbsiwGZ/jy+2HDuDEOIi5HjhyRZ555Ju0w2s7BgwdFRGRqairlSNLXrHERaFXcU4NU/OxnP/NNaACg3Tz99NNphwAAbY2kBqngaWcANpItW7akHQIAtDWSGgAAAACZRlIDAAAAINNIagAAAABkGkkNAAAAgEwjqQEAAACQaSQ1AAAAADKNpAYAAABAppHUAAAAAMg0khoAAAAAmUZSAwAAACDTSGoAAAAAZBpJDQAAAIBMI6mJSaVSkfHxcenv76+aPjw8LMPDwylF5c0vVmRTltoektPK9e2OzavNpjUutfJ6AwCYI6mJydGjR+Xw4cMyMzPTtHkuLy/L0NCQWJYlQ0NDcvLkSaPvRY317NmzYlmW/RoaGqp6v1KpyPDwsP3++Ph4qPKjcsbkfAW9H8bp06dr1vPq6mrocpLSim3Pr04sy5Jjx47JzMyMrK6uNi1etBavNptGO4a5JMe8Zo6njE1AG1MuExMTymMyDIhI09bdysqKmp6etv8eGxtTImJPqydKrCMjI/b33PMql8tqbm7O/l/HUywWQ83DGd/ExITx58vlsh1XuVz2fd/rvSBzc3NKRNTY2Jg9bWFhQeVyuZbqJ63Y9px1srKyYk/X6y+Xy4WuDy1s+wiLcTB5Xm22me0Y4UxPTydWN42UfeDAAXXgwIFQ30lybEpT0uMi0OImOVOTUadOnZJcLiciItu3b5fBwUERkUQv3bj++utFKWW/9PxFRN58802566677P91PE888URi8Th1d3d7/u2e5vVekJdeeklEriyPiMjevXvlqaeeihJmWzBte851vX37dvvvvXv3yvHjx0VE5KGHHuKoKNDiVldXZXR0NHNl+2FsAtpTw0lNvct9RNYvS5qZmbF3ekZHR+3LVpaWlkLP89ixY2JZloyOjkqlUrHnowdHPe/h4WGpVCp2DM7rtWdmZuwYlpeXRURkfHy8Zloj8ZtcN67j6O/vr5qniMjJkyelv7/fPi2ul0VEqhIKp3w+XzNtdXXVXrb+/v5I63x5eVn6+/tleHhYTp8+XfO+M6HR8xQRKRQKoefVDKZ1ev78eRFZv/TOae/evVX/0/a8256f7u5ueeyxx2RmZkZOnTpl/L1WZVJPzs+srq7K0NBQzX0memzr7++XkydPyunTp30vndSftSxL/vznP/vej+Ls/85xU8Rs/HbPzz3umqybqPfKmG5fGulfUftJlD7v1de86se9/tztIqxG20CxWLQvC9TTTdp81LLTFDQ2+dVFmPoO6kdx1DWwobnP3YS97MJ5GlcrlUpV08RxyZK+RGllZUXl83klImpxcdF4fsViUZVKJbuMQqFgz0eXVy6X7Rjy+bxSStmXC4mIWlhYUEpdubQon8/bcbm/FyZ+93pwztNrWtA89el4/Rl9iY+7PG1lZcX38rNcLqfy+bx9mt1Zlikdj34FnZ4vlUp2vYSpWyeJcBq93jI53zOt04WFBftzIyMjVZcqONH2vNteUJ3o7znnbSpK+wgj7DhoUk/u9b+wsGAve7lcVrlczr7McXZ21m4v+u9CoVAz30KhUHU5pFfMuVxOjYyMVM0nl8uplZUVo/FbqeBxtx6/2EymmcTXSP9qpJ+Y9vmgMvRnnXWbz+ft/4PaRRhxtAG//4PafNSyw4hy+Vm9eXqNTUF1YVrfQf0ojrpOelwEWtxkLPfUmGycvD6jdxjD3HehNyKaHjSVWt/Ae+0QhokzzDR3/HGW7/cZv3U1Oztrb6Sc9A6qM7nQA3bYel5ZWVELCwv2QKw3kk7ODVbYunWKMjjXW6YodaqUUouLi/aGWmT9/hr3eqbt1bY9v7LCvB/0vVZKapQKV0/udaUTR3d5eudW9znn9/ROUdD89Y6Rc8x03ydm2ib8xl0Tcbf/pPuXybQ4+ryud3f95HK5qvfdZXgluH7ibANJjU1RxwGlkklqvN6vVxeN9qM46jrpcRFocekmNUHT/eidS68dS61UKqlisZj4hs89Pc7y9XKaxKDU+pE45436QeXUK8vEyMiIveH1Ui/5qSfK4GyykTL5vN/0ubm5quTG68wEbc8/xijvB30vC0mNe7rfZ5xHet0vpa7sKDofWDE7O1t1FNerbK+61Ac1dP8N0yaCxt0gcbf/pPtXmH7YSJ+v98CReu3CRFxtIMmxqZHtUbOSmnp10Wg/iqOukx4XgRaXvaTGfRmH++ix3tleXFxMfMPnnh5n+e6dmKCzWmNjY76JQ1zr3U1vFIN41YGpKIOzyUbK5PP1ytFHUkWqExvaXq2gdanbUJgjkc5y2ympMekn+pIhzb3ekmw79cbdeuJu/0n3L9NpjfZ5kzGrkXE6qIywbSDJ9tXIciZ5+Vm9M6H1ygvTj+Kqa5IabGCtkdREuaZeX4/uHBj06Vt9zWqzdix1/HGXPz09bR8BdF5r614PQTuFjexE1WNSb1HnE2Vwrjcv95kl0zr1OjLtvi6ctuctqE70ZTGzs7OBZfiVm6WkJqienNOD7kHTbWxubk6VSqWaM4VeZesdKPf9b1HajlLe466JuNtn0v3LZFocfV7Xj999Eybtop642kCSY1Mj26MkkhqvsaleXTTaj+Ko66THRaDFpZvU6KNbpr+tostxP1del1tvnnFv5Nzxx1n+9PR03cs8yuVyzY6F8+Zjpa78tox7o9nIRkSp9SNZ9XZG9dEurx3ieqIMzn7LqtR6XbnPKJjWqd9y6h1+r7Joe/7z0993n3kII+mNd1zjoEk9KXWl7RYKBXvdu9exvv4+n897Xr7iVbYzEdJ0v9Tt2nT89ht3TcTd/pPuX1HmGSUmXe/OB7mUSiW7H5m0i3ribANJjU2NbI/iTmr8xqZ6ddFoP4qjrpMeF4EWF09S434Sk74JUQ/WSl3p3HoHV9/kGnanRnd6fXRMX8+s1JUjUqVSqepygHK57PljW14/2Oj3I4714jctyysO5437+nP6f/crn8/by+N3Da4zSdRnFHK5nL3O9FEoZ/0EGRsbq9qx9zpKnMvlPJ/sEuXSIr38YQdnvW6dy6rU+ka2UCh4Hqms1yb1Z2ZnZ6vqS+8o6ASKtlfb9pxlb4Qf3wxTT27O95wvZztW6soDA9w7On5tZ2VlpWZdj42NeT7Fq9747Tfu1hOlfTqXoV58jfSvRvpJ2D7vVYZXX8rn8/aymraLIHG1AecZH/eOfNAYGrVsU1GSmihjU1BdhBlb/fpRHHWd9LgItLh4kppSqVRzj4G+ZMW9o+R8/GHQI3L9OAc+98ZdH/XQO7D66TTuJ3Lp5TOdZhJ/I+V7TXM/JtK90XPesO5+uU9f6yN/zp0Ad/0EcT7OWT9CNugzul78bh43EXVwLpfL9hEv/RoZGfFcTpM2qetDn+lxrgfneqbtVbc9v/fjaBu6/FZNakzqyetgjvNR6LrtuOl25u7jfm1Hqdo+4T7LYzp++427puslavuvF1/c7T9MP2m0z+v60fXuHlf08tdrF/XE0Qbcy+tcpqAxNGrZpsImNY2MTX51EaYdBfWjRus66XERaHGTllJKicPk5KQcOnRIXJMbpn9gKu5ym6XZ8S8tLcmWLVtk165dNdNvvfXWzK5HU5ZlycTEhAwMDCQ6D5HWb5O0vVpJt48o42BW2hMQl1Zo8wcPHhQRkampqdRiaBXN2G4CLWyqI+0IUGt8fFz6+vpqdipFRHbu3CljY2MpRIWNgLYHAACyqClJTaVS8fw7K5od/29/+1sZHR2V5eXlqulLS0syOTkpg4ODicfQ7rLSJml72ZCV9gTEhTYPoNU0JanZuXOn599OlmUZvdJgEn+cTpw4IVu3bpWnn37aXu7h4WE5d+6cPPzww7HOq5XXe5KaXadRtXPbaydZaU9x26jjRytIe91v1DYPoHVtasZMTK63beXr0Jsd2/bt22VwcFAGBwfl+eefT3Rerbzek5SV5W7nttdOstKe4rZRl7sVpL3u054/ALhxTw0AAACATCOpAQAAAJBpJDUAAAAAMo2kBgAAAECmkdQAAAAAyDSSGgAAAACZRlIDAAAAINNIagAAAABkGkkNAAAAgEwjqQEAAACQaSQ1AAAAADKNpAYAAABAppHUAAAAAMi0TX5vTE5ONjMOoMbc3FzaIWCDYxwEWtu5c+dEhL4KICCpOXToUDPjAGo8++yz8uyzz6YdBjYwxkEgG+irACyllEo7CLS3gYEBEeFIGgAAABIxxT01AAAAADKNpAYAAABAppHUAAAAAMg0khoAAAAAmUZSAwAAACDTSGoAAAAAZBpJDQAAAIBMI6kBAAAAkGkkNQAAAAAyjaQGAAAAQKaR1AAAAADINJIaAAAAAJlGUgMAAAAg00hqAAAAAGQaSQ0AAACATCOpAQAAAJBpJDUAAAAAMo2kBgAAAECmkdQAAAAAyDSSGgAAAACZRlIDAAAAINNIagAAAABkGkkNAAAAgEwjqQEAAACQaSQ1AAAAADKNpAYAAABAppHUAAAAAMg0khoAAAAAmUZSAwAAACDTSGoAAAAAZBpJDQAAAIBMI6kBAAAAkGkkNQAAAAAybVPaAaC9vPLKK3L27NmqaW+++aaIiIyMjFRNv+222+Suu+5qWmwAAABoTyQ1iFWlUpFHHnlEOjs7paNj/USgUkpERB599FEREVlbW5PLly/L9PR0anECAACgfVhK73ECMbh06ZJ87GMfk/fffz/wc1u3bpV33nlHNm/e3KTIAAAA0KamuKcGserq6pLBwcHAZKWrq0sOHz5MQgMAAIBYkNQgdocPH5aLFy/6vn/p0iW5//77mxgRAAAA2hmXnyF2a2trcsMNN0i5XPZ8f8eOHfL222/b99wAAAAADeDyM8Svo6NDHnjgAc/LyzZv3izf+c53SGgAAAAQG/YskQi/S9AuXrwohw8fTiEiAAAAtCsuP0Nibr75ZnnjjTeqpt14443yr3/9K52AAAAA0I64/AzJeeCBB6Srq8v+f/PmzfLd7343xYgAAADQjjhTg8S8/vrrcsstt1RNW1xclL6+vpQiAgAAQBviTA2Sc/PNN8ttt90mlmWJZVly2223kdAAAAAgdiQ1SNSDDz4onZ2d0tnZKQ8++GDa4QAAAKANcfkZEnXhwgXp7e0VpZQsLy9LT09P2iEBAACgvUxtqveJyclJOXToUDOCQZvr7e1NOwRk2MTEhAwMDKQdBgAAaEF1kxptYmIiyTjQxv70pz+JZVnyla98Je1QMuWZZ54REZEjR46kHEn6OLACAACCGCc1HCFFVDqZue6661KOJFumpqZEhL4nQlIDAACCGSc1QFQkMwAAAEgSTz8DAAAAkGkkNQAAAAAyjaQGAAAAQKaR1AAAAADINJIaAAAAAJlGUgMAAAAg00hqAAAAAGQaSQ0AAACATCOpAQAAAJBpJDUAAAAAMo2kBgAAAECmkdQAAAAAyDSSGgAAAACZlkhSMzw8LMPDw/b/lUpFxsfHpb+/P3BaM7hjgz/WFQAAALJgUzNmcvToUXnhhRfqTkN6VldX5ZprrhGlVNqhNF2Sy97M9WpZlu97xWJR+vr65J577pHt27cnHgsAAEAzWarO3tbk5KQcOnSo4Z0yvcPlLMdrGtIxMzMj/f39G7Iuklz2Rso+ePCgiIhMTU0Zf6dSqcjOnTtFRGRlZcVOYM6ePWufdTt+/Lh0d3eHjidNlmXJxMSEDAwMpB0KAABoPVPcUwNZXV2V0dHRtMNIRZLLnsZ6dSYrzjMye/fulePHj4uIyEMPPSSrq6tNjQsAACBJsSc1jdwrY1mW/fKb5i5/ZmZGLMuSoaEhWV5eFhGR8fHxmml+sfmV19/fX/VdvYOqYxkeHpZKpRKqDF2Ojs+yrJqd3kqlIseOHbO/f/LkSeP1V6lU7DMDq6urMjQ0VHNvk1fZxWJRZmZmqtZ3I+sqaF7O+tR16jXNlNf61PVi0p78ll2vRxGx631oaEiWlpYaKjtN3d3d8thjj8nMzIycOnWq6j2/+gpT5/r7ug6cy9tIuwYAAKhL1TExMaEMPmbL5XJKRGq+YzKtXC7XTCuVSlXTnOUvLCwopZSam5tTIqLy+byam5ur+l4+nw+MzTkt6Lv5fF6JiCqXyzXvm5ahP1soFKrK1f+Xy2WVy+XU2NiYUkqp2dnZquWsxx3HwsKCPf96ZQetl7Drqt68RkZG7HXp/LzpcrqXeWRkpKoLD1fwAAAE/0lEQVScXC6nVlZWjNqT17Lr/53LubKyYreBxcXFyGWHceDAAXXgwIHQ3wua58rKSqj6Mq3zYrGoSqWSPY9CoWDH0Gi71ss0MTERYi0AAIANZDL2pEYpswSmkWlJl+81rVAoVO3ERYlpbGysamdeqfWELJfLVb3vLsOZBNWj57myslI1vV7Zca4rk+VwJonFYrFqnZjSO8fu9Ski9g50nO1pYWFBiYgqFosNlW0qiaTG6/042oa7HnTSZ1K+6TKR1AAAAB8kNWGmKbV+lLpYLEaKSR/19uM8Ku5+mfL7fL2y41xXJsuhd3pzuZxaXFw0Xj4nnRg56TMROlGMsz25p7dLUhNH29B1MTY2VpNQx9WuSWoAAICPSR4UEMLo6Kg8+uijksvlIn1f319R732lVM2rUUmWHWVe3d3dMjY2JjMzM/Luu+9Gmo/XI8H1zfH11vVGpR8QUCgU7GlxtI0jR45ILpeTw4cPyzXXXCPHjh2LtXwAAIAgJDWGxsfH5fvf/7788pe/lL6+vkhl6GTo7NmzgZ/TN6MnIcmyw8yrUqnI+fPnpVgsyhe+8AX75v4w9Pr0+m4+nw9dnqkky07aq6++KiIiX/7yl2vea6Rt9PX1yfT0tCwsLEg+n5cnnniiKrFptHwAAIAgJDWGDh8+LCIiu3btilyG3gl/4YUX7CPmy8vLMjQ0JCIiIyMjIiJy4sQJ+3391KhGJVl2lHmdOHFCHn/8cXnooYckl8vJ0aNHQ8/n/vvvFxGRN998056m56d/4yVOeqf8G9/4RuxlN0OlUpFnn31Wcrmc3Hvvvfb0ONqGZVmyuroqe/fuleeff14WFhbkiSeeiK18AACAQPUuUAt7T43zqVDOp1uZTFNKVT1hSqkrN36LrD9tyfk9fe1+I/P0Kk/fl+H8nL4voFQqqcXFxar3TcvQT4HS0/Uy6WV1luN86adKhVn3Qe95la3jct64H3VdBc1LPxnLed+FLiPMjeP6e/ppZ3reY2Njnk+t82tPXsuu1JV7RvQDB3Tc+l6dRso2FeWeGmd9ONexfpKZc11pQfVlWue6/nR70vee1SvflHBPDQAA8Bf/gwLcOy5hpim1vjOkdwSnp6eVUsp+HKzXzlGj8zSdpp98VSgUVLlctp+G5nyMr8ny6e/qstw3yZdKJft9XX6Ude/c+TYp2718cdSj17y8PutXholyuWw/IlonIc6d+XrtyWvZnTE5H2s8MjISS9mmwiY1XomDfhWLRfuRzF5M6qteO9CJm56fSflhlo2kBgAA+Ji0lAq+W3dyclIOHTrETb3YUPQPR6bZ7vUldFNTU6nF0Cosy5KJiQkZGBhIOxQAANB6prinBgAAAECmkdQALs6nqUV5KhsAAACaa1PaAcCMvhyqnna4TDDtZd25c2fV3+2wTgEAANoZSU1GbKQd67SXNe35AwAAIBwuPwMAAACQaSQ1AAAAADKNpAYAAABAppHUAAAAAMg0khoAAAAAmUZSAwAAACDTSGoAAAAAZBpJDQAAAIBMI6kBAAAAkGkkNQAAAAAyjaQGAAAAQKaR1AAAAADINJIaAAAAAJm2yfSDlmUlGQcAH/Q9AACAYJZSSgV94Ny5c/Lyyy83Kx4A8LR//37p6elJOwwAANB6puomNQAAAADQwqa4pwYAAABAppHUAAAAAMg0khoAAAAAmbZJRKbSDgIAAAAAIjr9/wAMF7sDX4abigAAAABJRU5ErkJggg==",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(multi_task_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6972d9a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T08:40:30.414603Z",
     "start_time": "2023-10-31T08:40:30.410190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape of the first layer: [(None, 224, 224, 3)]\n"
     ]
    }
   ],
   "source": [
    "first_layer = multi_task_model.layers[0]  # Get the first layer\n",
    "input_shape = first_layer.input_shape\n",
    "print(\"Input shape of the first layer:\", input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a37f2d1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T08:41:01.607248Z",
     "start_time": "2023-10-31T08:41:01.603397Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "13\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "print(len(trainLandmarks))\n",
    "print(len(trainIllumsRaw))\n",
    "print(len(trainIllumsRet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6d29847",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T08:40:59.372833Z",
     "start_time": "2023-10-31T08:40:59.368844Z"
    }
   },
   "outputs": [],
   "source": [
    "# multi_task_model.fit()\n",
    "# trainLandmarks = trainLandmarks[:-1]\n",
    "trainIllumsRet = trainIllumsRet[:-1]\n",
    "# print(trainLandmarks)\n",
    "# print(trainIllumsRaw)\n",
    "# print(trainIllumsRet)\n",
    "\n",
    "trainIllumsRawArray = np.array(trainIllumsRaw)\n",
    "trainIllumsRetArray = np.array(trainIllumsRet)\n",
    "\n",
    "# print(trainIllumsRawArray)\n",
    "# print(trainIllumsRetArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d888c6eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T08:41:06.328293Z",
     "start_time": "2023-10-31T08:41:06.323814Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'left_eye': (213, 232), 'right_eye': (306, 222), 'nose': (239, 283), 'mouth_left': (227, 342), 'mouth_right': (298, 332)}\n",
      "  0.29567792373263885 0.45679098484374997]\n",
      " [{'left_eye': (215, 230), 'right_eye': (307, 222), 'nose': (238, 285), 'mouth_left': (228, 339), 'mouth_right': (299, 333)}\n",
      "  0.2929762817534723 0.45493657617621525]\n",
      " [{'left_eye': (207, 242), 'right_eye': (299, 228), 'nose': (236, 291), 'mouth_left': (226, 352), 'mouth_right': (296, 340)}\n",
      "  0.29606498448350704 0.45702348750868055]\n",
      " [{'left_eye': (218, 240), 'right_eye': (313, 232), 'nose': (245, 291), 'mouth_left': (231, 352), 'mouth_right': (304, 345)}\n",
      "  0.2991066410894097 0.45941350959201405]\n",
      " [{'left_eye': (223, 246), 'right_eye': (304, 238), 'nose': (242, 295), 'mouth_left': (234, 350), 'mouth_right': (302, 344)}\n",
      "  0.29778815756076393 0.45841267974826383]\n",
      " [{'left_eye': (214, 237), 'right_eye': (306, 233), 'nose': (241, 295), 'mouth_left': (227, 352), 'mouth_right': (298, 348)}\n",
      "  0.29540649650173617 0.45675732172309014]\n",
      " [{'left_eye': (216, 241), 'right_eye': (309, 230), 'nose': (245, 288), 'mouth_left': (231, 351), 'mouth_right': (303, 342)}\n",
      "  0.2979124167057292 0.45879753255208333]\n",
      " [{'left_eye': (225, 235), 'right_eye': (317, 232), 'nose': (249, 291), 'mouth_left': (235, 346), 'mouth_right': (305, 343)}\n",
      "  0.29994796240885413 0.46005176036458334]\n",
      " [{'left_eye': (214, 240), 'right_eye': (306, 231), 'nose': (243, 293), 'mouth_left': (229, 350), 'mouth_right': (300, 342)}\n",
      "  0.29637356141927085 0.45746501846354154]\n",
      " [{'left_eye': (214, 239), 'right_eye': (305, 232), 'nose': (240, 294), 'mouth_left': (227, 352), 'mouth_right': (299, 347)}\n",
      "  0.2969686096050347 0.45746883875434036]\n",
      " [{'left_eye': (207, 241), 'right_eye': (301, 230), 'nose': (236, 291), 'mouth_left': (224, 354), 'mouth_right': (296, 345)}\n",
      "  0.2955887041015627 0.45666625572048614]\n",
      " [{'left_eye': (206, 241), 'right_eye': (298, 229), 'nose': (232, 291), 'mouth_left': (223, 354), 'mouth_right': (293, 344)}\n",
      "  0.2953554853255209 0.4563671923090278]\n",
      " [{'left_eye': (205, 243), 'right_eye': (299, 232), 'nose': (234, 293), 'mouth_left': (222, 355), 'mouth_right': (293, 346)}\n",
      "  0.29572583037326383 0.45686267787760415]]\n"
     ]
    }
   ],
   "source": [
    "Y_train = np.vstack((trainLandmarks, trainIllumsRaw, trainIllumsRet)).T\n",
    "print(Y_train)\n",
    "\n",
    "# Sample 2D NumPy array\n",
    "data = Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6eff25d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T08:41:08.592765Z",
     "start_time": "2023-10-31T08:41:08.586660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[213 232 306 222 239 283 227 342 298 332]\n",
      " [215 230 307 222 238 285 228 339 299 333]\n",
      " [207 242 299 228 236 291 226 352 296 340]\n",
      " [218 240 313 232 245 291 231 352 304 345]\n",
      " [223 246 304 238 242 295 234 350 302 344]\n",
      " [214 237 306 233 241 295 227 352 298 348]\n",
      " [216 241 309 230 245 288 231 351 303 342]\n",
      " [225 235 317 232 249 291 235 346 305 343]\n",
      " [214 240 306 231 243 293 229 350 300 342]\n",
      " [214 239 305 232 240 294 227 352 299 347]\n",
      " [207 241 301 230 236 291 224 354 296 345]\n",
      " [206 241 298 229 232 291 223 354 293 344]\n",
      " [205 243 299 232 234 293 222 355 293 346]]\n"
     ]
    }
   ],
   "source": [
    "# Sample 2D NumPy array\n",
    "data = Y_train\n",
    "\n",
    "# Extract numerical values from the dictionaries\n",
    "numerical_values = []\n",
    "\n",
    "for row in data:\n",
    "    row_values = []\n",
    "    for element in row:\n",
    "        if isinstance(element, dict):\n",
    "            # Extract numerical values from the dictionary\n",
    "            dict_values = [val for key, val in element.items() if isinstance(val, tuple)]\n",
    "            for tpl in dict_values:\n",
    "                for value in tpl:\n",
    "                    if isinstance(value, (int, float)):\n",
    "                        row_values.append(value)\n",
    "    numerical_values.append(row_values)\n",
    "\n",
    "# Print the numerical values\n",
    "# for row in numerical_values:\n",
    "#     print(row)\n",
    "\n",
    "numerical_values = np.array(numerical_values)\n",
    "print(numerical_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a40ff52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T08:41:37.110670Z",
     "start_time": "2023-10-31T08:41:37.096118Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255)  # Rescale pixel values to [0, 1]\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(r'data\\Training', \n",
    "                                                    target_size=(224, 224), \n",
    "                                                    batch_size=22, \n",
    "                                                    class_mode='input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e13d4b91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T08:41:38.747319Z",
     "start_time": "2023-10-31T08:41:38.666421Z"
    }
   },
   "outputs": [],
   "source": [
    "for batch in train_generator:\n",
    "    images, labels = batch\n",
    "    break\n",
    "    print(len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "82905e04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T09:03:51.551192Z",
     "start_time": "2023-10-31T09:03:51.533969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255)  # Rescale pixel values to [0, 1]\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(r'data\\RetTraining', \n",
    "                                                    target_size=(224, 224), \n",
    "                                                    batch_size=22,\n",
    "                                                    class_mode='input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1c4df977",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T09:03:53.820192Z",
     "start_time": "2023-10-31T09:03:53.635378Z"
    }
   },
   "outputs": [],
   "source": [
    "for batch in train_generator:\n",
    "    retImages, labels = batch\n",
    "    break\n",
    "    print(len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "badf21bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T10:59:38.316834Z",
     "start_time": "2023-10-31T10:59:38.293872Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 13 224 224   3], shape=(4,), dtype=int32)\n",
      "tf.Tensor([13], shape=(1,), dtype=int32)\n",
      "tf.Tensor([13 10], shape=(2,), dtype=int32)\n",
      "tf.Tensor([13], shape=(1,), dtype=int32)\n",
      "tf.Tensor([ 13 224 224   3], shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "imageTensor = tf.convert_to_tensor(images)\n",
    "landmarkTensor = tf.convert_to_tensor(numerical_values)\n",
    "illuminanceTensor = tf.convert_to_tensor(trainIllumsRawArray)\n",
    "illumsRetTensor = tf.convert_to_tensor(trainIllumsRetArray)\n",
    "imageRetTensor = tf.convert_to_tensor(retImages)\n",
    "\n",
    "print(tf.shape(imageTensor))\n",
    "print(tf.shape(illuminanceTensor))\n",
    "print(tf.shape(landmarkTensor))\n",
    "print(tf.shape(illumsRetTensor))\n",
    "print(tf.shape(retImages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72055f6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T08:48:26.289228Z",
     "start_time": "2023-10-31T08:48:26.284416Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13 10]\n"
     ]
    }
   ],
   "source": [
    "print(tf.shape(landmarkTensor).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "badb57bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T08:43:01.181248Z",
     "start_time": "2023-10-31T08:43:01.069142Z"
    }
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input to reshape is a tensor with 1956864 values, but the requested shape has 451584 [Op:Reshape]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m imageTensor \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(imageTensor, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tf\u001b[38;5;241m.\u001b[39mshape(imageTensor))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input to reshape is a tensor with 1956864 values, but the requested shape has 451584 [Op:Reshape]"
     ]
    }
   ],
   "source": [
    "imageTensor = tf.reshape(imageTensor, (3, 224, 224, 3))\n",
    "print(tf.shape(imageTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00e8ac7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T08:43:10.260307Z",
     "start_time": "2023-10-31T08:43:10.254088Z"
    }
   },
   "outputs": [],
   "source": [
    "class InputOutputShapeCallback(Callback):\n",
    "    def __init__(self, input_data, task_names):\n",
    "        super().__init__()\n",
    "        self.input_data = input_data\n",
    "        self.task_names = task_names\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # Get the model's first layer (input layer) and last layer (output layer)\n",
    "        input_layer = self.model.layers[0]\n",
    "        output_layers = self.model.layers[1:]  # Exclude the input layer\n",
    "\n",
    "        # Print the shapes of the input and output tensors of the model\n",
    "        print(f\"Input Data Shape: {self.input_data.shape}\")\n",
    "        print(f\"Input Layer Shape: {input_layer.input_shape}\")\n",
    "\n",
    "        # Print the shapes of the output tensors for each task\n",
    "        for task_name, output_layer in zip(self.task_names, output_layers):\n",
    "            print(f\"Output Layer Shape for {task_name}: {output_layer.output_shape}\")\n",
    "\n",
    "# List of task names\n",
    "task_names = ['landmark_output', 'previous_illuminance_output', 'image_retinex_output']\n",
    "\n",
    "# Create an instance of the callback with input data and task names\n",
    "shape_callback = InputOutputShapeCallback(input_data=imageTensor, task_names=task_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ec3d61ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T11:05:08.866023Z",
     "start_time": "2023-10-31T11:03:45.866261Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 12s 12s/step - loss: 77176.3125 - landmark_output_loss: 77175.6953 - previous_illuminance_output_loss: 0.2093 - illuminance_retinex_output_loss: 0.4060 - landmark_output_mse: 77175.6953 - landmark_output_accuracy: 0.0000e+00 - previous_illuminance_output_mse: 0.2093 - previous_illuminance_output_accuracy: 0.0000e+00 - illuminance_retinex_output_mse: 0.4060 - illuminance_retinex_output_accuracy: 0.4000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 8s 8s/step - loss: 69850.1484 - landmark_output_loss: 69816.2578 - previous_illuminance_output_loss: 2.6238 - illuminance_retinex_output_loss: 31.2672 - landmark_output_mse: 69816.2578 - landmark_output_accuracy: 0.0000e+00 - previous_illuminance_output_mse: 2.6238 - previous_illuminance_output_accuracy: 0.0000e+00 - illuminance_retinex_output_mse: 31.2672 - illuminance_retinex_output_accuracy: 0.4898\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 8s 8s/step - loss: 169203040.0000 - landmark_output_loss: 167703920.0000 - previous_illuminance_output_loss: 1481891.8750 - illuminance_retinex_output_loss: 17230.7754 - landmark_output_mse: 167703920.0000 - landmark_output_accuracy: 0.0000e+00 - previous_illuminance_output_mse: 1481891.8750 - previous_illuminance_output_accuracy: 0.0000e+00 - illuminance_retinex_output_mse: 17230.7754 - illuminance_retinex_output_accuracy: 0.1524\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 8s 8s/step - loss: 76554.8594 - landmark_output_loss: 76522.4844 - previous_illuminance_output_loss: 0.2093 - illuminance_retinex_output_loss: 32.1628 - landmark_output_mse: 76522.4844 - landmark_output_accuracy: 0.0000e+00 - previous_illuminance_output_mse: 0.2093 - previous_illuminance_output_accuracy: 0.0000e+00 - illuminance_retinex_output_mse: 32.1628 - illuminance_retinex_output_accuracy: 0.1044\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 8s 8s/step - loss: 77026.6250 - landmark_output_loss: 77026.0547 - previous_illuminance_output_loss: 0.2093 - illuminance_retinex_output_loss: 0.3591 - landmark_output_mse: 77026.0547 - landmark_output_accuracy: 0.0000e+00 - previous_illuminance_output_mse: 0.2093 - previous_illuminance_output_accuracy: 0.0000e+00 - illuminance_retinex_output_mse: 0.3591 - illuminance_retinex_output_accuracy: 0.4058\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 8s 8s/step - loss: 76738.7812 - landmark_output_loss: 76738.2344 - previous_illuminance_output_loss: 0.2093 - illuminance_retinex_output_loss: 0.3375 - landmark_output_mse: 76738.2344 - landmark_output_accuracy: 0.0000e+00 - previous_illuminance_output_mse: 0.2093 - previous_illuminance_output_accuracy: 0.0000e+00 - illuminance_retinex_output_mse: 0.3375 - illuminance_retinex_output_accuracy: 0.4058\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 8s 8s/step - loss: 75830.9453 - landmark_output_loss: 75830.3984 - previous_illuminance_output_loss: 0.2093 - illuminance_retinex_output_loss: 0.3336 - landmark_output_mse: 75830.3984 - landmark_output_accuracy: 0.0000e+00 - previous_illuminance_output_mse: 0.2093 - previous_illuminance_output_accuracy: 0.0000e+00 - illuminance_retinex_output_mse: 0.3336 - illuminance_retinex_output_accuracy: 0.4058\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 7s 7s/step - loss: 71149.5000 - landmark_output_loss: 71148.7812 - previous_illuminance_output_loss: 0.2093 - illuminance_retinex_output_loss: 0.5116 - landmark_output_mse: 71148.7812 - landmark_output_accuracy: 0.0000e+00 - previous_illuminance_output_mse: 0.2093 - previous_illuminance_output_accuracy: 0.0000e+00 - illuminance_retinex_output_mse: 0.5116 - illuminance_retinex_output_accuracy: 0.1044\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 8s 8s/step - loss: 52312.4102 - landmark_output_loss: 52299.7695 - previous_illuminance_output_loss: 0.2093 - illuminance_retinex_output_loss: 12.4280 - landmark_output_mse: 52299.7695 - landmark_output_accuracy: 0.0000e+00 - previous_illuminance_output_mse: 0.2093 - previous_illuminance_output_accuracy: 0.0000e+00 - illuminance_retinex_output_mse: 12.4280 - illuminance_retinex_output_accuracy: 0.1044\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 8s 8s/step - loss: 343341.1562 - landmark_output_loss: 343058.7812 - previous_illuminance_output_loss: 0.2093 - illuminance_retinex_output_loss: 282.1578 - landmark_output_mse: 343058.7812 - landmark_output_accuracy: 0.0000e+00 - previous_illuminance_output_mse: 0.2093 - previous_illuminance_output_accuracy: 0.0000e+00 - illuminance_retinex_output_mse: 282.1578 - illuminance_retinex_output_accuracy: 0.1044\n"
     ]
    }
   ],
   "source": [
    "history = multi_task_model.fit(x=imageTensor,\n",
    "                              y=[landmarkTensor, illumsRetTensor, imageRetTensor],\n",
    "                              epochs=10,\n",
    "                              batch_size=13)\n",
    "\n",
    "\n",
    "# history = multi_task_model.fit(x=imageTensor,\n",
    "#                                y={'landmark_output': landmarkTensor, \n",
    "#                                 'previous_illuminance_output': illuminanceTensor, \n",
    "#                                 'illuminance_retinex_output': illumsRetTensor},\n",
    "#                                epochs=10,\n",
    "#                                batch_size=4,\n",
    "#                                callbacks=[shape_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "52600a91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T11:05:18.752754Z",
     "start_time": "2023-10-31T11:05:18.746455Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [77176.3125,\n",
       "  69850.1484375,\n",
       "  169203040.0,\n",
       "  76554.859375,\n",
       "  77026.625,\n",
       "  76738.78125,\n",
       "  75830.9453125,\n",
       "  71149.5,\n",
       "  52312.41015625,\n",
       "  343341.15625],\n",
       " 'landmark_output_loss': [77175.6953125,\n",
       "  69816.2578125,\n",
       "  167703920.0,\n",
       "  76522.484375,\n",
       "  77026.0546875,\n",
       "  76738.234375,\n",
       "  75830.3984375,\n",
       "  71148.78125,\n",
       "  52299.76953125,\n",
       "  343058.78125],\n",
       " 'previous_illuminance_output_loss': [0.2092738002538681,\n",
       "  2.6238365173339844,\n",
       "  1481891.875,\n",
       "  0.2092737853527069,\n",
       "  0.2092737853527069,\n",
       "  0.2092738151550293,\n",
       "  0.2092738002538681,\n",
       "  0.2092738002538681,\n",
       "  0.2092738002538681,\n",
       "  0.2092738002538681],\n",
       " 'illuminance_retinex_output_loss': [0.4060458540916443,\n",
       "  31.267208099365234,\n",
       "  17230.775390625,\n",
       "  32.162776947021484,\n",
       "  0.35910695791244507,\n",
       "  0.33750659227371216,\n",
       "  0.33361107110977173,\n",
       "  0.5116495490074158,\n",
       "  12.428034782409668,\n",
       "  282.1578063964844],\n",
       " 'landmark_output_mse': [77175.6953125,\n",
       "  69816.2578125,\n",
       "  167703920.0,\n",
       "  76522.484375,\n",
       "  77026.0546875,\n",
       "  76738.234375,\n",
       "  75830.3984375,\n",
       "  71148.78125,\n",
       "  52299.76953125,\n",
       "  343058.78125],\n",
       " 'landmark_output_accuracy': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 'previous_illuminance_output_mse': [0.2092738002538681,\n",
       "  2.6238365173339844,\n",
       "  1481891.875,\n",
       "  0.2092737853527069,\n",
       "  0.2092737853527069,\n",
       "  0.2092738151550293,\n",
       "  0.2092738002538681,\n",
       "  0.2092738002538681,\n",
       "  0.2092738002538681,\n",
       "  0.2092738002538681],\n",
       " 'previous_illuminance_output_accuracy': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 'illuminance_retinex_output_mse': [0.4060458242893219,\n",
       "  31.2672061920166,\n",
       "  17230.775390625,\n",
       "  32.162776947021484,\n",
       "  0.35910695791244507,\n",
       "  0.33750659227371216,\n",
       "  0.3336111009120941,\n",
       "  0.5116495490074158,\n",
       "  12.428034782409668,\n",
       "  282.1578063964844],\n",
       " 'illuminance_retinex_output_accuracy': [0.4000288248062134,\n",
       "  0.4897729158401489,\n",
       "  0.15244953334331512,\n",
       "  0.10439560562372208,\n",
       "  0.4058314859867096,\n",
       "  0.4058314859867096,\n",
       "  0.4058314859867096,\n",
       "  0.10439560562372208,\n",
       "  0.10439560562372208,\n",
       "  0.10439560562372208]}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "fab1e751",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T09:11:38.404672Z",
     "start_time": "2023-10-24T09:11:38.396304Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.1603e-01, 5.1305e-01, 5.1783e-01, 5.2052e-01, 5.1887e-01,\n",
       "       5.1625e-01, 5.1905e-01, 5.2186e-01, 5.1639e-01, 5.1689e-01,\n",
       "       5.1621e-01, 5.1533e-01, 5.1777e-01])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_values\n",
    "trainIllumsRawArray\n",
    "trainIllumsRetArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6f354310",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T09:11:40.662748Z",
     "start_time": "2023-10-24T09:11:40.655325Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.9568e-01, 2.9298e-01, 2.9606e-01, 2.9911e-01, 2.9779e-01,\n",
       "       2.9541e-01, 2.9791e-01, 2.9995e-01, 2.9637e-01, 2.9697e-01,\n",
       "       2.9559e-01, 2.9536e-01, 2.9573e-01])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainIllumsRawArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9e7e661e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T08:49:41.960426Z",
     "start_time": "2023-10-24T08:49:41.953075Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.1603e-01, 5.1305e-01, 5.1783e-01, 5.2052e-01, 5.1887e-01,\n",
       "       5.1625e-01, 5.1905e-01, 5.2186e-01, 5.1639e-01, 5.1689e-01,\n",
       "       5.1621e-01, 5.1533e-01, 5.1777e-01])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainIllumsRetArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8295d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
